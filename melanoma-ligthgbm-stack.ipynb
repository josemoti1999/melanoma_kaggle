{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-08-16T18:16:53.045232Z",
     "iopub.status.busy": "2020-08-16T18:16:53.044345Z",
     "iopub.status.idle": "2020-08-16T18:16:54.459781Z",
     "shell.execute_reply": "2020-08-16T18:16:54.458993Z"
    },
    "papermill": {
     "duration": 1.42902,
     "end_time": "2020-08-16T18:16:54.459957",
     "exception": false,
     "start_time": "2020-08-16T18:16:53.030937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from bayes_opt import BayesianOptimization\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-08-16T18:16:54.528201Z",
     "iopub.status.busy": "2020-08-16T18:16:54.491224Z",
     "iopub.status.idle": "2020-08-16T18:16:58.147359Z",
     "shell.execute_reply": "2020-08-16T18:16:58.146647Z"
    },
    "papermill": {
     "duration": 3.681511,
     "end_time": "2020-08-16T18:16:58.147517",
     "exception": false,
     "start_time": "2020-08-16T18:16:54.466006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model 1 out of folds roc auc score is 0.9270712268453912\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 2 out of folds roc auc score is 0.9304029895010804\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 3 out of folds roc auc score is 0.9313697307816352\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 4 out of folds roc auc score is 0.9332493446918823\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 5 out of folds roc auc score is 0.9298934081441147\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 6 out of folds roc auc score is 0.9317107381018219\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 7 out of folds roc auc score is 0.9282868895335141\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 8 out of folds roc auc score is 0.9300936601636396\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 9 out of folds roc auc score is 0.9297062346826099\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 10 out of folds roc auc score is 0.932329343176056\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 11 out of folds roc auc score is 0.9350473248157974\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 12 out of folds roc auc score is 0.9313629771000346\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 13 out of folds roc auc score is 0.9358071675965217\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 14 out of folds roc auc score is 0.9316151681471075\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 15 out of folds roc auc score is 0.9326007232549788\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 16 out of folds roc auc score is 0.9351944049928788\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 17 out of folds roc auc score is 0.9297767195342361\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 18 out of folds roc auc score is 0.9332261892121085\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 19 out of folds roc auc score is 0.9356232101738746\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 20 out of folds roc auc score is 0.9307897985746622\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 21 out of folds roc auc score is 0.9352259221736821\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 22 out of folds roc auc score is 0.9381752442085708\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 23 out of folds roc auc score is 0.9397735619200845\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 24 out of folds roc auc score is 0.9385955804872417\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 25 out of folds roc auc score is 0.9424062649294555\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 26 out of folds roc auc score is 0.9416580534892655\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 27 out of folds roc auc score is 0.9408261982384576\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our model 28 out of folds roc auc score is 0.9413700036089315\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "# function to read data and image data models predictions\n",
    "def read_data():\n",
    "    train = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/train.csv')\n",
    "    test = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')\n",
    "    groups = pd.read_csv('../input/melanoma-384x384/train.csv')\n",
    "    groups = groups[['image_name', 'tfrecord']]\n",
    "    sub = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "    train1 = pd.read_csv('../input/melanoma-subs/EfficientNetB3_384.csv')\n",
    "    train2 = pd.read_csv('../input/melanoma-subs/EfficientNetB3_384_lr.csv')\n",
    "    train3 = pd.read_csv('../input/melanoma-subs/EfficientNetB3_384_no_head.csv')\n",
    "    train4 = pd.read_csv('../input/melanoma-subs/EfficientNetB3_384_v2.0.csv')\n",
    "    train5 = pd.read_csv('../input/melanoma-subs/EfficientNetB3_384_v2.1.csv')\n",
    "    train6 = pd.read_csv('../input/melanoma-subs/EfficientNetB3_384_v2.csv')\n",
    "    train7 = pd.read_csv('../input/melanoma-subs/EfficientNetB3_512.csv')\n",
    "    train8 = pd.read_csv('../input/melanoma-subs/EfficientNetB4_384.csv')\n",
    "    train9 = pd.read_csv('../input/melanoma-subs/EfficientNetB4_384_no_head.csv')\n",
    "    train10 = pd.read_csv('../input/melanoma-subs/EfficientNetB4_384_v2.0.csv')\n",
    "    train11 = pd.read_csv('../input/melanoma-subs/EfficientNetB4_384_v2.1.csv')\n",
    "    train12 = pd.read_csv('../input/melanoma-subs/EfficientNetB5_384.csv')\n",
    "    train13 = pd.read_csv('../input/melanoma-subs/EfficientNetB5_384_v2.0.csv')\n",
    "    train14 = pd.read_csv('../input/melanoma-subs/EfficientNetB5_384_v2.csv')\n",
    "    train15 = pd.read_csv('../input/melanoma-subs/EfficientNetB6_384.csv')\n",
    "    train16 = pd.read_csv('../input/melanoma-subs/EfficientNetB6_384_no_head.csv')\n",
    "    train17 = pd.read_csv('../input/melanoma-subs/EfficientNetB6_384_v2.csv')\n",
    "    train18 = pd.read_csv('../input/melanoma-subs/experiment1.csv')\n",
    "    train19 = pd.read_csv('../input/melanoma-subs/experiment2.csv')\n",
    "    train20 = pd.read_csv('../input/melanoma-subs/experiment3.csv')\n",
    "    train21 = pd.read_csv('../input/melanoma-subs/experiment4.csv')\n",
    "    train22 = pd.read_csv('../input/melanoma-subs/experiment5.csv')\n",
    "    train23 = pd.read_csv('../input/melanoma-subs/EfficientNetB6_512_999.csv')\n",
    "    train24 = pd.read_csv('../input/melanoma-subs/EfficientNetB6_512_622.csv')\n",
    "    train25 = pd.read_csv('../input/melanoma-subs/experiment7.csv')\n",
    "    train26 = pd.read_csv('../input/melanoma-subs/experiment8.csv')\n",
    "    train27 = pd.read_csv('../input/melanoma-subs/experiment9.csv')\n",
    "    train28 = pd.read_csv('../input/melanoma-subs/experiment10.csv')\n",
    "    test1 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB3_384.csv')\n",
    "    test2 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB3_384_lr.csv')\n",
    "    test3 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB3_384_no_head.csv')\n",
    "    test4 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB3_384_v2.0.csv')\n",
    "    test5 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB3_384_v2.1.csv')\n",
    "    test6 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB3_384_v2.csv')\n",
    "    test7 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB3_512.csv')\n",
    "    test8 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB4_384.csv')\n",
    "    test9 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB4_384_no_head.csv')\n",
    "    test10 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB4_384_v2.0.csv')\n",
    "    test11 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB4_384_v2.1.csv')\n",
    "    test12 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB5_384.csv')\n",
    "    test13 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB5_384_v2.0.csv')\n",
    "    test14 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB5_384_v2.csv')\n",
    "    test15 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB6_384.csv')\n",
    "    test16 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB6_384_no_head.csv')\n",
    "    test17 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB6_384_v2.csv')\n",
    "    test18 = pd.read_csv('../input/melanoma-subs/sub_experiment1.csv')\n",
    "    test19 = pd.read_csv('../input/melanoma-subs/sub_experiment2.csv')\n",
    "    test20 = pd.read_csv('../input/melanoma-subs/sub_experiment3.csv')\n",
    "    test21 = pd.read_csv('../input/melanoma-subs/sub_experiment4.csv')\n",
    "    test22 = pd.read_csv('../input/melanoma-subs/sub_experiment5.csv')\n",
    "    test23 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB6_512_999.csv')\n",
    "    test24 = pd.read_csv('../input/melanoma-subs/sub_EfficientNetB6_512_622.csv')\n",
    "    test25 = pd.read_csv('../input/melanoma-subs/sub_experiment7.csv')\n",
    "    test26 = pd.read_csv('../input/melanoma-subs/sub_experiment8.csv')\n",
    "    test27 = pd.read_csv('../input/melanoma-subs/sub_experiment9.csv')\n",
    "    test28 = pd.read_csv('../input/melanoma-subs/sub_experiment10.csv')\n",
    "   \n",
    "    def print_roc_auc(df, model):\n",
    "        roc_auc = metrics.roc_auc_score(df['oof_target'], df['oof_prediction'])\n",
    "        print(f'Our model {model} out of folds roc auc score is {roc_auc}')\n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "        \n",
    "    print_roc_auc(train1, 1)\n",
    "    print_roc_auc(train2, 2)\n",
    "    print_roc_auc(train3, 3)\n",
    "    print_roc_auc(train4, 4)\n",
    "    print_roc_auc(train5, 5)\n",
    "    print_roc_auc(train6, 6)\n",
    "    print_roc_auc(train7, 7)\n",
    "    print_roc_auc(train8, 8)\n",
    "    print_roc_auc(train9, 9)\n",
    "    print_roc_auc(train10, 10)\n",
    "    print_roc_auc(train11, 11)\n",
    "    print_roc_auc(train12, 12)\n",
    "    print_roc_auc(train13, 13)\n",
    "    print_roc_auc(train14, 14)\n",
    "    print_roc_auc(train15, 15)\n",
    "    print_roc_auc(train16, 16)\n",
    "    print_roc_auc(train17, 17)\n",
    "    print_roc_auc(train18, 18)\n",
    "    print_roc_auc(train19, 19)\n",
    "    print_roc_auc(train20, 20)\n",
    "    print_roc_auc(train21, 21)\n",
    "    print_roc_auc(train22, 22)\n",
    "    print_roc_auc(train23, 23)\n",
    "    print_roc_auc(train24, 24)\n",
    "    print_roc_auc(train25, 25)\n",
    "    print_roc_auc(train26, 26)\n",
    "    print_roc_auc(train27, 27)\n",
    "    print_roc_auc(train28, 28)\n",
    "    \n",
    "    def fix_predictions(train, test, model):\n",
    "        test.columns = ['image_name', 'predictions_{}'.format(model)]\n",
    "        train = train[['oof_image_name', 'oof_prediction']]\n",
    "        train.columns = ['image_name', 'predictions_{}'.format(model)]\n",
    "        return train, test\n",
    "    \n",
    "    train1, test1 = fix_predictions(train1, test1, 1)\n",
    "    train2, test2 = fix_predictions(train2, test2, 2)\n",
    "    train3, test3 = fix_predictions(train3, test3, 3)\n",
    "    train4, test4 = fix_predictions(train4, test4, 4)\n",
    "    train5, test5 = fix_predictions(train5, test5, 5)\n",
    "    train6, test6 = fix_predictions(train6, test6, 6)\n",
    "    train7, test7 = fix_predictions(train7, test7, 7)\n",
    "    train8, test8 = fix_predictions(train8, test8, 8)\n",
    "    train9, test9 = fix_predictions(train9, test9, 9)\n",
    "    train10, test10 = fix_predictions(train10, test10, 10)\n",
    "    train11, test11 = fix_predictions(train11, test11, 11)\n",
    "    train12, test12 = fix_predictions(train12, test12, 12)\n",
    "    train13, test13 = fix_predictions(train13, test13, 13)\n",
    "    train14, test14 = fix_predictions(train14, test14, 14)\n",
    "    train15, test15 = fix_predictions(train15, test15, 15)\n",
    "    train16, test16 = fix_predictions(train16, test16, 16)\n",
    "    train17, test17 = fix_predictions(train17, test17, 17)\n",
    "    train18, test18 = fix_predictions(train18, test18, 18)\n",
    "    train19, test19 = fix_predictions(train19, test19, 19)\n",
    "    train20, test20 = fix_predictions(train20, test20, 20)\n",
    "    train21, test21 = fix_predictions(train21, test21, 21)\n",
    "    train22, test22 = fix_predictions(train22, test22, 22)\n",
    "    train23, test23 = fix_predictions(train23, test23, 23)\n",
    "    train24, test24 = fix_predictions(train24, test24, 24)\n",
    "    train25, test25 = fix_predictions(train25, test25, 25)\n",
    "    train26, test26 = fix_predictions(train26, test26, 26)\n",
    "    train27, test27 = fix_predictions(train27, test27, 27)\n",
    "    train28, test28 = fix_predictions(train28, test28, 28)\n",
    "    \n",
    "    train = train.merge(train1, on = 'image_name').merge(train2, on = 'image_name').merge(train3, on = 'image_name').merge(train4, on = 'image_name')\\\n",
    "    .merge(train5, on = 'image_name').merge(train6, on = 'image_name').merge(train7, on = 'image_name').merge(train8, on = 'image_name').merge(train9, on = 'image_name')\\\n",
    "    .merge(train10, on = 'image_name').merge(train11, on = 'image_name').merge(train12, on = 'image_name').merge(train13, on = 'image_name').merge(train14, on = 'image_name')\\\n",
    "    .merge(train15, on = 'image_name').merge(train16, on = 'image_name').merge(train17, on = 'image_name').merge(train18, on = 'image_name')\\\n",
    "    .merge(train19, on = 'image_name').merge(train20, on = 'image_name').merge(train21, on = 'image_name').merge(train22, on = 'image_name')\\\n",
    "    .merge(train23, on = 'image_name').merge(train24, on = 'image_name').merge(train25, on = 'image_name').merge(train26, on = 'image_name')\\\n",
    "    .merge(train27, on = 'image_name').merge(train28, on = 'image_name').merge(groups, on = 'image_name')\n",
    "    test = test.merge(test1, on = 'image_name').merge(test2, on = 'image_name').merge(test3, on = 'image_name').merge(test4, on = 'image_name')\\\n",
    "    .merge(test5, on = 'image_name').merge(test6, on = 'image_name').merge(test7, on = 'image_name').merge(test8, on = 'image_name').merge(test9, on = 'image_name')\\\n",
    "    .merge(test10, on = 'image_name').merge(test11, on = 'image_name').merge(test12, on = 'image_name').merge(test13, on = 'image_name').merge(test14, on = 'image_name')\\\n",
    "    .merge(test15, on = 'image_name').merge(test16, on = 'image_name').merge(test17, on = 'image_name').merge(test18, on = 'image_name').merge(test19, on = 'image_name')\\\n",
    "    .merge(test20, on = 'image_name').merge(test21, on = 'image_name').merge(test22, on = 'image_name').merge(test23, on = 'image_name').merge(test24, on = 'image_name')\\\n",
    "    .merge(test25, on = 'image_name').merge(test26, on = 'image_name').merge(test27, on = 'image_name').merge(test28, on = 'image_name')\n",
    "    return train, test, sub\n",
    "\n",
    "def encode_categorical(train, test):\n",
    "    for col in ['sex', 'anatom_site_general_challenge']:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        train[col].fillna('unknown', inplace = True)\n",
    "        test[col].fillna('unknown', inplace = True)\n",
    "        train[col] = encoder.fit_transform(train[col])\n",
    "        test[col] = encoder.transform(test[col])\n",
    "      # dont impute age, let light gradient boosting handle this\n",
    "    age_approx = np.nanmean(np.concatenate([np.array(train['age_approx']), np.array(test['age_approx'])]))\n",
    "    train['age_approx'].fillna(age_approx, inplace = True)\n",
    "    test['age_approx'].fillna(age_approx, inplace = True)\n",
    "    train['patient_id'].fillna('unknown', inplace = True)\n",
    "    return train, test\n",
    "\n",
    "train, test, sub = read_data()\n",
    "train, test = encode_categorical(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T18:16:58.185224Z",
     "iopub.status.busy": "2020-08-16T18:16:58.179286Z",
     "iopub.status.idle": "2020-08-16T18:59:48.549308Z",
     "shell.execute_reply": "2020-08-16T18:59:48.550431Z"
    },
    "papermill": {
     "duration": 2570.39684,
     "end_time": "2020-08-16T18:59:48.550662",
     "exception": false,
     "start_time": "2020-08-16T18:16:58.153822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to use seed 101\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.825427\tvalid_1's auc: 0.782851\n",
      "Early stopping, best iteration is:\n",
      "[141]\ttraining's auc: 0.826957\tvalid_1's auc: 0.784427\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.835581\tvalid_1's auc: 0.832355\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's auc: 0.836294\tvalid_1's auc: 0.83376\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.830577\tvalid_1's auc: 0.820987\n",
      "[200]\ttraining's auc: 0.831823\tvalid_1's auc: 0.822056\n",
      "[300]\ttraining's auc: 0.832087\tvalid_1's auc: 0.822508\n",
      "Early stopping, best iteration is:\n",
      "[332]\ttraining's auc: 0.832752\tvalid_1's auc: 0.823085\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.821554\tvalid_1's auc: 0.799154\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's auc: 0.822412\tvalid_1's auc: 0.800267\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.82678\tvalid_1's auc: 0.79838\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's auc: 0.828047\tvalid_1's auc: 0.800192\n",
      "Our adversarial validation roc auc score for our model is 0.8037677969108343\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_10', 'predictions_20', 'predictions_27', 'predictions_24', 'predictions_28', 'predictions_3', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_20', 'predictions_27', 'predictions_24', 'predictions_28', 'predictions_3', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_6\n",
      "Our roc auc score is now 0.8037464334093922\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_20', 'predictions_27', 'predictions_24', 'predictions_28', 'predictions_3', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature predictions_10 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_27', 'predictions_24', 'predictions_28', 'predictions_3', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.8013921797284693\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_24', 'predictions_28', 'predictions_3', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7991516080070782\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_28', 'predictions_3', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_24\n",
      "Our roc auc score is now 0.7846873769370402\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_3', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7813062616305743\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_3\n",
      "Our roc auc score is now 0.7810364255665639\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_11', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature predictions_23 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'anatom_site_general_challenge', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature predictions_11 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'predictions_1', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7770261258966349\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_22', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature predictions_2 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_18', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.7755111375091324\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.7740424608476374\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_21', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.7716345644451663\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_7', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature predictions_13 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.7716113584461748\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_17', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature predictions_5 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'predictions_26', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.7706873730821397\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'predictions_14', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7568184734425105\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'predictions_19', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7472270982874594\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'age_approx', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_19\n",
      "Our roc auc score is now 0.7453282729558259\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'predictions_12', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'age_approx', 'predictions_9', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_12\n",
      "Our roc auc score is now 0.7415850407292508\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'age_approx', 'predictions_15', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_9\n",
      "Our roc auc score is now 0.7415197915265412\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'age_approx', 'predictions_4', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.7408055403185481\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'age_approx', 'predictions_25', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_4\n",
      "Our roc auc score is now 0.7404433871891143\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'age_approx', 'predictions_16']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7321252808980127\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_10', 'predictions_23', 'predictions_11', 'anatom_site_general_challenge', 'predictions_2', 'predictions_8', 'predictions_13', 'predictions_5', 'age_approx']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7249300452563077\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.822811\tvalid_1's auc: 0.783119\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.83579\tvalid_1's auc: 0.831647\n",
      "[200]\ttraining's auc: 0.838039\tvalid_1's auc: 0.834445\n",
      "Early stopping, best iteration is:\n",
      "[205]\ttraining's auc: 0.838219\tvalid_1's auc: 0.834738\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's auc: 0.835986\tvalid_1's auc: 0.824865\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's auc: 0.825025\tvalid_1's auc: 0.804774\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.828078\tvalid_1's auc: 0.801931\n",
      "[200]\ttraining's auc: 0.829201\tvalid_1's auc: 0.802567\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's auc: 0.829351\tvalid_1's auc: 0.802973\n",
      "Our adversarial validation roc auc score for our model is 0.8056159319735309\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_13', 'predictions_22', 'predictions_8', 'predictions_12', 'sex', 'predictions_16', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.8019630183362014\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_22', 'predictions_8', 'predictions_12', 'sex', 'predictions_16', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_13\n",
      "Our roc auc score is now 0.8019537543198002\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'predictions_16', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.8017511157986899\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'sex', 'predictions_16', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'sex', 'predictions_16', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_12 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'predictions_16', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.8004031108890173\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7767643101979964\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'predictions_6', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_1', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_6\n",
      "Our roc auc score is now 0.7767196292842566\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7724316222002421\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_5 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_18', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_4 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_25', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.770120631698739\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_21', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7586433871868861\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_14', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.7578219981584271\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'age_approx', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7564929321181232\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is age_approx\n",
      "Our roc auc score is now 0.756227602722344\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_11 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_10', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_7 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_24', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_10 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_26', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_24 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_27', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7546179659459882\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_20', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7514583235800268\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7462397549615855\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_3', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.7451053655690058\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_9', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_3\n",
      "Our roc auc score is now 0.7405252815397533\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_17', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_9\n",
      "Our roc auc score is now 0.7391359924295104\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_19', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.7355368162150391\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_19 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_12', 'sex', 'anatom_site_general_challenge', 'predictions_5', 'predictions_4', 'predictions_11', 'predictions_7', 'predictions_10', 'predictions_24', 'predictions_19']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.718098869304237\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 103\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.821851\tvalid_1's auc: 0.783374\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.833621\tvalid_1's auc: 0.832721\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.830969\tvalid_1's auc: 0.820347\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.82061\tvalid_1's auc: 0.798361\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's auc: 0.821299\tvalid_1's auc: 0.798834\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.828508\tvalid_1's auc: 0.802797\n",
      "Our adversarial validation roc auc score for our model is 0.8034633642299515\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_23', 'predictions_9', 'predictions_11', 'predictions_2', 'predictions_5', 'predictions_3', 'predictions_16', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_6 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_11', 'predictions_2', 'predictions_5', 'predictions_3', 'predictions_16', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7798113276381674\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_11', 'predictions_2', 'predictions_5', 'predictions_3', 'predictions_16', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_9 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_2', 'predictions_5', 'predictions_3', 'predictions_16', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_11\n",
      "Our roc auc score is now 0.778790375095846\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_5', 'predictions_3', 'predictions_16', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.777487365842503\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_16', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_5\n",
      "Our roc auc score is now 0.7774868032053073\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_16', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_3 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7753027820927532\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_15', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_22 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.7752928871985064\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'predictions_7', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_14', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.7751535746079092\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7735692787880228\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_24 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_19', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_13 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_8', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_19 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_27', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_10', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7716965436673423\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_10\n",
      "Our roc auc score is now 0.771281155867594\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_18 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_17', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_1 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'anatom_site_general_challenge', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_17 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'predictions_25', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_4', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7600060582656384\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_26', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_4\n",
      "Our roc auc score is now 0.7598370595996344\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_28', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7575263211707364\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_21', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7560812167794767\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_12', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.7507295621815822\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_12\n",
      "Our roc auc score is now 0.7494224529185752\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_9', 'predictions_3', 'predictions_22', 'sex', 'predictions_24', 'age_approx', 'predictions_13', 'predictions_19', 'predictions_8', 'predictions_18', 'predictions_1', 'predictions_17', 'anatom_site_general_challenge']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7397199973046893\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 104\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's auc: 0.819744\tvalid_1's auc: 0.779938\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's auc: 0.835718\tvalid_1's auc: 0.830732\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.828837\tvalid_1's auc: 0.819317\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.825323\tvalid_1's auc: 0.803891\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.826512\tvalid_1's auc: 0.799091\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttraining's auc: 0.828\tvalid_1's auc: 0.80103\n",
      "Our adversarial validation roc auc score for our model is 0.8021019075562354\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_26', 'predictions_11', 'predictions_12', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_14', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.802095444191816\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_11', 'predictions_12', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_14', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7993534373890532\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_14', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_11\n",
      "Our roc auc score is now 0.7993208629236862\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_23', 'anatom_site_general_challenge', 'predictions_6', 'predictions_14', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_12 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_14', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7728288064584422\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_6', 'predictions_14', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_14', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_6 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7724908049484354\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_17', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_13 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.7720858315074735\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_16', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_9 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7701492649184032\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_20', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_24 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'age_approx', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7664668560009535\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is age_approx\n",
      "Our roc auc score is now 0.7664645706355124\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_18', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_15 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_1', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.766330373308331\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7616318917513665\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_19', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_3 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_2', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_19 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_27', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_2 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_8', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7555585950652863\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_8\n",
      "Our roc auc score is now 0.7537799693715909\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_5', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_10 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_22', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_5 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_5', 'predictions_21', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.7498657859608227\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_5', 'predictions_4', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.7438389904033702\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_5', 'predictions_25', 'sex', 'predictions_28']\"\n",
      "Lets continue, eliminating feature predictions_4 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_5', 'predictions_4', 'sex', 'predictions_28']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7386720353916401\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_5', 'predictions_4', 'predictions_28']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'anatom_site_general_challenge', 'predictions_6', 'predictions_13', 'predictions_9', 'predictions_24', 'predictions_15', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_10', 'predictions_5', 'predictions_4', 'sex']\"\n",
      "Lets continue, eliminating feature predictions_28 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 105\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's auc: 0.825202\tvalid_1's auc: 0.783954\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.838613\tvalid_1's auc: 0.83467\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's auc: 0.838979\tvalid_1's auc: 0.835234\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.829201\tvalid_1's auc: 0.818973\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.820721\tvalid_1's auc: 0.799963\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.828779\tvalid_1's auc: 0.801823\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's auc: 0.830638\tvalid_1's auc: 0.804105\n",
      "Our adversarial validation roc auc score for our model is 0.8035701650251661\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_11', 'predictions_13', 'predictions_12', 'predictions_15', 'predictions_24', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.8026738825796895\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_11', 'predictions_13', 'predictions_12', 'predictions_15', 'predictions_24', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_6 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_13', 'predictions_12', 'predictions_15', 'predictions_24', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_11\n",
      "Our roc auc score is now 0.8026605491922837\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_24', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_13\n",
      "Our roc auc score is now 0.8023611259321756\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_15', 'predictions_24', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_12 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_24', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_15 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_24\n",
      "Our roc auc score is now 0.7901112343206105\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_5 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_10', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_17 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'anatom_site_general_challenge', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_10 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'predictions_20', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7855870157083626\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_18', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_4 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'age_approx', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_18 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'predictions_2', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.7846405359978286\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_28', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_19 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_3', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7795724435832543\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_21', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_3\n",
      "Our roc auc score is now 0.7794855690578331\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.778302613212464\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_25', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'predictions_14', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7626600137399345\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'predictions_7', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7600273451704327\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'sex', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.7596522444221653\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'predictions_1', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'sex', 'predictions_23', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7473373946751526\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'sex', 'predictions_27', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7210861761756772\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'sex', 'predictions_9', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7201451585024741\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'sex', 'predictions_16', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_9\n",
      "Our roc auc score is now 0.707035883139742\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'sex', 'predictions_26']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7061822441371701\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_6', 'predictions_12', 'predictions_15', 'predictions_5', 'predictions_17', 'predictions_10', 'anatom_site_general_challenge', 'predictions_4', 'predictions_18', 'age_approx', 'predictions_19', 'predictions_8', 'sex']\"\n",
      "Lets continue, eliminating feature predictions_26 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 106\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.821574\tvalid_1's auc: 0.778617\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.837955\tvalid_1's auc: 0.834565\n",
      "[200]\ttraining's auc: 0.838404\tvalid_1's auc: 0.835183\n",
      "Early stopping, best iteration is:\n",
      "[173]\ttraining's auc: 0.83939\tvalid_1's auc: 0.836294\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.831292\tvalid_1's auc: 0.821337\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttraining's auc: 0.831347\tvalid_1's auc: 0.821545\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.818163\tvalid_1's auc: 0.796532\n",
      "[200]\ttraining's auc: 0.819531\tvalid_1's auc: 0.797664\n",
      "[300]\ttraining's auc: 0.820468\tvalid_1's auc: 0.798866\n",
      "[400]\ttraining's auc: 0.821542\tvalid_1's auc: 0.800265\n",
      "Early stopping, best iteration is:\n",
      "[417]\ttraining's auc: 0.821867\tvalid_1's auc: 0.800554\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.82721\tvalid_1's auc: 0.799441\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's auc: 0.829645\tvalid_1's auc: 0.80199\n",
      "Our adversarial validation roc auc score for our model is 0.8026433233024963\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_21', 'predictions_25', 'predictions_3', 'predictions_24', 'predictions_18', 'predictions_17', 'predictions_28', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_25', 'predictions_3', 'predictions_24', 'predictions_18', 'predictions_17', 'predictions_28', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_21 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_24', 'predictions_18', 'predictions_17', 'predictions_28', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7980639620670671\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_24', 'predictions_18', 'predictions_17', 'predictions_28', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_3 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_18', 'predictions_17', 'predictions_28', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_24\n",
      "Our roc auc score is now 0.7852691451900993\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_17', 'predictions_28', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.7832075213429457\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_28', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.7830928297003275\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7773409102663196\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_10', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_19 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_2', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_10\n",
      "Our roc auc score is now 0.7770072301999225\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_5', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.7742499737008891\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_23', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_5\n",
      "Our roc auc score is now 0.7742490879651057\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_1', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7610795714277725\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_16', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7460457704690253\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_7', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7459106275214085\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_12', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.7456265027008926\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_12\n",
      "Our roc auc score is now 0.7440426957068866\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_11\n",
      "Our roc auc score is now 0.7378818588008813\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_22', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_9 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.7358752773049335\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_4 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'predictions_6', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'age_approx', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_6 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'predictions_27', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_14', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7310679867279122\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_15', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7259870608931429\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'sex', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_15 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_15', 'predictions_13', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_15', 'sex', 'predictions_26', 'predictions_20']\"\n",
      "Lets continue, eliminating feature predictions_13 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_15', 'sex', 'predictions_13', 'predictions_20']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7160729088563618\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_8', 'predictions_21', 'predictions_3', 'predictions_19', 'predictions_9', 'predictions_4', 'anatom_site_general_challenge', 'predictions_6', 'age_approx', 'predictions_15', 'sex', 'predictions_13']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.6936636027413289\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 107\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.818981\tvalid_1's auc: 0.778939\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.832631\tvalid_1's auc: 0.83113\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.831811\tvalid_1's auc: 0.822099\n",
      "Early stopping, best iteration is:\n",
      "[113]\ttraining's auc: 0.832438\tvalid_1's auc: 0.822546\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.818399\tvalid_1's auc: 0.796159\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's auc: 0.824296\tvalid_1's auc: 0.797141\n",
      "Our adversarial validation roc auc score for our model is 0.800537975303369\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_27', 'predictions_4', 'predictions_24', 'predictions_10', 'predictions_12', 'predictions_14', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7965006356240526\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_27', 'predictions_4', 'predictions_24', 'predictions_10', 'predictions_12', 'predictions_14', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_24', 'predictions_10', 'predictions_12', 'predictions_14', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7957738309217961\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_24', 'predictions_10', 'predictions_12', 'predictions_14', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature predictions_4 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'predictions_12', 'predictions_14', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_24\n",
      "Our roc auc score is now 0.7818586348754888\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_12', 'predictions_14', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature predictions_10 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'predictions_14', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_12\n",
      "Our roc auc score is now 0.781611171996007\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'predictions_18', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7787672930441576\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.7766379257289043\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'predictions_26', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.773316515420504\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature predictions_3 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature predictions_23 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_9', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'age_approx', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature predictions_9 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'predictions_20', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7659314036519009\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_1', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Lets continue, eliminating feature predictions_6 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_15', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7594661953423312\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_25', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.757115566771855\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_2', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7449791231518789\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_7', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.738624188947341\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_13', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.7384873037741502\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_11', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_13\n",
      "Our roc auc score is now 0.7378745737633295\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_19', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_11\n",
      "Our roc auc score is now 0.7356368291545805\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_21', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_19\n",
      "Our roc auc score is now 0.7350900976009528\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_22', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.7278894974085599\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_16', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.7224464017323611\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_17', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7169856136231556\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6', 'predictions_5']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.7154984855255064\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_4', 'predictions_10', 'anatom_site_general_challenge', 'predictions_3', 'predictions_23', 'predictions_8', 'predictions_9', 'age_approx', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_5\n",
      "Our roc auc score is now 0.7114871051465081\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 108\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's auc: 0.822082\tvalid_1's auc: 0.780303\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's auc: 0.834441\tvalid_1's auc: 0.832302\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.831791\tvalid_1's auc: 0.822284\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's auc: 0.831671\tvalid_1's auc: 0.822304\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.82964\tvalid_1's auc: 0.808028\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's auc: 0.829184\tvalid_1's auc: 0.80234\n",
      "Our adversarial validation roc auc score for our model is 0.8045892346826146\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_28', 'predictions_18', 'sex', 'predictions_13', 'predictions_27', 'predictions_5', 'predictions_15', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.8039456417933415\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_18', 'sex', 'predictions_13', 'predictions_27', 'predictions_5', 'predictions_15', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.8002485611361465\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_13', 'predictions_27', 'predictions_5', 'predictions_15', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.7995089759350156\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_13', 'predictions_27', 'predictions_5', 'predictions_15', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_27', 'predictions_5', 'predictions_15', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_13\n",
      "Our roc auc score is now 0.7995059078910992\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_15', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7986603045732288\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_15', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_5 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.7985196090649699\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_14', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_9 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_22', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7984866195850375\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.7979054348591691\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_12', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_11 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_12\n",
      "Our roc auc score is now 0.7975612318060121\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'predictions_1', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7914195273499947\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_16', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_3', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7900272662898118\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_26', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_3\n",
      "Our roc auc score is now 0.7899881379367143\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7799361634623049\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_25', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature predictions_7 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_4', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7729513610951375\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_23', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_4\n",
      "Our roc auc score is now 0.7727938574969891\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_6', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7504601104934778\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_21', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_6\n",
      "Our roc auc score is now 0.7503293781201157\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_10', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.7412521168806689\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_19', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_10\n",
      "Our roc auc score is now 0.7411039650369001\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'anatom_site_general_challenge', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_19\n",
      "Our roc auc score is now 0.7407703518184869\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'predictions_24', 'predictions_20', 'predictions_2']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'anatom_site_general_challenge', 'predictions_20', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_24\n",
      "Our roc auc score is now 0.7250834780907851\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'anatom_site_general_challenge', 'predictions_2']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7024537351789943\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['sex', 'predictions_5', 'predictions_9', 'predictions_11', 'age_approx', 'predictions_8', 'predictions_7', 'anatom_site_general_challenge']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.6408786438808036\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 109\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.823583\tvalid_1's auc: 0.780523\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's auc: 0.824181\tvalid_1's auc: 0.78093\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.83579\tvalid_1's auc: 0.830858\n",
      "[200]\ttraining's auc: 0.837821\tvalid_1's auc: 0.833644\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's auc: 0.839121\tvalid_1's auc: 0.834884\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.834501\tvalid_1's auc: 0.824059\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.822439\tvalid_1's auc: 0.801336\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttraining's auc: 0.822672\tvalid_1's auc: 0.801646\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.830537\tvalid_1's auc: 0.802392\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's auc: 0.831533\tvalid_1's auc: 0.803317\n",
      "Our adversarial validation roc auc score for our model is 0.80404747912577\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_9', 'predictions_10', 'predictions_23', 'predictions_18', 'sex', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.8033374309847491\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_9', 'predictions_10', 'predictions_23', 'predictions_18', 'sex', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_17 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'predictions_23', 'predictions_18', 'sex', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_9\n",
      "Our roc auc score is now 0.8032686778335629\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_23', 'predictions_18', 'sex', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_10 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'predictions_18', 'sex', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.78138905564366\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.7804400788824033\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'predictions_3', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_19', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_3 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_11', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_19\n",
      "Our roc auc score is now 0.7803936376384274\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_11\n",
      "Our roc auc score is now 0.7802148972714726\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_8', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_24 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_2', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_4 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_28', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.7767501746347867\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_25', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7735060921241421\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_21', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7655044803969736\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.762923311235544\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'predictions_20', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_16', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7600287155540975\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7579028981453093\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_13', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_7 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_14', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature predictions_13 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'predictions_27', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7525210825727908\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'predictions_1', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.748659334441866\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'age_approx', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7380498087891417\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'predictions_15', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'age_approx', 'predictions_26', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.7311180126950114\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'age_approx', 'predictions_12', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7260992178830477\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'age_approx', 'predictions_5', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_12\n",
      "Our roc auc score is now 0.7219689107631337\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'age_approx', 'predictions_6']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_5\n",
      "Our roc auc score is now 0.7216156929808482\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_17', 'predictions_10', 'sex', 'predictions_3', 'predictions_24', 'predictions_8', 'predictions_4', 'anatom_site_general_challenge', 'predictions_7', 'predictions_13', 'age_approx']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_6\n",
      "Our roc auc score is now 0.7146052585899492\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 110\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.820669\tvalid_1's auc: 0.778678\n",
      "[200]\ttraining's auc: 0.82307\tvalid_1's auc: 0.780202\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's auc: 0.822856\tvalid_1's auc: 0.780634\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.834557\tvalid_1's auc: 0.831746\n",
      "Early stopping, best iteration is:\n",
      "[123]\ttraining's auc: 0.836493\tvalid_1's auc: 0.833589\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.830899\tvalid_1's auc: 0.821778\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's auc: 0.831238\tvalid_1's auc: 0.822344\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.817713\tvalid_1's auc: 0.79516\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's auc: 0.818644\tvalid_1's auc: 0.79654\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's auc: 0.831068\tvalid_1's auc: 0.803755\n",
      "Our adversarial validation roc auc score for our model is 0.8024743107098291\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_7', 'predictions_3', 'predictions_21', 'predictions_4', 'predictions_1', 'predictions_2', 'predictions_14', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature predictions_12 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_21', 'predictions_4', 'predictions_1', 'predictions_2', 'predictions_14', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.8023899360204634\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_21', 'predictions_4', 'predictions_1', 'predictions_2', 'predictions_14', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature predictions_3 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_1', 'predictions_2', 'predictions_14', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.8021433755887608\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_1', 'predictions_2', 'predictions_14', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature predictions_4 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'predictions_14', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7973996059155385\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_14', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature predictions_2 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'predictions_26', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7958260183070334\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'predictions_16', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7907286701509468\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'predictions_28', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.7890248612776214\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'predictions_19', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7850170670144128\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_19\n",
      "Our roc auc score is now 0.7848879640606523\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'sex', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'predictions_9', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_8', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature predictions_9 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_22', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature predictions_8 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_25', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.7822423548356483\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7784946688621623\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'predictions_15', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_18', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.7770475938480514\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_6', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_18\n",
      "Our roc auc score is now 0.7749206107775484\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_20', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_6\n",
      "Our roc auc score is now 0.7745251186089345\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_13', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7688391586374625\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_11', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_13\n",
      "Our roc auc score is now 0.7678173663173467\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_5', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Lets continue, eliminating feature predictions_11 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_11', 'predictions_17', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_5\n",
      "Our roc auc score is now 0.7675970771432193\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_11', 'predictions_24', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.7663574829510346\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_11', 'predictions_23', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_24\n",
      "Our roc auc score is now 0.7389877500624304\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_11', 'predictions_10', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7371775804764491\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_11', 'predictions_27']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_10\n",
      "Our roc auc score is now 0.7251437345847158\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_12', 'predictions_3', 'predictions_4', 'predictions_2', 'age_approx', 'sex', 'predictions_9', 'predictions_8', 'anatom_site_general_challenge', 'predictions_11']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7208424108252911\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Going to use seed 111\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.821519\tvalid_1's auc: 0.77803\n",
      "[200]\ttraining's auc: 0.823532\tvalid_1's auc: 0.780632\n",
      "[300]\ttraining's auc: 0.824192\tvalid_1's auc: 0.781859\n",
      "Early stopping, best iteration is:\n",
      "[295]\ttraining's auc: 0.824147\tvalid_1's auc: 0.781927\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.836646\tvalid_1's auc: 0.834699\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.831703\tvalid_1's auc: 0.822406\n",
      "Early stopping, best iteration is:\n",
      "[113]\ttraining's auc: 0.832288\tvalid_1's auc: 0.823221\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.821582\tvalid_1's auc: 0.800664\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's auc: 0.822203\tvalid_1's auc: 0.801114\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.826462\tvalid_1's auc: 0.797762\n",
      "[200]\ttraining's auc: 0.829101\tvalid_1's auc: 0.801384\n",
      "Early stopping, best iteration is:\n",
      "[188]\ttraining's auc: 0.829338\tvalid_1's auc: 0.801633\n",
      "Our adversarial validation roc auc score for our model is 0.8038103400817636\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_7', 'predictions_26', 'age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_14', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_16\n",
      "Our roc auc score is now 0.800669484784541\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['predictions_26', 'age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_14', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_7\n",
      "Our roc auc score is now 0.8006550567614028\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_14', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_26\n",
      "Our roc auc score is now 0.7986598937366627\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_14', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature age_approx did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'sex', 'predictions_6', 'predictions_14', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature anatom_site_general_challenge did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'predictions_6', 'predictions_14', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature sex did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_14', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature predictions_6 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_1', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_14\n",
      "Our roc auc score is now 0.7935024116412822\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_23', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_1\n",
      "Our roc auc score is now 0.7906647732272399\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_2', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_23\n",
      "Our roc auc score is now 0.7678387785621101\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_2\n",
      "Our roc auc score is now 0.7670994523969158\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_25', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature predictions_9 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_21', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_25\n",
      "Our roc auc score is now 0.7538067795910343\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_21\n",
      "Our roc auc score is now 0.7502475102301369\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_12', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature predictions_19 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_20', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_12\n",
      "Our roc auc score is now 0.7495189089883196\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_20\n",
      "Our roc auc score is now 0.7414663507416104\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_15', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature predictions_5 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_22', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_15\n",
      "Our roc auc score is now 0.7390731998902\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_28', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_22\n",
      "Our roc auc score is now 0.7364347030121233\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_28\n",
      "Our roc auc score is now 0.7318786132310031\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_3', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Lets continue, eliminating feature predictions_18 did not improve our adversarial validation score\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_17', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_3\n",
      "Our roc auc score is now 0.729791712490031\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_11', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_17\n",
      "Our roc auc score is now 0.7278260990593977\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_10', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_11\n",
      "Our roc auc score is now 0.7226852746459436\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_27', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_10\n",
      "Our roc auc score is now 0.7208886668446457\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_13', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_27\n",
      "Our roc auc score is now 0.7087370960830357\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_8', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_13\n",
      "Our roc auc score is now 0.7061358056785267\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_4', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_8\n",
      "Our roc auc score is now 0.6869107531287697\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18', 'predictions_24']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_4\n",
      "Our roc auc score is now 0.6783704121086833\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training with features ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18']\"\n",
      "Great we found a feature that improves our adversarial validation score, this is predictions_24\n",
      "Our roc auc score is now 0.6150688212247162\n",
      "\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# adversarial validation\n",
    "def run_adversarial_val(train, test, folds = 5):\n",
    "    \n",
    "    SEED = 100\n",
    "    roc_auc_adversarial = 1\n",
    "    # iterate different seed until we find a combination of models that gives < 0.65 adversarial roc auc\n",
    "    while roc_auc_adversarial > 0.64:\n",
    "        # seed everything\n",
    "        SEED = SEED + 1\n",
    "        seed_everything(SEED)\n",
    "        print(f'Going to use seed {SEED}')\n",
    "        train_c = train.copy()\n",
    "        test_c = test.copy()\n",
    "        # define some basic params for adversarial validation, using rf\n",
    "        params = {\n",
    "            'boosting_type': 'rf',\n",
    "            'metric': 'auc',\n",
    "            'objective': 'binary',\n",
    "            'n_jobs': -1,\n",
    "            'seed': SEED,\n",
    "            'learning_rate': 0.1,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 1,\n",
    "\n",
    "        }\n",
    "\n",
    "        features = [col for col in train.columns if col not in ['image_name', 'patient_id', 'diagnosis', 'benign_malignant', 'target', 'source', 'tfrecord']]\n",
    "        # shuffle the list to make it random\n",
    "        random.shuffle(features)\n",
    "        train_c['target'] = 1\n",
    "        test_c['target'] = 0\n",
    "        data = pd.concat([train_c, test_c], axis = 0)\n",
    "        data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        # run a normal experiment to get the out of folds roc auc\n",
    "        kf = GroupKFold(n_splits = folds)\n",
    "        target = 'target'\n",
    "        oof_pred = np.zeros(len(data))\n",
    "        for fold, (tr_ind, val_ind) in enumerate(kf.split(data, groups = data['patient_id'])):\n",
    "            x_train, x_val = data[features].iloc[tr_ind], data[features].iloc[val_ind]\n",
    "            y_train, y_val = data[target][tr_ind], data[target][val_ind]\n",
    "            train_set = lgb.Dataset(x_train, y_train)\n",
    "            val_set = lgb.Dataset(x_val, y_val)\n",
    "\n",
    "            model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n",
    "                              valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "            oof_pred[val_ind] = model.predict(x_val)\n",
    "        rauc = metrics.roc_auc_score(data['target'], oof_pred)\n",
    "        print(f'Our adversarial validation roc auc score for our model is {rauc}')\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "\n",
    "        # list to store our bad features\n",
    "        bad_features = []\n",
    "        # lets iterate and remove features that minimize our roc auc so the distribution from our trian and test are more similar\n",
    "        for feature in features:\n",
    "            oof_pred = np.zeros(len(data))\n",
    "            new_features = [col for col in features if col not in [feature] + bad_features]\n",
    "            print(f'Training with features {new_features}\"')\n",
    "            for fold, (tr_ind, val_ind) in enumerate(kf.split(data, groups = data['patient_id'])):\n",
    "                x_train, x_val = data[new_features].iloc[tr_ind], data[new_features].iloc[val_ind]\n",
    "                y_train, y_val = data[target][tr_ind], data[target][val_ind]\n",
    "                train_set = lgb.Dataset(x_train, y_train)\n",
    "                val_set = lgb.Dataset(x_val, y_val)\n",
    "\n",
    "                model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n",
    "                                  valid_sets = [train_set, val_set], verbose_eval = False)\n",
    "                oof_pred[val_ind] = model.predict(x_val)\n",
    "            rauc_ = metrics.roc_auc_score(data['target'], oof_pred)\n",
    "            if rauc_ < rauc:\n",
    "                print(f'Great we found a feature that improves our adversarial validation score, this is {feature}')\n",
    "                print(f'Our roc auc score is now {rauc_}')\n",
    "                # append feature to bad features list\n",
    "                bad_features.append(feature)\n",
    "                # update rauc\n",
    "                rauc = rauc_\n",
    "            else:\n",
    "                print(f'Lets continue, eliminating feature {feature} did not improve our adversarial validation score')\n",
    "            print('\\n')\n",
    "            print('-'*50)\n",
    "\n",
    "\n",
    "        good_features = [col for col in features if col not in bad_features]\n",
    "        \n",
    "        # update roc auc adversarial\n",
    "        roc_auc_adversarial = rauc\n",
    "    return good_features, SEED\n",
    "    \n",
    "    \n",
    "    \n",
    "good_features, SEED = run_adversarial_val(train, test, folds = 5)\n",
    "# seed everything with optimal seed\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T18:59:48.893666Z",
     "iopub.status.busy": "2020-08-16T18:59:48.888434Z",
     "iopub.status.idle": "2020-08-16T19:35:09.263503Z",
     "shell.execute_reply": "2020-08-16T19:35:09.262796Z"
    },
    "papermill": {
     "duration": 2120.530429,
     "end_time": "2020-08-16T19:35:09.263658",
     "exception": false,
     "start_time": "2020-08-16T18:59:48.733229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | baggin... | colsam... | colsam... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.7672  \u001b[0m | \u001b[0m 2.522   \u001b[0m | \u001b[0m 0.6616  \u001b[0m | \u001b[0m 0.8616  \u001b[0m | \u001b[0m 0.886   \u001b[0m | \u001b[0m 0.4475  \u001b[0m | \u001b[0m 0.01427 \u001b[0m | \u001b[0m 46.66   \u001b[0m | \u001b[0m 31.48   \u001b[0m | \u001b[0m 0.003443\u001b[0m | \u001b[0m 99.26   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.944   \u001b[0m | \u001b[95m 0.5426  \u001b[0m | \u001b[95m 1.731   \u001b[0m | \u001b[95m 0.8018  \u001b[0m | \u001b[95m 0.7727  \u001b[0m | \u001b[95m 0.8228  \u001b[0m | \u001b[95m 1.399   \u001b[0m | \u001b[95m 0.03249 \u001b[0m | \u001b[95m 14.8    \u001b[0m | \u001b[95m 91.07   \u001b[0m | \u001b[95m 0.00796 \u001b[0m | \u001b[95m 87.25   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 0.889   \u001b[0m | \u001b[0m 9.919   \u001b[0m | \u001b[0m 0.7464  \u001b[0m | \u001b[0m 0.8883  \u001b[0m | \u001b[0m 1.264   \u001b[0m | \u001b[0m 0.08234 \u001b[0m | \u001b[0m 0.09629 \u001b[0m | \u001b[0m 17.69   \u001b[0m | \u001b[0m 83.55   \u001b[0m | \u001b[0m 0.007008\u001b[0m | \u001b[0m 65.22   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.5645  \u001b[0m | \u001b[0m 9.986   \u001b[0m | \u001b[0m 0.4828  \u001b[0m | \u001b[0m 0.7692  \u001b[0m | \u001b[0m 1.455   \u001b[0m | \u001b[0m 1.215   \u001b[0m | \u001b[0m 0.1483  \u001b[0m | \u001b[0m 37.7    \u001b[0m | \u001b[0m 46.05   \u001b[0m | \u001b[0m 0.003258\u001b[0m | \u001b[0m 95.78   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.938   \u001b[0m | \u001b[0m 0.9511  \u001b[0m | \u001b[0m 8.325   \u001b[0m | \u001b[0m 0.4204  \u001b[0m | \u001b[0m 0.966   \u001b[0m | \u001b[0m 2.851   \u001b[0m | \u001b[0m 2.42    \u001b[0m | \u001b[0m 0.1014  \u001b[0m | \u001b[0m 96.94   \u001b[0m | \u001b[0m 47.53   \u001b[0m | \u001b[0m 0.003257\u001b[0m | \u001b[0m 21.32   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.4229  \u001b[0m | \u001b[0m 1.478   \u001b[0m | \u001b[0m 0.4768  \u001b[0m | \u001b[0m 0.42    \u001b[0m | \u001b[0m 0.6793  \u001b[0m | \u001b[0m 1.632   \u001b[0m | \u001b[0m 0.04394 \u001b[0m | \u001b[0m 24.5    \u001b[0m | \u001b[0m 23.47   \u001b[0m | \u001b[0m 0.006862\u001b[0m | \u001b[0m 57.52   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9443  \u001b[0m | \u001b[95m 0.8609  \u001b[0m | \u001b[95m 7.019   \u001b[0m | \u001b[95m 0.6648  \u001b[0m | \u001b[95m 0.7571  \u001b[0m | \u001b[95m 1.385   \u001b[0m | \u001b[95m 2.89    \u001b[0m | \u001b[95m 0.03808 \u001b[0m | \u001b[95m 91.43   \u001b[0m | \u001b[95m 54.81   \u001b[0m | \u001b[95m 0.005966\u001b[0m | \u001b[95m 74.77   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.5065  \u001b[0m | \u001b[0m 8.782   \u001b[0m | \u001b[0m 0.6893  \u001b[0m | \u001b[0m 0.4416  \u001b[0m | \u001b[0m 1.094   \u001b[0m | \u001b[0m 1.085   \u001b[0m | \u001b[0m 0.09969 \u001b[0m | \u001b[0m 24.51   \u001b[0m | \u001b[0m 25.78   \u001b[0m | \u001b[0m 0.009496\u001b[0m | \u001b[0m 72.45   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.481   \u001b[0m | \u001b[0m 9.35    \u001b[0m | \u001b[0m 0.9051  \u001b[0m | \u001b[0m 0.4317  \u001b[0m | \u001b[0m 0.6111  \u001b[0m | \u001b[0m 2.618   \u001b[0m | \u001b[0m 0.138   \u001b[0m | \u001b[0m 35.35   \u001b[0m | \u001b[0m 76.1    \u001b[0m | \u001b[0m 0.006222\u001b[0m | \u001b[0m 36.84   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9332  \u001b[0m | \u001b[0m 0.8812  \u001b[0m | \u001b[0m 1.532   \u001b[0m | \u001b[0m 0.5849  \u001b[0m | \u001b[0m 0.5244  \u001b[0m | \u001b[0m 2.917   \u001b[0m | \u001b[0m 2.728   \u001b[0m | \u001b[0m 0.1385  \u001b[0m | \u001b[0m 18.6    \u001b[0m | \u001b[0m 96.22   \u001b[0m | \u001b[0m 0.001008\u001b[0m | \u001b[0m 70.38   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 4.058   \u001b[0m | \u001b[0m 0.7917  \u001b[0m | \u001b[0m 0.9551  \u001b[0m | \u001b[0m 1.122   \u001b[0m | \u001b[0m 2.314   \u001b[0m | \u001b[0m 0.1803  \u001b[0m | \u001b[0m 65.06   \u001b[0m | \u001b[0m 23.6    \u001b[0m | \u001b[0m 0.003812\u001b[0m | \u001b[0m 70.87   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 5.79    \u001b[0m | \u001b[0m 0.6114  \u001b[0m | \u001b[0m 0.7959  \u001b[0m | \u001b[0m 1.43    \u001b[0m | \u001b[0m 1.364   \u001b[0m | \u001b[0m 0.03371 \u001b[0m | \u001b[0m 45.95   \u001b[0m | \u001b[0m 34.58   \u001b[0m | \u001b[0m 0.003533\u001b[0m | \u001b[0m 97.95   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.5293  \u001b[0m | \u001b[0m 7.146   \u001b[0m | \u001b[0m 0.4191  \u001b[0m | \u001b[0m 0.9012  \u001b[0m | \u001b[0m 2.105   \u001b[0m | \u001b[0m 2.898   \u001b[0m | \u001b[0m 0.1224  \u001b[0m | \u001b[0m 28.0    \u001b[0m | \u001b[0m 74.73   \u001b[0m | \u001b[0m 0.000405\u001b[0m | \u001b[0m 53.5    \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.5687  \u001b[0m | \u001b[0m 1.956   \u001b[0m | \u001b[0m 0.6558  \u001b[0m | \u001b[0m 0.9043  \u001b[0m | \u001b[0m 0.3312  \u001b[0m | \u001b[0m 2.96    \u001b[0m | \u001b[0m 0.05433 \u001b[0m | \u001b[0m 74.37   \u001b[0m | \u001b[0m 69.48   \u001b[0m | \u001b[0m 0.000869\u001b[0m | \u001b[0m 75.78   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 9.117   \u001b[0m | \u001b[0m 0.891   \u001b[0m | \u001b[0m 0.6395  \u001b[0m | \u001b[0m 2.461   \u001b[0m | \u001b[0m 0.5934  \u001b[0m | \u001b[0m 0.1093  \u001b[0m | \u001b[0m 45.95   \u001b[0m | \u001b[0m 93.33   \u001b[0m | \u001b[0m 0.008271\u001b[0m | \u001b[0m 60.08   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.9408  \u001b[0m | \u001b[0m 0.4701  \u001b[0m | \u001b[0m 1.104   \u001b[0m | \u001b[0m 0.9479  \u001b[0m | \u001b[0m 0.7074  \u001b[0m | \u001b[0m 2.529   \u001b[0m | \u001b[0m 0.2845  \u001b[0m | \u001b[0m 0.1627  \u001b[0m | \u001b[0m 13.72   \u001b[0m | \u001b[0m 67.19   \u001b[0m | \u001b[0m 0.009691\u001b[0m | \u001b[0m 51.79   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.9382  \u001b[0m | \u001b[0m 0.7957  \u001b[0m | \u001b[0m 2.934   \u001b[0m | \u001b[0m 0.9339  \u001b[0m | \u001b[0m 0.992   \u001b[0m | \u001b[0m 2.216   \u001b[0m | \u001b[0m 1.924   \u001b[0m | \u001b[0m 0.05791 \u001b[0m | \u001b[0m 51.77   \u001b[0m | \u001b[0m 34.13   \u001b[0m | \u001b[0m 0.008696\u001b[0m | \u001b[0m 41.35   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.94    \u001b[0m | \u001b[0m 0.9837  \u001b[0m | \u001b[0m 2.922   \u001b[0m | \u001b[0m 0.5787  \u001b[0m | \u001b[0m 0.8421  \u001b[0m | \u001b[0m 0.03867 \u001b[0m | \u001b[0m 0.0907  \u001b[0m | \u001b[0m 0.1494  \u001b[0m | \u001b[0m 53.84   \u001b[0m | \u001b[0m 29.63   \u001b[0m | \u001b[0m 0.000188\u001b[0m | \u001b[0m 88.06   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.7218  \u001b[0m | \u001b[0m 7.512   \u001b[0m | \u001b[0m 0.7969  \u001b[0m | \u001b[0m 0.9033  \u001b[0m | \u001b[0m 1.165   \u001b[0m | \u001b[0m 1.279   \u001b[0m | \u001b[0m 0.1673  \u001b[0m | \u001b[0m 51.15   \u001b[0m | \u001b[0m 83.8    \u001b[0m | \u001b[0m 0.000240\u001b[0m | \u001b[0m 51.74   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.9391  \u001b[0m | \u001b[0m 0.477   \u001b[0m | \u001b[0m 2.063   \u001b[0m | \u001b[0m 0.5236  \u001b[0m | \u001b[0m 0.8486  \u001b[0m | \u001b[0m 2.79    \u001b[0m | \u001b[0m 1.142   \u001b[0m | \u001b[0m 0.09751 \u001b[0m | \u001b[0m 88.35   \u001b[0m | \u001b[0m 83.31   \u001b[0m | \u001b[0m 0.000347\u001b[0m | \u001b[0m 89.84   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.7627  \u001b[0m | \u001b[0m 5.648   \u001b[0m | \u001b[0m 0.7536  \u001b[0m | \u001b[0m 0.8799  \u001b[0m | \u001b[0m 0.5198  \u001b[0m | \u001b[0m 0.5606  \u001b[0m | \u001b[0m 0.05905 \u001b[0m | \u001b[0m 12.7    \u001b[0m | \u001b[0m 59.74   \u001b[0m | \u001b[0m 0.003583\u001b[0m | \u001b[0m 93.36   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.6859  \u001b[0m | \u001b[0m 1.941   \u001b[0m | \u001b[0m 0.4915  \u001b[0m | \u001b[0m 0.5641  \u001b[0m | \u001b[0m 2.345   \u001b[0m | \u001b[0m 1.179   \u001b[0m | \u001b[0m 0.1618  \u001b[0m | \u001b[0m 47.84   \u001b[0m | \u001b[0m 17.34   \u001b[0m | \u001b[0m 0.000205\u001b[0m | \u001b[0m 54.47   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.7976  \u001b[0m | \u001b[0m 5.637   \u001b[0m | \u001b[0m 0.7481  \u001b[0m | \u001b[0m 0.6959  \u001b[0m | \u001b[0m 2.221   \u001b[0m | \u001b[0m 2.368   \u001b[0m | \u001b[0m 0.1584  \u001b[0m | \u001b[0m 19.73   \u001b[0m | \u001b[0m 94.61   \u001b[0m | \u001b[0m 0.008789\u001b[0m | \u001b[0m 75.96   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.962   \u001b[0m | \u001b[0m 7.064   \u001b[0m | \u001b[0m 0.7567  \u001b[0m | \u001b[0m 0.607   \u001b[0m | \u001b[0m 0.2507  \u001b[0m | \u001b[0m 1.738   \u001b[0m | \u001b[0m 0.01335 \u001b[0m | \u001b[0m 58.82   \u001b[0m | \u001b[0m 66.93   \u001b[0m | \u001b[0m 0.006817\u001b[0m | \u001b[0m 33.64   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.7462  \u001b[0m | \u001b[0m 2.854   \u001b[0m | \u001b[0m 0.5509  \u001b[0m | \u001b[0m 0.9816  \u001b[0m | \u001b[0m 2.69    \u001b[0m | \u001b[0m 0.3685  \u001b[0m | \u001b[0m 0.1774  \u001b[0m | \u001b[0m 95.21   \u001b[0m | \u001b[0m 51.39   \u001b[0m | \u001b[0m 0.004188\u001b[0m | \u001b[0m 66.17   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.6301  \u001b[0m | \u001b[0m 2.478   \u001b[0m | \u001b[0m 0.7892  \u001b[0m | \u001b[0m 0.5494  \u001b[0m | \u001b[0m 1.977   \u001b[0m | \u001b[0m 2.701   \u001b[0m | \u001b[0m 0.1824  \u001b[0m | \u001b[0m 75.1    \u001b[0m | \u001b[0m 25.75   \u001b[0m | \u001b[0m 0.004233\u001b[0m | \u001b[0m 40.34   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 4.344   \u001b[0m | \u001b[0m 0.6695  \u001b[0m | \u001b[0m 0.9622  \u001b[0m | \u001b[0m 2.439   \u001b[0m | \u001b[0m 0.09076 \u001b[0m | \u001b[0m 0.05872 \u001b[0m | \u001b[0m 61.29   \u001b[0m | \u001b[0m 44.52   \u001b[0m | \u001b[0m 0.007801\u001b[0m | \u001b[0m 62.37   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.4696  \u001b[0m | \u001b[0m 7.88    \u001b[0m | \u001b[0m 0.5154  \u001b[0m | \u001b[0m 0.6255  \u001b[0m | \u001b[0m 0.000123\u001b[0m | \u001b[0m 2.841   \u001b[0m | \u001b[0m 0.1427  \u001b[0m | \u001b[0m 16.6    \u001b[0m | \u001b[0m 94.55   \u001b[0m | \u001b[0m 0.003217\u001b[0m | \u001b[0m 40.75   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.5586  \u001b[0m | \u001b[0m 7.619   \u001b[0m | \u001b[0m 0.9176  \u001b[0m | \u001b[0m 0.4333  \u001b[0m | \u001b[0m 0.03493 \u001b[0m | \u001b[0m 2.994   \u001b[0m | \u001b[0m 0.08701 \u001b[0m | \u001b[0m 85.34   \u001b[0m | \u001b[0m 17.06   \u001b[0m | \u001b[0m 0.008434\u001b[0m | \u001b[0m 21.78   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.6519  \u001b[0m | \u001b[0m 5.217   \u001b[0m | \u001b[0m 0.5361  \u001b[0m | \u001b[0m 0.6295  \u001b[0m | \u001b[0m 1.162   \u001b[0m | \u001b[0m 2.56    \u001b[0m | \u001b[0m 0.03639 \u001b[0m | \u001b[0m 69.57   \u001b[0m | \u001b[0m 61.82   \u001b[0m | \u001b[0m 0.001407\u001b[0m | \u001b[0m 33.14   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.6515  \u001b[0m | \u001b[0m 1.101   \u001b[0m | \u001b[0m 0.5295  \u001b[0m | \u001b[0m 0.5928  \u001b[0m | \u001b[0m 2.453   \u001b[0m | \u001b[0m 2.521   \u001b[0m | \u001b[0m 0.1102  \u001b[0m | \u001b[0m 31.58   \u001b[0m | \u001b[0m 16.27   \u001b[0m | \u001b[0m 0.008763\u001b[0m | \u001b[0m 56.7    \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.6355  \u001b[0m | \u001b[0m 4.83    \u001b[0m | \u001b[0m 0.7858  \u001b[0m | \u001b[0m 0.7249  \u001b[0m | \u001b[0m 2.127   \u001b[0m | \u001b[0m 0.134   \u001b[0m | \u001b[0m 0.1291  \u001b[0m | \u001b[0m 97.19   \u001b[0m | \u001b[0m 83.56   \u001b[0m | \u001b[0m 0.007639\u001b[0m | \u001b[0m 59.07   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.9381  \u001b[0m | \u001b[0m 0.4753  \u001b[0m | \u001b[0m 3.656   \u001b[0m | \u001b[0m 0.4156  \u001b[0m | \u001b[0m 0.4484  \u001b[0m | \u001b[0m 0.1266  \u001b[0m | \u001b[0m 1.366   \u001b[0m | \u001b[0m 0.01591 \u001b[0m | \u001b[0m 96.41   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 0.003802\u001b[0m | \u001b[0m 60.44   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.9387  \u001b[0m | \u001b[0m 0.4283  \u001b[0m | \u001b[0m 7.562   \u001b[0m | \u001b[0m 0.4556  \u001b[0m | \u001b[0m 0.9854  \u001b[0m | \u001b[0m 1.236   \u001b[0m | \u001b[0m 2.363   \u001b[0m | \u001b[0m 0.1938  \u001b[0m | \u001b[0m 93.45   \u001b[0m | \u001b[0m 83.41   \u001b[0m | \u001b[0m 0.009221\u001b[0m | \u001b[0m 76.93   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.7409  \u001b[0m | \u001b[0m 8.549   \u001b[0m | \u001b[0m 0.7756  \u001b[0m | \u001b[0m 0.4488  \u001b[0m | \u001b[0m 2.343   \u001b[0m | \u001b[0m 1.073   \u001b[0m | \u001b[0m 0.16    \u001b[0m | \u001b[0m 61.2    \u001b[0m | \u001b[0m 80.48   \u001b[0m | \u001b[0m 0.008102\u001b[0m | \u001b[0m 46.49   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.9809  \u001b[0m | \u001b[0m 9.107   \u001b[0m | \u001b[0m 0.8953  \u001b[0m | \u001b[0m 0.6523  \u001b[0m | \u001b[0m 0.267   \u001b[0m | \u001b[0m 1.957   \u001b[0m | \u001b[0m 0.06948 \u001b[0m | \u001b[0m 53.0    \u001b[0m | \u001b[0m 80.9    \u001b[0m | \u001b[0m 0.003748\u001b[0m | \u001b[0m 95.99   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.5618  \u001b[0m | \u001b[0m 2.13    \u001b[0m | \u001b[0m 0.8387  \u001b[0m | \u001b[0m 0.4696  \u001b[0m | \u001b[0m 2.797   \u001b[0m | \u001b[0m 0.08054 \u001b[0m | \u001b[0m 0.0945  \u001b[0m | \u001b[0m 68.67   \u001b[0m | \u001b[0m 18.08   \u001b[0m | \u001b[0m 0.000698\u001b[0m | \u001b[0m 95.17   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.962   \u001b[0m | \u001b[0m 9.466   \u001b[0m | \u001b[0m 0.4859  \u001b[0m | \u001b[0m 0.7332  \u001b[0m | \u001b[0m 0.4658  \u001b[0m | \u001b[0m 1.272   \u001b[0m | \u001b[0m 0.1616  \u001b[0m | \u001b[0m 26.06   \u001b[0m | \u001b[0m 56.69   \u001b[0m | \u001b[0m 0.001699\u001b[0m | \u001b[0m 20.33   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.7639  \u001b[0m | \u001b[0m 9.678   \u001b[0m | \u001b[0m 0.9077  \u001b[0m | \u001b[0m 0.7997  \u001b[0m | \u001b[0m 2.266   \u001b[0m | \u001b[0m 0.1896  \u001b[0m | \u001b[0m 0.09933 \u001b[0m | \u001b[0m 37.11   \u001b[0m | \u001b[0m 26.25   \u001b[0m | \u001b[0m 0.005184\u001b[0m | \u001b[0m 44.95   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.9405  \u001b[0m | \u001b[0m 0.8668  \u001b[0m | \u001b[0m 3.38    \u001b[0m | \u001b[0m 0.9525  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.008841\u001b[0m | \u001b[0m 2.889   \u001b[0m | \u001b[0m 0.1368  \u001b[0m | \u001b[0m 17.49   \u001b[0m | \u001b[0m 85.11   \u001b[0m | \u001b[0m 0.003336\u001b[0m | \u001b[0m 76.83   \u001b[0m |\n",
      "| \u001b[95m 41      \u001b[0m | \u001b[95m 0.9443  \u001b[0m | \u001b[95m 0.7142  \u001b[0m | \u001b[95m 1.211   \u001b[0m | \u001b[95m 0.4212  \u001b[0m | \u001b[95m 0.8186  \u001b[0m | \u001b[95m 0.8928  \u001b[0m | \u001b[95m 0.9324  \u001b[0m | \u001b[95m 0.0406  \u001b[0m | \u001b[95m 99.74   \u001b[0m | \u001b[95m 32.06   \u001b[0m | \u001b[95m 0.009865\u001b[0m | \u001b[95m 44.35   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.5559  \u001b[0m | \u001b[0m 4.804   \u001b[0m | \u001b[0m 0.7137  \u001b[0m | \u001b[0m 0.6219  \u001b[0m | \u001b[0m 0.6384  \u001b[0m | \u001b[0m 0.3819  \u001b[0m | \u001b[0m 0.0572  \u001b[0m | \u001b[0m 54.5    \u001b[0m | \u001b[0m 15.56   \u001b[0m | \u001b[0m 0.003076\u001b[0m | \u001b[0m 71.18   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.725   \u001b[0m | \u001b[0m 5.228   \u001b[0m | \u001b[0m 0.7168  \u001b[0m | \u001b[0m 0.8466  \u001b[0m | \u001b[0m 1.163   \u001b[0m | \u001b[0m 2.028   \u001b[0m | \u001b[0m 0.1744  \u001b[0m | \u001b[0m 63.81   \u001b[0m | \u001b[0m 75.23   \u001b[0m | \u001b[0m 0.000431\u001b[0m | \u001b[0m 55.99   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.9406  \u001b[0m | \u001b[0m 0.623   \u001b[0m | \u001b[0m 2.33    \u001b[0m | \u001b[0m 0.7067  \u001b[0m | \u001b[0m 0.406   \u001b[0m | \u001b[0m 2.553   \u001b[0m | \u001b[0m 1.671   \u001b[0m | \u001b[0m 0.08346 \u001b[0m | \u001b[0m 67.4    \u001b[0m | \u001b[0m 78.65   \u001b[0m | \u001b[0m 0.00256 \u001b[0m | \u001b[0m 52.34   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.5074  \u001b[0m | \u001b[0m 6.944   \u001b[0m | \u001b[0m 0.9997  \u001b[0m | \u001b[0m 0.4196  \u001b[0m | \u001b[0m 0.414   \u001b[0m | \u001b[0m 0.631   \u001b[0m | \u001b[0m 0.01833 \u001b[0m | \u001b[0m 36.66   \u001b[0m | \u001b[0m 23.32   \u001b[0m | \u001b[0m 0.004937\u001b[0m | \u001b[0m 50.57   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.7159  \u001b[0m | \u001b[0m 1.611   \u001b[0m | \u001b[0m 0.6049  \u001b[0m | \u001b[0m 0.791   \u001b[0m | \u001b[0m 1.843   \u001b[0m | \u001b[0m 0.6592  \u001b[0m | \u001b[0m 0.07487 \u001b[0m | \u001b[0m 32.23   \u001b[0m | \u001b[0m 11.89   \u001b[0m | \u001b[0m 0.001321\u001b[0m | \u001b[0m 58.13   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 0.6904  \u001b[0m | \u001b[0m 5.418   \u001b[0m | \u001b[0m 0.9955  \u001b[0m | \u001b[0m 0.7591  \u001b[0m | \u001b[0m 0.9165  \u001b[0m | \u001b[0m 2.561   \u001b[0m | \u001b[0m 0.1518  \u001b[0m | \u001b[0m 35.11   \u001b[0m | \u001b[0m 85.66   \u001b[0m | \u001b[0m 0.002721\u001b[0m | \u001b[0m 58.72   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.7293  \u001b[0m | \u001b[0m 4.113   \u001b[0m | \u001b[0m 0.7512  \u001b[0m | \u001b[0m 0.7392  \u001b[0m | \u001b[0m 1.11    \u001b[0m | \u001b[0m 1.906   \u001b[0m | \u001b[0m 0.1701  \u001b[0m | \u001b[0m 66.83   \u001b[0m | \u001b[0m 48.89   \u001b[0m | \u001b[0m 0.007924\u001b[0m | \u001b[0m 29.01   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.5736  \u001b[0m | \u001b[0m 9.884   \u001b[0m | \u001b[0m 0.5748  \u001b[0m | \u001b[0m 0.7238  \u001b[0m | \u001b[0m 1.359   \u001b[0m | \u001b[0m 0.2413  \u001b[0m | \u001b[0m 0.1138  \u001b[0m | \u001b[0m 77.57   \u001b[0m | \u001b[0m 77.76   \u001b[0m | \u001b[0m 0.007243\u001b[0m | \u001b[0m 49.41   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.5295  \u001b[0m | \u001b[0m 4.976   \u001b[0m | \u001b[0m 0.7713  \u001b[0m | \u001b[0m 0.7649  \u001b[0m | \u001b[0m 1.309   \u001b[0m | \u001b[0m 2.35    \u001b[0m | \u001b[0m 0.1631  \u001b[0m | \u001b[0m 63.13   \u001b[0m | \u001b[0m 29.64   \u001b[0m | \u001b[0m 0.0018  \u001b[0m | \u001b[0m 70.94   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.6405  \u001b[0m | \u001b[0m 8.335   \u001b[0m | \u001b[0m 0.5132  \u001b[0m | \u001b[0m 0.8005  \u001b[0m | \u001b[0m 1.425   \u001b[0m | \u001b[0m 2.336   \u001b[0m | \u001b[0m 0.1134  \u001b[0m | \u001b[0m 45.94   \u001b[0m | \u001b[0m 87.22   \u001b[0m | \u001b[0m 0.000316\u001b[0m | \u001b[0m 96.9    \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.9836  \u001b[0m | \u001b[0m 3.085   \u001b[0m | \u001b[0m 0.6464  \u001b[0m | \u001b[0m 0.6654  \u001b[0m | \u001b[0m 1.253   \u001b[0m | \u001b[0m 0.5632  \u001b[0m | \u001b[0m 0.1334  \u001b[0m | \u001b[0m 38.11   \u001b[0m | \u001b[0m 75.76   \u001b[0m | \u001b[0m 0.007374\u001b[0m | \u001b[0m 97.71   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.9532  \u001b[0m | \u001b[0m 2.398   \u001b[0m | \u001b[0m 0.7831  \u001b[0m | \u001b[0m 0.904   \u001b[0m | \u001b[0m 0.1429  \u001b[0m | \u001b[0m 0.1365  \u001b[0m | \u001b[0m 0.03044 \u001b[0m | \u001b[0m 84.3    \u001b[0m | \u001b[0m 44.92   \u001b[0m | \u001b[0m 0.004304\u001b[0m | \u001b[0m 45.64   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.9308  \u001b[0m | \u001b[0m 2.347   \u001b[0m | \u001b[0m 0.9591  \u001b[0m | \u001b[0m 0.6448  \u001b[0m | \u001b[0m 1.975   \u001b[0m | \u001b[0m 1.788   \u001b[0m | \u001b[0m 0.05734 \u001b[0m | \u001b[0m 80.68   \u001b[0m | \u001b[0m 81.65   \u001b[0m | \u001b[0m 0.00223 \u001b[0m | \u001b[0m 73.89   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.5624  \u001b[0m | \u001b[0m 4.288   \u001b[0m | \u001b[0m 0.449   \u001b[0m | \u001b[0m 0.4106  \u001b[0m | \u001b[0m 2.727   \u001b[0m | \u001b[0m 2.3     \u001b[0m | \u001b[0m 0.1525  \u001b[0m | \u001b[0m 14.49   \u001b[0m | \u001b[0m 32.78   \u001b[0m | \u001b[0m 0.002295\u001b[0m | \u001b[0m 31.11   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.7043  \u001b[0m | \u001b[0m 6.37    \u001b[0m | \u001b[0m 0.7816  \u001b[0m | \u001b[0m 0.4805  \u001b[0m | \u001b[0m 2.916   \u001b[0m | \u001b[0m 2.623   \u001b[0m | \u001b[0m 0.09058 \u001b[0m | \u001b[0m 60.24   \u001b[0m | \u001b[0m 50.34   \u001b[0m | \u001b[0m 0.007119\u001b[0m | \u001b[0m 57.86   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.7755  \u001b[0m | \u001b[0m 2.256   \u001b[0m | \u001b[0m 0.8102  \u001b[0m | \u001b[0m 0.9664  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 0.5522  \u001b[0m | \u001b[0m 0.1486  \u001b[0m | \u001b[0m 50.34   \u001b[0m | \u001b[0m 61.54   \u001b[0m | \u001b[0m 0.004782\u001b[0m | \u001b[0m 33.11   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.8567  \u001b[0m | \u001b[0m 4.064   \u001b[0m | \u001b[0m 0.9296  \u001b[0m | \u001b[0m 0.5276  \u001b[0m | \u001b[0m 2.839   \u001b[0m | \u001b[0m 0.89    \u001b[0m | \u001b[0m 0.02859 \u001b[0m | \u001b[0m 33.11   \u001b[0m | \u001b[0m 81.41   \u001b[0m | \u001b[0m 0.005984\u001b[0m | \u001b[0m 82.64   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.9137  \u001b[0m | \u001b[0m 6.908   \u001b[0m | \u001b[0m 0.4354  \u001b[0m | \u001b[0m 0.5805  \u001b[0m | \u001b[0m 0.01457 \u001b[0m | \u001b[0m 2.049   \u001b[0m | \u001b[0m 0.1924  \u001b[0m | \u001b[0m 86.44   \u001b[0m | \u001b[0m 26.97   \u001b[0m | \u001b[0m 0.00338 \u001b[0m | \u001b[0m 88.57   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.934   \u001b[0m | \u001b[0m 0.908   \u001b[0m | \u001b[0m 7.874   \u001b[0m | \u001b[0m 0.4406  \u001b[0m | \u001b[0m 0.4385  \u001b[0m | \u001b[0m 2.105   \u001b[0m | \u001b[0m 1.16    \u001b[0m | \u001b[0m 0.03198 \u001b[0m | \u001b[0m 92.43   \u001b[0m | \u001b[0m 55.62   \u001b[0m | \u001b[0m 0.005052\u001b[0m | \u001b[0m 62.78   \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.933   \u001b[0m | \u001b[0m 2.433   \u001b[0m | \u001b[0m 0.6266  \u001b[0m | \u001b[0m 0.4494  \u001b[0m | \u001b[0m 2.508   \u001b[0m | \u001b[0m 2.28    \u001b[0m | \u001b[0m 0.01142 \u001b[0m | \u001b[0m 44.16   \u001b[0m | \u001b[0m 94.61   \u001b[0m | \u001b[0m 0.001947\u001b[0m | \u001b[0m 64.51   \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.9408  \u001b[0m | \u001b[0m 0.7371  \u001b[0m | \u001b[0m 1.967   \u001b[0m | \u001b[0m 0.9127  \u001b[0m | \u001b[0m 0.9268  \u001b[0m | \u001b[0m 1.936   \u001b[0m | \u001b[0m 2.886   \u001b[0m | \u001b[0m 0.03203 \u001b[0m | \u001b[0m 97.47   \u001b[0m | \u001b[0m 65.93   \u001b[0m | \u001b[0m 0.008361\u001b[0m | \u001b[0m 38.89   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.9319  \u001b[0m | \u001b[0m 0.6624  \u001b[0m | \u001b[0m 2.592   \u001b[0m | \u001b[0m 0.4991  \u001b[0m | \u001b[0m 0.5053  \u001b[0m | \u001b[0m 2.703   \u001b[0m | \u001b[0m 0.8437  \u001b[0m | \u001b[0m 0.1901  \u001b[0m | \u001b[0m 19.72   \u001b[0m | \u001b[0m 76.83   \u001b[0m | \u001b[0m 0.00704 \u001b[0m | \u001b[0m 71.13   \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 0.4481  \u001b[0m | \u001b[0m 3.877   \u001b[0m | \u001b[0m 0.475   \u001b[0m | \u001b[0m 0.8934  \u001b[0m | \u001b[0m 2.284   \u001b[0m | \u001b[0m 0.09352 \u001b[0m | \u001b[0m 0.04547 \u001b[0m | \u001b[0m 91.9    \u001b[0m | \u001b[0m 65.03   \u001b[0m | \u001b[0m 0.006493\u001b[0m | \u001b[0m 38.55   \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.7476  \u001b[0m | \u001b[0m 4.012   \u001b[0m | \u001b[0m 0.6016  \u001b[0m | \u001b[0m 0.9502  \u001b[0m | \u001b[0m 0.7384  \u001b[0m | \u001b[0m 0.2577  \u001b[0m | \u001b[0m 0.05618 \u001b[0m | \u001b[0m 80.39   \u001b[0m | \u001b[0m 65.17   \u001b[0m | \u001b[0m 0.003252\u001b[0m | \u001b[0m 87.47   \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.5984  \u001b[0m | \u001b[0m 3.66    \u001b[0m | \u001b[0m 0.4521  \u001b[0m | \u001b[0m 0.8496  \u001b[0m | \u001b[0m 2.849   \u001b[0m | \u001b[0m 0.3928  \u001b[0m | \u001b[0m 0.04674 \u001b[0m | \u001b[0m 20.77   \u001b[0m | \u001b[0m 88.7    \u001b[0m | \u001b[0m 0.005331\u001b[0m | \u001b[0m 45.91   \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.5435  \u001b[0m | \u001b[0m 7.404   \u001b[0m | \u001b[0m 0.4305  \u001b[0m | \u001b[0m 0.4272  \u001b[0m | \u001b[0m 1.097   \u001b[0m | \u001b[0m 1.787   \u001b[0m | \u001b[0m 0.01385 \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 54.66   \u001b[0m | \u001b[0m 0.002805\u001b[0m | \u001b[0m 94.07   \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 5.313   \u001b[0m | \u001b[0m 0.6665  \u001b[0m | \u001b[0m 0.6134  \u001b[0m | \u001b[0m 2.78    \u001b[0m | \u001b[0m 1.866   \u001b[0m | \u001b[0m 0.0291  \u001b[0m | \u001b[0m 22.46   \u001b[0m | \u001b[0m 26.96   \u001b[0m | \u001b[0m 0.009938\u001b[0m | \u001b[0m 58.71   \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.732   \u001b[0m | \u001b[0m 7.903   \u001b[0m | \u001b[0m 0.4217  \u001b[0m | \u001b[0m 0.8647  \u001b[0m | \u001b[0m 0.7015  \u001b[0m | \u001b[0m 0.5163  \u001b[0m | \u001b[0m 0.1487  \u001b[0m | \u001b[0m 40.04   \u001b[0m | \u001b[0m 16.15   \u001b[0m | \u001b[0m 0.00593 \u001b[0m | \u001b[0m 97.78   \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.6715  \u001b[0m | \u001b[0m 3.562   \u001b[0m | \u001b[0m 0.8593  \u001b[0m | \u001b[0m 0.7726  \u001b[0m | \u001b[0m 1.327   \u001b[0m | \u001b[0m 0.775   \u001b[0m | \u001b[0m 0.04333 \u001b[0m | \u001b[0m 26.26   \u001b[0m | \u001b[0m 65.73   \u001b[0m | \u001b[0m 0.004788\u001b[0m | \u001b[0m 56.4    \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.9039  \u001b[0m | \u001b[0m 8.871   \u001b[0m | \u001b[0m 0.7519  \u001b[0m | \u001b[0m 0.7115  \u001b[0m | \u001b[0m 2.245   \u001b[0m | \u001b[0m 1.133   \u001b[0m | \u001b[0m 0.1504  \u001b[0m | \u001b[0m 85.67   \u001b[0m | \u001b[0m 73.08   \u001b[0m | \u001b[0m 0.005145\u001b[0m | \u001b[0m 74.66   \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.7246  \u001b[0m | \u001b[0m 1.839   \u001b[0m | \u001b[0m 0.7039  \u001b[0m | \u001b[0m 0.8481  \u001b[0m | \u001b[0m 0.2956  \u001b[0m | \u001b[0m 2.869   \u001b[0m | \u001b[0m 0.1168  \u001b[0m | \u001b[0m 48.97   \u001b[0m | \u001b[0m 38.65   \u001b[0m | \u001b[0m 0.0045  \u001b[0m | \u001b[0m 76.97   \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.94    \u001b[0m | \u001b[0m 0.448   \u001b[0m | \u001b[0m 8.856   \u001b[0m | \u001b[0m 0.5697  \u001b[0m | \u001b[0m 0.8485  \u001b[0m | \u001b[0m 1.822   \u001b[0m | \u001b[0m 1.364   \u001b[0m | \u001b[0m 0.01467 \u001b[0m | \u001b[0m 46.77   \u001b[0m | \u001b[0m 55.25   \u001b[0m | \u001b[0m 0.00347 \u001b[0m | \u001b[0m 37.93   \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.729   \u001b[0m | \u001b[0m 1.734   \u001b[0m | \u001b[0m 0.5608  \u001b[0m | \u001b[0m 0.8934  \u001b[0m | \u001b[0m 2.702   \u001b[0m | \u001b[0m 2.972   \u001b[0m | \u001b[0m 0.1189  \u001b[0m | \u001b[0m 90.92   \u001b[0m | \u001b[0m 74.03   \u001b[0m | \u001b[0m 0.001802\u001b[0m | \u001b[0m 63.53   \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.9361  \u001b[0m | \u001b[0m 0.8549  \u001b[0m | \u001b[0m 4.759   \u001b[0m | \u001b[0m 0.5479  \u001b[0m | \u001b[0m 0.4569  \u001b[0m | \u001b[0m 1.558   \u001b[0m | \u001b[0m 1.329   \u001b[0m | \u001b[0m 0.09881 \u001b[0m | \u001b[0m 84.21   \u001b[0m | \u001b[0m 12.69   \u001b[0m | \u001b[0m 0.009021\u001b[0m | \u001b[0m 84.01   \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.825   \u001b[0m | \u001b[0m 4.237   \u001b[0m | \u001b[0m 0.4219  \u001b[0m | \u001b[0m 0.871   \u001b[0m | \u001b[0m 2.277   \u001b[0m | \u001b[0m 1.909   \u001b[0m | \u001b[0m 0.1191  \u001b[0m | \u001b[0m 96.14   \u001b[0m | \u001b[0m 48.4    \u001b[0m | \u001b[0m 0.008816\u001b[0m | \u001b[0m 28.0    \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.9388  \u001b[0m | \u001b[0m 0.8698  \u001b[0m | \u001b[0m 7.448   \u001b[0m | \u001b[0m 0.7706  \u001b[0m | \u001b[0m 0.7892  \u001b[0m | \u001b[0m 2.615   \u001b[0m | \u001b[0m 2.615   \u001b[0m | \u001b[0m 0.01488 \u001b[0m | \u001b[0m 26.94   \u001b[0m | \u001b[0m 54.26   \u001b[0m | \u001b[0m 0.000968\u001b[0m | \u001b[0m 41.14   \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.9374  \u001b[0m | \u001b[0m 5.251   \u001b[0m | \u001b[0m 0.4359  \u001b[0m | \u001b[0m 0.8456  \u001b[0m | \u001b[0m 1.256   \u001b[0m | \u001b[0m 2.588   \u001b[0m | \u001b[0m 0.01293 \u001b[0m | \u001b[0m 42.89   \u001b[0m | \u001b[0m 41.98   \u001b[0m | \u001b[0m 0.006783\u001b[0m | \u001b[0m 47.74   \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.9951  \u001b[0m | \u001b[0m 1.72    \u001b[0m | \u001b[0m 0.5715  \u001b[0m | \u001b[0m 0.4221  \u001b[0m | \u001b[0m 1.057   \u001b[0m | \u001b[0m 1.017   \u001b[0m | \u001b[0m 0.119   \u001b[0m | \u001b[0m 29.84   \u001b[0m | \u001b[0m 62.29   \u001b[0m | \u001b[0m 0.000980\u001b[0m | \u001b[0m 57.23   \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.5679  \u001b[0m | \u001b[0m 4.389   \u001b[0m | \u001b[0m 0.9728  \u001b[0m | \u001b[0m 0.6498  \u001b[0m | \u001b[0m 2.782   \u001b[0m | \u001b[0m 1.801   \u001b[0m | \u001b[0m 0.07963 \u001b[0m | \u001b[0m 78.45   \u001b[0m | \u001b[0m 66.64   \u001b[0m | \u001b[0m 0.001012\u001b[0m | \u001b[0m 23.26   \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.9317  \u001b[0m | \u001b[0m 4.622   \u001b[0m | \u001b[0m 0.989   \u001b[0m | \u001b[0m 0.4439  \u001b[0m | \u001b[0m 1.397   \u001b[0m | \u001b[0m 0.09408 \u001b[0m | \u001b[0m 0.1579  \u001b[0m | \u001b[0m 11.69   \u001b[0m | \u001b[0m 91.31   \u001b[0m | \u001b[0m 0.003233\u001b[0m | \u001b[0m 45.43   \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.8977  \u001b[0m | \u001b[0m 7.556   \u001b[0m | \u001b[0m 0.7456  \u001b[0m | \u001b[0m 0.5512  \u001b[0m | \u001b[0m 1.484   \u001b[0m | \u001b[0m 0.1908  \u001b[0m | \u001b[0m 0.09396 \u001b[0m | \u001b[0m 55.34   \u001b[0m | \u001b[0m 64.23   \u001b[0m | \u001b[0m 0.008724\u001b[0m | \u001b[0m 86.62   \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.8127  \u001b[0m | \u001b[0m 4.871   \u001b[0m | \u001b[0m 0.6533  \u001b[0m | \u001b[0m 0.5265  \u001b[0m | \u001b[0m 2.934   \u001b[0m | \u001b[0m 1.432   \u001b[0m | \u001b[0m 0.1594  \u001b[0m | \u001b[0m 50.47   \u001b[0m | \u001b[0m 14.31   \u001b[0m | \u001b[0m 0.007018\u001b[0m | \u001b[0m 62.96   \u001b[0m |\n",
      "| \u001b[95m 84      \u001b[0m | \u001b[95m 0.9443  \u001b[0m | \u001b[95m 0.6545  \u001b[0m | \u001b[95m 2.154   \u001b[0m | \u001b[95m 0.5021  \u001b[0m | \u001b[95m 0.7858  \u001b[0m | \u001b[95m 1.552   \u001b[0m | \u001b[95m 1.324   \u001b[0m | \u001b[95m 0.1785  \u001b[0m | \u001b[95m 18.6    \u001b[0m | \u001b[95m 34.7    \u001b[0m | \u001b[95m 0.002475\u001b[0m | \u001b[95m 63.99   \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.5763  \u001b[0m | \u001b[0m 8.655   \u001b[0m | \u001b[0m 0.4838  \u001b[0m | \u001b[0m 0.7068  \u001b[0m | \u001b[0m 0.7622  \u001b[0m | \u001b[0m 2.997   \u001b[0m | \u001b[0m 0.1923  \u001b[0m | \u001b[0m 60.19   \u001b[0m | \u001b[0m 16.23   \u001b[0m | \u001b[0m 0.007694\u001b[0m | \u001b[0m 91.55   \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.6824  \u001b[0m | \u001b[0m 5.087   \u001b[0m | \u001b[0m 0.5596  \u001b[0m | \u001b[0m 0.7148  \u001b[0m | \u001b[0m 1.008   \u001b[0m | \u001b[0m 2.588   \u001b[0m | \u001b[0m 0.06917 \u001b[0m | \u001b[0m 15.81   \u001b[0m | \u001b[0m 70.49   \u001b[0m | \u001b[0m 0.005488\u001b[0m | \u001b[0m 88.76   \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.8452  \u001b[0m | \u001b[0m 5.398   \u001b[0m | \u001b[0m 0.5207  \u001b[0m | \u001b[0m 0.7268  \u001b[0m | \u001b[0m 0.06448 \u001b[0m | \u001b[0m 1.969   \u001b[0m | \u001b[0m 0.1982  \u001b[0m | \u001b[0m 53.07   \u001b[0m | \u001b[0m 52.74   \u001b[0m | \u001b[0m 0.004125\u001b[0m | \u001b[0m 75.45   \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.9876  \u001b[0m | \u001b[0m 2.378   \u001b[0m | \u001b[0m 0.7691  \u001b[0m | \u001b[0m 0.4583  \u001b[0m | \u001b[0m 2.667   \u001b[0m | \u001b[0m 2.714   \u001b[0m | \u001b[0m 0.1661  \u001b[0m | \u001b[0m 36.22   \u001b[0m | \u001b[0m 18.48   \u001b[0m | \u001b[0m 0.007048\u001b[0m | \u001b[0m 38.02   \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.6462  \u001b[0m | \u001b[0m 3.104   \u001b[0m | \u001b[0m 0.4985  \u001b[0m | \u001b[0m 0.4169  \u001b[0m | \u001b[0m 1.266   \u001b[0m | \u001b[0m 1.257   \u001b[0m | \u001b[0m 0.02677 \u001b[0m | \u001b[0m 40.87   \u001b[0m | \u001b[0m 11.57   \u001b[0m | \u001b[0m 0.007162\u001b[0m | \u001b[0m 67.84   \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.6496  \u001b[0m | \u001b[0m 8.607   \u001b[0m | \u001b[0m 0.5451  \u001b[0m | \u001b[0m 0.6079  \u001b[0m | \u001b[0m 0.7737  \u001b[0m | \u001b[0m 2.325   \u001b[0m | \u001b[0m 0.0889  \u001b[0m | \u001b[0m 19.45   \u001b[0m | \u001b[0m 66.33   \u001b[0m | \u001b[0m 0.002954\u001b[0m | \u001b[0m 46.08   \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.975   \u001b[0m | \u001b[0m 5.401   \u001b[0m | \u001b[0m 0.5894  \u001b[0m | \u001b[0m 0.9608  \u001b[0m | \u001b[0m 1.411   \u001b[0m | \u001b[0m 2.697   \u001b[0m | \u001b[0m 0.05184 \u001b[0m | \u001b[0m 27.73   \u001b[0m | \u001b[0m 85.49   \u001b[0m | \u001b[0m 0.001117\u001b[0m | \u001b[0m 61.64   \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 0.9399  \u001b[0m | \u001b[0m 0.8696  \u001b[0m | \u001b[0m 8.277   \u001b[0m | \u001b[0m 0.5498  \u001b[0m | \u001b[0m 0.9758  \u001b[0m | \u001b[0m 0.5663  \u001b[0m | \u001b[0m 0.8518  \u001b[0m | \u001b[0m 0.1537  \u001b[0m | \u001b[0m 53.38   \u001b[0m | \u001b[0m 96.53   \u001b[0m | \u001b[0m 0.009332\u001b[0m | \u001b[0m 54.79   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.8178  \u001b[0m | \u001b[0m 7.634   \u001b[0m | \u001b[0m 0.4224  \u001b[0m | \u001b[0m 0.758   \u001b[0m | \u001b[0m 2.431   \u001b[0m | \u001b[0m 2.952   \u001b[0m | \u001b[0m 0.1308  \u001b[0m | \u001b[0m 82.12   \u001b[0m | \u001b[0m 75.13   \u001b[0m | \u001b[0m 0.002414\u001b[0m | \u001b[0m 46.3    \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 0.938   \u001b[0m | \u001b[0m 0.6695  \u001b[0m | \u001b[0m 9.861   \u001b[0m | \u001b[0m 0.482   \u001b[0m | \u001b[0m 0.9956  \u001b[0m | \u001b[0m 2.939   \u001b[0m | \u001b[0m 0.5758  \u001b[0m | \u001b[0m 0.0902  \u001b[0m | \u001b[0m 14.56   \u001b[0m | \u001b[0m 78.6    \u001b[0m | \u001b[0m 0.008348\u001b[0m | \u001b[0m 51.03   \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.6849  \u001b[0m | \u001b[0m 6.495   \u001b[0m | \u001b[0m 0.8199  \u001b[0m | \u001b[0m 0.6716  \u001b[0m | \u001b[0m 1.465   \u001b[0m | \u001b[0m 0.2926  \u001b[0m | \u001b[0m 0.1849  \u001b[0m | \u001b[0m 90.47   \u001b[0m | \u001b[0m 71.08   \u001b[0m | \u001b[0m 0.007504\u001b[0m | \u001b[0m 21.85   \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.6743  \u001b[0m | \u001b[0m 3.624   \u001b[0m | \u001b[0m 0.4161  \u001b[0m | \u001b[0m 0.6124  \u001b[0m | \u001b[0m 1.724   \u001b[0m | \u001b[0m 1.303   \u001b[0m | \u001b[0m 0.142   \u001b[0m | \u001b[0m 99.03   \u001b[0m | \u001b[0m 83.83   \u001b[0m | \u001b[0m 0.00728 \u001b[0m | \u001b[0m 71.16   \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.6138  \u001b[0m | \u001b[0m 7.911   \u001b[0m | \u001b[0m 0.9883  \u001b[0m | \u001b[0m 0.6315  \u001b[0m | \u001b[0m 2.519   \u001b[0m | \u001b[0m 0.7721  \u001b[0m | \u001b[0m 0.1459  \u001b[0m | \u001b[0m 43.23   \u001b[0m | \u001b[0m 47.87   \u001b[0m | \u001b[0m 0.009637\u001b[0m | \u001b[0m 77.24   \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.6674  \u001b[0m | \u001b[0m 1.859   \u001b[0m | \u001b[0m 0.6697  \u001b[0m | \u001b[0m 0.5181  \u001b[0m | \u001b[0m 2.607   \u001b[0m | \u001b[0m 1.426   \u001b[0m | \u001b[0m 0.1447  \u001b[0m | \u001b[0m 65.02   \u001b[0m | \u001b[0m 51.05   \u001b[0m | \u001b[0m 0.005685\u001b[0m | \u001b[0m 38.43   \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.7076  \u001b[0m | \u001b[0m 3.42    \u001b[0m | \u001b[0m 0.8928  \u001b[0m | \u001b[0m 0.8904  \u001b[0m | \u001b[0m 0.964   \u001b[0m | \u001b[0m 2.141   \u001b[0m | \u001b[0m 0.1702  \u001b[0m | \u001b[0m 14.74   \u001b[0m | \u001b[0m 34.67   \u001b[0m | \u001b[0m 0.006894\u001b[0m | \u001b[0m 69.93   \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.8468  \u001b[0m | \u001b[0m 6.382   \u001b[0m | \u001b[0m 0.5078  \u001b[0m | \u001b[0m 0.6735  \u001b[0m | \u001b[0m 0.8478  \u001b[0m | \u001b[0m 0.124   \u001b[0m | \u001b[0m 0.08796 \u001b[0m | \u001b[0m 48.43   \u001b[0m | \u001b[0m 23.25   \u001b[0m | \u001b[0m 0.003153\u001b[0m | \u001b[0m 74.98   \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m 0.9405  \u001b[0m | \u001b[0m 0.6337  \u001b[0m | \u001b[0m 8.763   \u001b[0m | \u001b[0m 0.9325  \u001b[0m | \u001b[0m 0.4704  \u001b[0m | \u001b[0m 2.591   \u001b[0m | \u001b[0m 0.4252  \u001b[0m | \u001b[0m 0.1259  \u001b[0m | \u001b[0m 8.384   \u001b[0m | \u001b[0m 68.96   \u001b[0m | \u001b[0m 0.004801\u001b[0m | \u001b[0m 79.85   \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.5748  \u001b[0m | \u001b[0m 1.543   \u001b[0m | \u001b[0m 0.6173  \u001b[0m | \u001b[0m 0.4329  \u001b[0m | \u001b[0m 1.384   \u001b[0m | \u001b[0m 0.01756 \u001b[0m | \u001b[0m 0.1464  \u001b[0m | \u001b[0m 59.48   \u001b[0m | \u001b[0m 31.31   \u001b[0m | \u001b[0m 0.005651\u001b[0m | \u001b[0m 84.89   \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.554   \u001b[0m | \u001b[0m 5.277   \u001b[0m | \u001b[0m 0.4342  \u001b[0m | \u001b[0m 0.8802  \u001b[0m | \u001b[0m 2.782   \u001b[0m | \u001b[0m 1.098   \u001b[0m | \u001b[0m 0.1268  \u001b[0m | \u001b[0m 49.43   \u001b[0m | \u001b[0m 43.87   \u001b[0m | \u001b[0m 0.006168\u001b[0m | \u001b[0m 35.34   \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m 0.9394  \u001b[0m | \u001b[0m 0.4422  \u001b[0m | \u001b[0m 5.848   \u001b[0m | \u001b[0m 0.7508  \u001b[0m | \u001b[0m 0.693   \u001b[0m | \u001b[0m 2.534   \u001b[0m | \u001b[0m 1.947   \u001b[0m | \u001b[0m 0.1728  \u001b[0m | \u001b[0m 80.9    \u001b[0m | \u001b[0m 98.91   \u001b[0m | \u001b[0m 0.009616\u001b[0m | \u001b[0m 66.08   \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 0.5508  \u001b[0m | \u001b[0m 3.976   \u001b[0m | \u001b[0m 0.9035  \u001b[0m | \u001b[0m 0.5424  \u001b[0m | \u001b[0m 0.3088  \u001b[0m | \u001b[0m 0.06136 \u001b[0m | \u001b[0m 0.1962  \u001b[0m | \u001b[0m 79.47   \u001b[0m | \u001b[0m 14.89   \u001b[0m | \u001b[0m 0.003183\u001b[0m | \u001b[0m 55.21   \u001b[0m |\n",
      "| \u001b[0m 106     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.7528  \u001b[0m | \u001b[0m 5.681   \u001b[0m | \u001b[0m 0.9322  \u001b[0m | \u001b[0m 0.5691  \u001b[0m | \u001b[0m 1.713   \u001b[0m | \u001b[0m 0.6872  \u001b[0m | \u001b[0m 0.1163  \u001b[0m | \u001b[0m 81.79   \u001b[0m | \u001b[0m 25.52   \u001b[0m | \u001b[0m 0.001437\u001b[0m | \u001b[0m 47.02   \u001b[0m |\n",
      "| \u001b[0m 107     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.5126  \u001b[0m | \u001b[0m 1.797   \u001b[0m | \u001b[0m 0.8126  \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.8503  \u001b[0m | \u001b[0m 0.3342  \u001b[0m | \u001b[0m 0.1471  \u001b[0m | \u001b[0m 41.35   \u001b[0m | \u001b[0m 34.61   \u001b[0m | \u001b[0m 0.005026\u001b[0m | \u001b[0m 37.76   \u001b[0m |\n",
      "| \u001b[0m 108     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.5079  \u001b[0m | \u001b[0m 5.796   \u001b[0m | \u001b[0m 0.5746  \u001b[0m | \u001b[0m 0.7411  \u001b[0m | \u001b[0m 1.166   \u001b[0m | \u001b[0m 2.297   \u001b[0m | \u001b[0m 0.174   \u001b[0m | \u001b[0m 72.95   \u001b[0m | \u001b[0m 37.43   \u001b[0m | \u001b[0m 0.008858\u001b[0m | \u001b[0m 79.73   \u001b[0m |\n",
      "| \u001b[0m 109     \u001b[0m | \u001b[0m 0.9402  \u001b[0m | \u001b[0m 0.8762  \u001b[0m | \u001b[0m 8.754   \u001b[0m | \u001b[0m 0.9003  \u001b[0m | \u001b[0m 0.9009  \u001b[0m | \u001b[0m 2.527   \u001b[0m | \u001b[0m 2.54    \u001b[0m | \u001b[0m 0.04553 \u001b[0m | \u001b[0m 35.17   \u001b[0m | \u001b[0m 13.35   \u001b[0m | \u001b[0m 0.002775\u001b[0m | \u001b[0m 92.74   \u001b[0m |\n",
      "| \u001b[0m 110     \u001b[0m | \u001b[0m 0.9348  \u001b[0m | \u001b[0m 0.9886  \u001b[0m | \u001b[0m 9.746   \u001b[0m | \u001b[0m 0.6119  \u001b[0m | \u001b[0m 0.4576  \u001b[0m | \u001b[0m 2.839   \u001b[0m | \u001b[0m 2.063   \u001b[0m | \u001b[0m 0.03213 \u001b[0m | \u001b[0m 40.75   \u001b[0m | \u001b[0m 39.04   \u001b[0m | \u001b[0m 0.002108\u001b[0m | \u001b[0m 54.47   \u001b[0m |\n",
      "| \u001b[0m 111     \u001b[0m | \u001b[0m 0.9411  \u001b[0m | \u001b[0m 0.9618  \u001b[0m | \u001b[0m 4.167   \u001b[0m | \u001b[0m 0.6687  \u001b[0m | \u001b[0m 0.9501  \u001b[0m | \u001b[0m 2.076   \u001b[0m | \u001b[0m 2.442   \u001b[0m | \u001b[0m 0.1972  \u001b[0m | \u001b[0m 94.58   \u001b[0m | \u001b[0m 61.48   \u001b[0m | \u001b[0m 0.009446\u001b[0m | \u001b[0m 38.29   \u001b[0m |\n",
      "| \u001b[0m 112     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.9183  \u001b[0m | \u001b[0m 6.003   \u001b[0m | \u001b[0m 0.527   \u001b[0m | \u001b[0m 0.4121  \u001b[0m | \u001b[0m 1.737   \u001b[0m | \u001b[0m 1.504   \u001b[0m | \u001b[0m 0.1233  \u001b[0m | \u001b[0m 28.72   \u001b[0m | \u001b[0m 20.28   \u001b[0m | \u001b[0m 0.008963\u001b[0m | \u001b[0m 78.74   \u001b[0m |\n",
      "| \u001b[0m 113     \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.8532  \u001b[0m | \u001b[0m 9.059   \u001b[0m | \u001b[0m 0.5879  \u001b[0m | \u001b[0m 0.9816  \u001b[0m | \u001b[0m 0.2238  \u001b[0m | \u001b[0m 2.388   \u001b[0m | \u001b[0m 0.08749 \u001b[0m | \u001b[0m 64.44   \u001b[0m | \u001b[0m 26.45   \u001b[0m | \u001b[0m 0.004636\u001b[0m | \u001b[0m 65.14   \u001b[0m |\n",
      "| \u001b[0m 114     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.5436  \u001b[0m | \u001b[0m 8.984   \u001b[0m | \u001b[0m 0.7096  \u001b[0m | \u001b[0m 0.8381  \u001b[0m | \u001b[0m 1.519   \u001b[0m | \u001b[0m 0.5219  \u001b[0m | \u001b[0m 0.1573  \u001b[0m | \u001b[0m 66.02   \u001b[0m | \u001b[0m 87.36   \u001b[0m | \u001b[0m 0.00905 \u001b[0m | \u001b[0m 31.69   \u001b[0m |\n",
      "| \u001b[0m 115     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 4.498   \u001b[0m | \u001b[0m 0.7751  \u001b[0m | \u001b[0m 0.4077  \u001b[0m | \u001b[0m 2.492   \u001b[0m | \u001b[0m 2.707   \u001b[0m | \u001b[0m 0.06051 \u001b[0m | \u001b[0m 16.45   \u001b[0m | \u001b[0m 27.28   \u001b[0m | \u001b[0m 0.007073\u001b[0m | \u001b[0m 98.03   \u001b[0m |\n",
      "| \u001b[0m 116     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.8607  \u001b[0m | \u001b[0m 6.686   \u001b[0m | \u001b[0m 0.8292  \u001b[0m | \u001b[0m 0.5312  \u001b[0m | \u001b[0m 0.1496  \u001b[0m | \u001b[0m 2.77    \u001b[0m | \u001b[0m 0.1974  \u001b[0m | \u001b[0m 53.73   \u001b[0m | \u001b[0m 79.21   \u001b[0m | \u001b[0m 0.007952\u001b[0m | \u001b[0m 67.47   \u001b[0m |\n",
      "| \u001b[0m 117     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.9849  \u001b[0m | \u001b[0m 2.628   \u001b[0m | \u001b[0m 0.7815  \u001b[0m | \u001b[0m 0.7558  \u001b[0m | \u001b[0m 1.534   \u001b[0m | \u001b[0m 2.782   \u001b[0m | \u001b[0m 0.01395 \u001b[0m | \u001b[0m 33.14   \u001b[0m | \u001b[0m 61.77   \u001b[0m | \u001b[0m 0.004062\u001b[0m | \u001b[0m 67.8    \u001b[0m |\n",
      "| \u001b[0m 118     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.6943  \u001b[0m | \u001b[0m 2.223   \u001b[0m | \u001b[0m 0.9122  \u001b[0m | \u001b[0m 0.6736  \u001b[0m | \u001b[0m 2.748   \u001b[0m | \u001b[0m 2.424   \u001b[0m | \u001b[0m 0.1239  \u001b[0m | \u001b[0m 9.929   \u001b[0m | \u001b[0m 85.14   \u001b[0m | \u001b[0m 0.003023\u001b[0m | \u001b[0m 88.05   \u001b[0m |\n",
      "| \u001b[0m 119     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.6827  \u001b[0m | \u001b[0m 2.823   \u001b[0m | \u001b[0m 0.9112  \u001b[0m | \u001b[0m 0.5484  \u001b[0m | \u001b[0m 1.608   \u001b[0m | \u001b[0m 2.904   \u001b[0m | \u001b[0m 0.1347  \u001b[0m | \u001b[0m 98.99   \u001b[0m | \u001b[0m 76.69   \u001b[0m | \u001b[0m 0.009174\u001b[0m | \u001b[0m 66.93   \u001b[0m |\n",
      "| \u001b[0m 120     \u001b[0m | \u001b[0m 0.9388  \u001b[0m | \u001b[0m 0.6066  \u001b[0m | \u001b[0m 5.035   \u001b[0m | \u001b[0m 0.89    \u001b[0m | \u001b[0m 0.9468  \u001b[0m | \u001b[0m 1.655   \u001b[0m | \u001b[0m 2.462   \u001b[0m | \u001b[0m 0.1232  \u001b[0m | \u001b[0m 69.83   \u001b[0m | \u001b[0m 64.53   \u001b[0m | \u001b[0m 0.009537\u001b[0m | \u001b[0m 76.39   \u001b[0m |\n",
      "| \u001b[0m 121     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.5367  \u001b[0m | \u001b[0m 2.362   \u001b[0m | \u001b[0m 0.9228  \u001b[0m | \u001b[0m 0.7521  \u001b[0m | \u001b[0m 2.944   \u001b[0m | \u001b[0m 0.5931  \u001b[0m | \u001b[0m 0.1886  \u001b[0m | \u001b[0m 62.02   \u001b[0m | \u001b[0m 14.82   \u001b[0m | \u001b[0m 0.004235\u001b[0m | \u001b[0m 71.82   \u001b[0m |\n",
      "| \u001b[0m 122     \u001b[0m | \u001b[0m 0.9411  \u001b[0m | \u001b[0m 0.6397  \u001b[0m | \u001b[0m 6.61    \u001b[0m | \u001b[0m 0.988   \u001b[0m | \u001b[0m 0.4438  \u001b[0m | \u001b[0m 1.955   \u001b[0m | \u001b[0m 1.111   \u001b[0m | \u001b[0m 0.1893  \u001b[0m | \u001b[0m 17.11   \u001b[0m | \u001b[0m 28.87   \u001b[0m | \u001b[0m 0.003733\u001b[0m | \u001b[0m 44.87   \u001b[0m |\n",
      "| \u001b[0m 123     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.9689  \u001b[0m | \u001b[0m 7.434   \u001b[0m | \u001b[0m 0.4056  \u001b[0m | \u001b[0m 0.7965  \u001b[0m | \u001b[0m 2.452   \u001b[0m | \u001b[0m 0.1717  \u001b[0m | \u001b[0m 0.1189  \u001b[0m | \u001b[0m 68.31   \u001b[0m | \u001b[0m 64.53   \u001b[0m | \u001b[0m 0.00809 \u001b[0m | \u001b[0m 29.85   \u001b[0m |\n",
      "| \u001b[0m 124     \u001b[0m | \u001b[0m 0.9326  \u001b[0m | \u001b[0m 0.4899  \u001b[0m | \u001b[0m 8.52    \u001b[0m | \u001b[0m 0.6073  \u001b[0m | \u001b[0m 0.4465  \u001b[0m | \u001b[0m 1.162   \u001b[0m | \u001b[0m 2.915   \u001b[0m | \u001b[0m 0.01215 \u001b[0m | \u001b[0m 55.82   \u001b[0m | \u001b[0m 92.0    \u001b[0m | \u001b[0m 0.000925\u001b[0m | \u001b[0m 48.55   \u001b[0m |\n",
      "| \u001b[0m 125     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.9256  \u001b[0m | \u001b[0m 9.332   \u001b[0m | \u001b[0m 0.7716  \u001b[0m | \u001b[0m 0.7919  \u001b[0m | \u001b[0m 0.9341  \u001b[0m | \u001b[0m 0.2669  \u001b[0m | \u001b[0m 0.1621  \u001b[0m | \u001b[0m 92.73   \u001b[0m | \u001b[0m 90.53   \u001b[0m | \u001b[0m 0.005438\u001b[0m | \u001b[0m 38.33   \u001b[0m |\n",
      "| \u001b[0m 126     \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.6354  \u001b[0m | \u001b[0m 9.637   \u001b[0m | \u001b[0m 0.6065  \u001b[0m | \u001b[0m 0.8987  \u001b[0m | \u001b[0m 1.436   \u001b[0m | \u001b[0m 0.4146  \u001b[0m | \u001b[0m 0.08216 \u001b[0m | \u001b[0m 48.99   \u001b[0m | \u001b[0m 35.11   \u001b[0m | \u001b[0m 0.002688\u001b[0m | \u001b[0m 64.86   \u001b[0m |\n",
      "| \u001b[0m 127     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.6714  \u001b[0m | \u001b[0m 9.453   \u001b[0m | \u001b[0m 0.7003  \u001b[0m | \u001b[0m 0.8828  \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.3025  \u001b[0m | \u001b[0m 0.02746 \u001b[0m | \u001b[0m 19.84   \u001b[0m | \u001b[0m 51.58   \u001b[0m | \u001b[0m 0.002175\u001b[0m | \u001b[0m 56.52   \u001b[0m |\n",
      "| \u001b[0m 128     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.8957  \u001b[0m | \u001b[0m 3.776   \u001b[0m | \u001b[0m 0.8003  \u001b[0m | \u001b[0m 0.4457  \u001b[0m | \u001b[0m 1.707   \u001b[0m | \u001b[0m 0.5296  \u001b[0m | \u001b[0m 0.1533  \u001b[0m | \u001b[0m 74.78   \u001b[0m | \u001b[0m 60.34   \u001b[0m | \u001b[0m 0.008254\u001b[0m | \u001b[0m 65.11   \u001b[0m |\n",
      "| \u001b[0m 129     \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.56    \u001b[0m | \u001b[0m 6.258   \u001b[0m | \u001b[0m 0.8517  \u001b[0m | \u001b[0m 0.8461  \u001b[0m | \u001b[0m 1.695   \u001b[0m | \u001b[0m 1.953   \u001b[0m | \u001b[0m 0.07192 \u001b[0m | \u001b[0m 20.6    \u001b[0m | \u001b[0m 32.42   \u001b[0m | \u001b[0m 0.005036\u001b[0m | \u001b[0m 93.71   \u001b[0m |\n",
      "| \u001b[0m 130     \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.7231  \u001b[0m | \u001b[0m 2.017   \u001b[0m | \u001b[0m 0.8878  \u001b[0m | \u001b[0m 0.6963  \u001b[0m | \u001b[0m 2.731   \u001b[0m | \u001b[0m 1.143   \u001b[0m | \u001b[0m 0.1307  \u001b[0m | \u001b[0m 43.48   \u001b[0m | \u001b[0m 61.71   \u001b[0m | \u001b[0m 0.007738\u001b[0m | \u001b[0m 40.96   \u001b[0m |\n",
      "| \u001b[0m 131     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.929   \u001b[0m | \u001b[0m 2.229   \u001b[0m | \u001b[0m 0.69    \u001b[0m | \u001b[0m 0.708   \u001b[0m | \u001b[0m 2.73    \u001b[0m | \u001b[0m 1.381   \u001b[0m | \u001b[0m 0.05025 \u001b[0m | \u001b[0m 47.76   \u001b[0m | \u001b[0m 55.21   \u001b[0m | \u001b[0m 0.009277\u001b[0m | \u001b[0m 69.16   \u001b[0m |\n",
      "| \u001b[0m 132     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.6413  \u001b[0m | \u001b[0m 6.135   \u001b[0m | \u001b[0m 0.7146  \u001b[0m | \u001b[0m 0.7372  \u001b[0m | \u001b[0m 0.9344  \u001b[0m | \u001b[0m 0.6672  \u001b[0m | \u001b[0m 0.1921  \u001b[0m | \u001b[0m 33.67   \u001b[0m | \u001b[0m 72.35   \u001b[0m | \u001b[0m 0.007095\u001b[0m | \u001b[0m 53.74   \u001b[0m |\n",
      "| \u001b[0m 133     \u001b[0m | \u001b[0m 0.9406  \u001b[0m | \u001b[0m 0.4145  \u001b[0m | \u001b[0m 6.236   \u001b[0m | \u001b[0m 0.6154  \u001b[0m | \u001b[0m 0.8255  \u001b[0m | \u001b[0m 2.824   \u001b[0m | \u001b[0m 0.5586  \u001b[0m | \u001b[0m 0.03712 \u001b[0m | \u001b[0m 64.91   \u001b[0m | \u001b[0m 63.0    \u001b[0m | \u001b[0m 0.009872\u001b[0m | \u001b[0m 91.07   \u001b[0m |\n",
      "| \u001b[0m 134     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.7808  \u001b[0m | \u001b[0m 4.507   \u001b[0m | \u001b[0m 0.5537  \u001b[0m | \u001b[0m 0.7054  \u001b[0m | \u001b[0m 2.635   \u001b[0m | \u001b[0m 2.143   \u001b[0m | \u001b[0m 0.1399  \u001b[0m | \u001b[0m 18.01   \u001b[0m | \u001b[0m 79.1    \u001b[0m | \u001b[0m 0.005729\u001b[0m | \u001b[0m 63.8    \u001b[0m |\n",
      "| \u001b[0m 135     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.697   \u001b[0m | \u001b[0m 3.342   \u001b[0m | \u001b[0m 0.4762  \u001b[0m | \u001b[0m 0.8801  \u001b[0m | \u001b[0m 1.868   \u001b[0m | \u001b[0m 2.009   \u001b[0m | \u001b[0m 0.1804  \u001b[0m | \u001b[0m 30.79   \u001b[0m | \u001b[0m 21.4    \u001b[0m | \u001b[0m 0.003076\u001b[0m | \u001b[0m 45.08   \u001b[0m |\n",
      "| \u001b[0m 136     \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.525   \u001b[0m | \u001b[0m 7.908   \u001b[0m | \u001b[0m 0.5604  \u001b[0m | \u001b[0m 0.8256  \u001b[0m | \u001b[0m 1.654   \u001b[0m | \u001b[0m 0.3754  \u001b[0m | \u001b[0m 0.06914 \u001b[0m | \u001b[0m 58.37   \u001b[0m | \u001b[0m 49.37   \u001b[0m | \u001b[0m 0.006388\u001b[0m | \u001b[0m 48.75   \u001b[0m |\n",
      "| \u001b[0m 137     \u001b[0m | \u001b[0m 0.9388  \u001b[0m | \u001b[0m 0.8103  \u001b[0m | \u001b[0m 3.968   \u001b[0m | \u001b[0m 0.8075  \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 2.128   \u001b[0m | \u001b[0m 2.426   \u001b[0m | \u001b[0m 0.1714  \u001b[0m | \u001b[0m 72.28   \u001b[0m | \u001b[0m 34.21   \u001b[0m | \u001b[0m 0.005636\u001b[0m | \u001b[0m 89.52   \u001b[0m |\n",
      "| \u001b[0m 138     \u001b[0m | \u001b[0m 0.9391  \u001b[0m | \u001b[0m 0.9985  \u001b[0m | \u001b[0m 9.152   \u001b[0m | \u001b[0m 0.9828  \u001b[0m | \u001b[0m 0.8723  \u001b[0m | \u001b[0m 0.2952  \u001b[0m | \u001b[0m 2.831   \u001b[0m | \u001b[0m 0.1479  \u001b[0m | \u001b[0m 55.99   \u001b[0m | \u001b[0m 97.46   \u001b[0m | \u001b[0m 0.006416\u001b[0m | \u001b[0m 38.34   \u001b[0m |\n",
      "| \u001b[0m 139     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.4237  \u001b[0m | \u001b[0m 8.409   \u001b[0m | \u001b[0m 0.502   \u001b[0m | \u001b[0m 0.5644  \u001b[0m | \u001b[0m 0.1384  \u001b[0m | \u001b[0m 1.255   \u001b[0m | \u001b[0m 0.1559  \u001b[0m | \u001b[0m 30.33   \u001b[0m | \u001b[0m 79.05   \u001b[0m | \u001b[0m 0.008296\u001b[0m | \u001b[0m 94.31   \u001b[0m |\n",
      "| \u001b[0m 140     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.8276  \u001b[0m | \u001b[0m 1.432   \u001b[0m | \u001b[0m 0.7422  \u001b[0m | \u001b[0m 0.7376  \u001b[0m | \u001b[0m 0.5951  \u001b[0m | \u001b[0m 2.572   \u001b[0m | \u001b[0m 0.1558  \u001b[0m | \u001b[0m 15.03   \u001b[0m | \u001b[0m 79.6    \u001b[0m | \u001b[0m 0.005888\u001b[0m | \u001b[0m 65.29   \u001b[0m |\n",
      "| \u001b[0m 141     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.6694  \u001b[0m | \u001b[0m 7.939   \u001b[0m | \u001b[0m 0.4395  \u001b[0m | \u001b[0m 0.4052  \u001b[0m | \u001b[0m 1.134   \u001b[0m | \u001b[0m 2.319   \u001b[0m | \u001b[0m 0.1929  \u001b[0m | \u001b[0m 96.07   \u001b[0m | \u001b[0m 40.38   \u001b[0m | \u001b[0m 0.00116 \u001b[0m | \u001b[0m 86.54   \u001b[0m |\n",
      "| \u001b[0m 142     \u001b[0m | \u001b[0m 0.9416  \u001b[0m | \u001b[0m 0.6889  \u001b[0m | \u001b[0m 6.396   \u001b[0m | \u001b[0m 0.8665  \u001b[0m | \u001b[0m 0.8753  \u001b[0m | \u001b[0m 0.6274  \u001b[0m | \u001b[0m 2.313   \u001b[0m | \u001b[0m 0.1357  \u001b[0m | \u001b[0m 62.3    \u001b[0m | \u001b[0m 34.33   \u001b[0m | \u001b[0m 0.004042\u001b[0m | \u001b[0m 56.33   \u001b[0m |\n",
      "| \u001b[0m 143     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.7336  \u001b[0m | \u001b[0m 1.362   \u001b[0m | \u001b[0m 0.6201  \u001b[0m | \u001b[0m 0.6436  \u001b[0m | \u001b[0m 1.52    \u001b[0m | \u001b[0m 1.998   \u001b[0m | \u001b[0m 0.03372 \u001b[0m | \u001b[0m 46.92   \u001b[0m | \u001b[0m 95.14   \u001b[0m | \u001b[0m 0.007474\u001b[0m | \u001b[0m 70.89   \u001b[0m |\n",
      "| \u001b[0m 144     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 7.422   \u001b[0m | \u001b[0m 0.465   \u001b[0m | \u001b[0m 0.7805  \u001b[0m | \u001b[0m 0.3849  \u001b[0m | \u001b[0m 0.9363  \u001b[0m | \u001b[0m 0.1085  \u001b[0m | \u001b[0m 84.6    \u001b[0m | \u001b[0m 23.0    \u001b[0m | \u001b[0m 0.005075\u001b[0m | \u001b[0m 94.96   \u001b[0m |\n",
      "| \u001b[0m 145     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.5958  \u001b[0m | \u001b[0m 8.93    \u001b[0m | \u001b[0m 0.7089  \u001b[0m | \u001b[0m 0.7882  \u001b[0m | \u001b[0m 0.6434  \u001b[0m | \u001b[0m 2.472   \u001b[0m | \u001b[0m 0.04549 \u001b[0m | \u001b[0m 41.69   \u001b[0m | \u001b[0m 88.62   \u001b[0m | \u001b[0m 0.009229\u001b[0m | \u001b[0m 87.94   \u001b[0m |\n",
      "| \u001b[0m 146     \u001b[0m | \u001b[0m 0.9383  \u001b[0m | \u001b[0m 0.6717  \u001b[0m | \u001b[0m 6.33    \u001b[0m | \u001b[0m 0.5627  \u001b[0m | \u001b[0m 0.8744  \u001b[0m | \u001b[0m 2.589   \u001b[0m | \u001b[0m 2.511   \u001b[0m | \u001b[0m 0.1541  \u001b[0m | \u001b[0m 25.45   \u001b[0m | \u001b[0m 71.21   \u001b[0m | \u001b[0m 0.001305\u001b[0m | \u001b[0m 31.14   \u001b[0m |\n",
      "| \u001b[0m 147     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.4627  \u001b[0m | \u001b[0m 1.949   \u001b[0m | \u001b[0m 0.8311  \u001b[0m | \u001b[0m 0.9141  \u001b[0m | \u001b[0m 1.439   \u001b[0m | \u001b[0m 1.731   \u001b[0m | \u001b[0m 0.1516  \u001b[0m | \u001b[0m 90.06   \u001b[0m | \u001b[0m 60.63   \u001b[0m | \u001b[0m 0.006079\u001b[0m | \u001b[0m 62.99   \u001b[0m |\n",
      "| \u001b[0m 148     \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 0.9283  \u001b[0m | \u001b[0m 9.624   \u001b[0m | \u001b[0m 0.598   \u001b[0m | \u001b[0m 0.8445  \u001b[0m | \u001b[0m 2.939   \u001b[0m | \u001b[0m 2.595   \u001b[0m | \u001b[0m 0.09895 \u001b[0m | \u001b[0m 81.21   \u001b[0m | \u001b[0m 27.59   \u001b[0m | \u001b[0m 0.009226\u001b[0m | \u001b[0m 74.83   \u001b[0m |\n",
      "| \u001b[0m 149     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.5936  \u001b[0m | \u001b[0m 4.884   \u001b[0m | \u001b[0m 0.7235  \u001b[0m | \u001b[0m 0.9754  \u001b[0m | \u001b[0m 0.008007\u001b[0m | \u001b[0m 1.72    \u001b[0m | \u001b[0m 0.1647  \u001b[0m | \u001b[0m 26.36   \u001b[0m | \u001b[0m 30.99   \u001b[0m | \u001b[0m 0.008867\u001b[0m | \u001b[0m 23.86   \u001b[0m |\n",
      "| \u001b[0m 150     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.4105  \u001b[0m | \u001b[0m 4.368   \u001b[0m | \u001b[0m 0.5979  \u001b[0m | \u001b[0m 0.8118  \u001b[0m | \u001b[0m 1.768   \u001b[0m | \u001b[0m 1.723   \u001b[0m | \u001b[0m 0.08544 \u001b[0m | \u001b[0m 88.22   \u001b[0m | \u001b[0m 41.74   \u001b[0m | \u001b[0m 0.009558\u001b[0m | \u001b[0m 66.61   \u001b[0m |\n",
      "| \u001b[0m 151     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.4568  \u001b[0m | \u001b[0m 6.476   \u001b[0m | \u001b[0m 0.495   \u001b[0m | \u001b[0m 0.9854  \u001b[0m | \u001b[0m 0.8966  \u001b[0m | \u001b[0m 1.496   \u001b[0m | \u001b[0m 0.1725  \u001b[0m | \u001b[0m 51.66   \u001b[0m | \u001b[0m 17.15   \u001b[0m | \u001b[0m 0.000150\u001b[0m | \u001b[0m 99.88   \u001b[0m |\n",
      "| \u001b[0m 152     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.4775  \u001b[0m | \u001b[0m 1.348   \u001b[0m | \u001b[0m 0.851   \u001b[0m | \u001b[0m 0.4688  \u001b[0m | \u001b[0m 0.03108 \u001b[0m | \u001b[0m 2.776   \u001b[0m | \u001b[0m 0.1065  \u001b[0m | \u001b[0m 19.64   \u001b[0m | \u001b[0m 76.68   \u001b[0m | \u001b[0m 0.006198\u001b[0m | \u001b[0m 66.88   \u001b[0m |\n",
      "| \u001b[95m 153     \u001b[0m | \u001b[95m 0.9446  \u001b[0m | \u001b[95m 0.4885  \u001b[0m | \u001b[95m 4.705   \u001b[0m | \u001b[95m 0.6559  \u001b[0m | \u001b[95m 0.5777  \u001b[0m | \u001b[95m 0.1903  \u001b[0m | \u001b[95m 0.4563  \u001b[0m | \u001b[95m 0.02363 \u001b[0m | \u001b[95m 61.41   \u001b[0m | \u001b[95m 99.21   \u001b[0m | \u001b[95m 0.004713\u001b[0m | \u001b[95m 61.86   \u001b[0m |\n",
      "| \u001b[0m 154     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.4716  \u001b[0m | \u001b[0m 1.223   \u001b[0m | \u001b[0m 0.7164  \u001b[0m | \u001b[0m 0.5369  \u001b[0m | \u001b[0m 0.4452  \u001b[0m | \u001b[0m 2.127   \u001b[0m | \u001b[0m 0.1979  \u001b[0m | \u001b[0m 18.55   \u001b[0m | \u001b[0m 18.63   \u001b[0m | \u001b[0m 0.001634\u001b[0m | \u001b[0m 55.65   \u001b[0m |\n",
      "| \u001b[0m 155     \u001b[0m | \u001b[0m 0.9375  \u001b[0m | \u001b[0m 0.739   \u001b[0m | \u001b[0m 8.622   \u001b[0m | \u001b[0m 0.5365  \u001b[0m | \u001b[0m 0.9554  \u001b[0m | \u001b[0m 2.891   \u001b[0m | \u001b[0m 0.5499  \u001b[0m | \u001b[0m 0.1193  \u001b[0m | \u001b[0m 66.14   \u001b[0m | \u001b[0m 78.76   \u001b[0m | \u001b[0m 0.003755\u001b[0m | \u001b[0m 34.25   \u001b[0m |\n",
      "| \u001b[0m 156     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.7886  \u001b[0m | \u001b[0m 4.61    \u001b[0m | \u001b[0m 0.8875  \u001b[0m | \u001b[0m 0.4382  \u001b[0m | \u001b[0m 2.709   \u001b[0m | \u001b[0m 2.163   \u001b[0m | \u001b[0m 0.1245  \u001b[0m | \u001b[0m 77.11   \u001b[0m | \u001b[0m 61.15   \u001b[0m | \u001b[0m 0.002782\u001b[0m | \u001b[0m 92.16   \u001b[0m |\n",
      "| \u001b[0m 157     \u001b[0m | \u001b[0m 0.93    \u001b[0m | \u001b[0m 0.7948  \u001b[0m | \u001b[0m 5.798   \u001b[0m | \u001b[0m 0.5062  \u001b[0m | \u001b[0m 0.5425  \u001b[0m | \u001b[0m 2.249   \u001b[0m | \u001b[0m 2.387   \u001b[0m | \u001b[0m 0.08614 \u001b[0m | \u001b[0m 62.34   \u001b[0m | \u001b[0m 74.39   \u001b[0m | \u001b[0m 0.004828\u001b[0m | \u001b[0m 53.34   \u001b[0m |\n",
      "| \u001b[0m 158     \u001b[0m | \u001b[0m 0.9373  \u001b[0m | \u001b[0m 0.6045  \u001b[0m | \u001b[0m 6.227   \u001b[0m | \u001b[0m 0.4777  \u001b[0m | \u001b[0m 0.978   \u001b[0m | \u001b[0m 2.619   \u001b[0m | \u001b[0m 2.454   \u001b[0m | \u001b[0m 0.1232  \u001b[0m | \u001b[0m 98.0    \u001b[0m | \u001b[0m 73.63   \u001b[0m | \u001b[0m 0.00172 \u001b[0m | \u001b[0m 68.64   \u001b[0m |\n",
      "| \u001b[0m 159     \u001b[0m | \u001b[0m 0.9406  \u001b[0m | \u001b[0m 0.6748  \u001b[0m | \u001b[0m 3.461   \u001b[0m | \u001b[0m 0.6354  \u001b[0m | \u001b[0m 0.9455  \u001b[0m | \u001b[0m 2.037   \u001b[0m | \u001b[0m 0.5117  \u001b[0m | \u001b[0m 0.05568 \u001b[0m | \u001b[0m 80.76   \u001b[0m | \u001b[0m 48.51   \u001b[0m | \u001b[0m 0.004115\u001b[0m | \u001b[0m 23.17   \u001b[0m |\n",
      "| \u001b[0m 160     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.687   \u001b[0m | \u001b[0m 2.528   \u001b[0m | \u001b[0m 0.6689  \u001b[0m | \u001b[0m 0.4832  \u001b[0m | \u001b[0m 0.3791  \u001b[0m | \u001b[0m 2.624   \u001b[0m | \u001b[0m 0.1038  \u001b[0m | \u001b[0m 20.26   \u001b[0m | \u001b[0m 33.5    \u001b[0m | \u001b[0m 0.003274\u001b[0m | \u001b[0m 39.58   \u001b[0m |\n",
      "| \u001b[0m 161     \u001b[0m | \u001b[0m 0.9339  \u001b[0m | \u001b[0m 0.8281  \u001b[0m | \u001b[0m 9.839   \u001b[0m | \u001b[0m 0.5798  \u001b[0m | \u001b[0m 0.531   \u001b[0m | \u001b[0m 1.453   \u001b[0m | \u001b[0m 1.153   \u001b[0m | \u001b[0m 0.1278  \u001b[0m | \u001b[0m 70.73   \u001b[0m | \u001b[0m 29.32   \u001b[0m | \u001b[0m 0.009794\u001b[0m | \u001b[0m 26.18   \u001b[0m |\n",
      "| \u001b[0m 162     \u001b[0m | \u001b[0m 0.9411  \u001b[0m | \u001b[0m 0.5075  \u001b[0m | \u001b[0m 5.66    \u001b[0m | \u001b[0m 0.7065  \u001b[0m | \u001b[0m 0.4077  \u001b[0m | \u001b[0m 2.696   \u001b[0m | \u001b[0m 1.516   \u001b[0m | \u001b[0m 0.05992 \u001b[0m | \u001b[0m 16.27   \u001b[0m | \u001b[0m 64.53   \u001b[0m | \u001b[0m 0.004546\u001b[0m | \u001b[0m 97.66   \u001b[0m |\n",
      "| \u001b[0m 163     \u001b[0m | \u001b[0m 0.9401  \u001b[0m | \u001b[0m 0.8576  \u001b[0m | \u001b[0m 7.48    \u001b[0m | \u001b[0m 0.543   \u001b[0m | \u001b[0m 0.6355  \u001b[0m | \u001b[0m 2.995   \u001b[0m | \u001b[0m 1.998   \u001b[0m | \u001b[0m 0.119   \u001b[0m | \u001b[0m 21.34   \u001b[0m | \u001b[0m 63.54   \u001b[0m | \u001b[0m 0.009398\u001b[0m | \u001b[0m 94.74   \u001b[0m |\n",
      "| \u001b[0m 164     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.5253  \u001b[0m | \u001b[0m 6.588   \u001b[0m | \u001b[0m 0.4695  \u001b[0m | \u001b[0m 0.6378  \u001b[0m | \u001b[0m 1.678   \u001b[0m | \u001b[0m 2.706   \u001b[0m | \u001b[0m 0.1214  \u001b[0m | \u001b[0m 13.49   \u001b[0m | \u001b[0m 66.06   \u001b[0m | \u001b[0m 0.008266\u001b[0m | \u001b[0m 90.47   \u001b[0m |\n",
      "| \u001b[0m 165     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.7826  \u001b[0m | \u001b[0m 1.794   \u001b[0m | \u001b[0m 0.7516  \u001b[0m | \u001b[0m 0.5863  \u001b[0m | \u001b[0m 0.289   \u001b[0m | \u001b[0m 2.452   \u001b[0m | \u001b[0m 0.03694 \u001b[0m | \u001b[0m 74.64   \u001b[0m | \u001b[0m 88.61   \u001b[0m | \u001b[0m 0.005634\u001b[0m | \u001b[0m 43.99   \u001b[0m |\n",
      "| \u001b[0m 166     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.5571  \u001b[0m | \u001b[0m 5.305   \u001b[0m | \u001b[0m 0.7512  \u001b[0m | \u001b[0m 0.5262  \u001b[0m | \u001b[0m 0.8197  \u001b[0m | \u001b[0m 0.1664  \u001b[0m | \u001b[0m 0.02235 \u001b[0m | \u001b[0m 36.41   \u001b[0m | \u001b[0m 75.46   \u001b[0m | \u001b[0m 0.005872\u001b[0m | \u001b[0m 47.75   \u001b[0m |\n",
      "| \u001b[0m 167     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.4061  \u001b[0m | \u001b[0m 5.715   \u001b[0m | \u001b[0m 0.9714  \u001b[0m | \u001b[0m 0.982   \u001b[0m | \u001b[0m 0.08376 \u001b[0m | \u001b[0m 0.5673  \u001b[0m | \u001b[0m 0.1439  \u001b[0m | \u001b[0m 16.75   \u001b[0m | \u001b[0m 79.58   \u001b[0m | \u001b[0m 0.004181\u001b[0m | \u001b[0m 65.07   \u001b[0m |\n",
      "| \u001b[0m 168     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.7192  \u001b[0m | \u001b[0m 2.877   \u001b[0m | \u001b[0m 0.5352  \u001b[0m | \u001b[0m 0.8383  \u001b[0m | \u001b[0m 2.751   \u001b[0m | \u001b[0m 0.7526  \u001b[0m | \u001b[0m 0.0598  \u001b[0m | \u001b[0m 78.67   \u001b[0m | \u001b[0m 46.25   \u001b[0m | \u001b[0m 0.005761\u001b[0m | \u001b[0m 65.41   \u001b[0m |\n",
      "| \u001b[0m 169     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.9273  \u001b[0m | \u001b[0m 7.748   \u001b[0m | \u001b[0m 0.5935  \u001b[0m | \u001b[0m 0.6887  \u001b[0m | \u001b[0m 2.293   \u001b[0m | \u001b[0m 2.866   \u001b[0m | \u001b[0m 0.144   \u001b[0m | \u001b[0m 95.56   \u001b[0m | \u001b[0m 22.61   \u001b[0m | \u001b[0m 0.009253\u001b[0m | \u001b[0m 54.86   \u001b[0m |\n",
      "| \u001b[0m 170     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.8619  \u001b[0m | \u001b[0m 6.069   \u001b[0m | \u001b[0m 0.8633  \u001b[0m | \u001b[0m 0.4901  \u001b[0m | \u001b[0m 1.609   \u001b[0m | \u001b[0m 2.316   \u001b[0m | \u001b[0m 0.05834 \u001b[0m | \u001b[0m 26.98   \u001b[0m | \u001b[0m 94.82   \u001b[0m | \u001b[0m 0.001836\u001b[0m | \u001b[0m 21.31   \u001b[0m |\n",
      "| \u001b[0m 171     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.6105  \u001b[0m | \u001b[0m 4.293   \u001b[0m | \u001b[0m 0.4903  \u001b[0m | \u001b[0m 0.8594  \u001b[0m | \u001b[0m 2.754   \u001b[0m | \u001b[0m 2.613   \u001b[0m | \u001b[0m 0.05979 \u001b[0m | \u001b[0m 11.27   \u001b[0m | \u001b[0m 45.38   \u001b[0m | \u001b[0m 0.005864\u001b[0m | \u001b[0m 40.14   \u001b[0m |\n",
      "| \u001b[0m 172     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.9694  \u001b[0m | \u001b[0m 3.197   \u001b[0m | \u001b[0m 0.5799  \u001b[0m | \u001b[0m 0.7856  \u001b[0m | \u001b[0m 2.146   \u001b[0m | \u001b[0m 1.931   \u001b[0m | \u001b[0m 0.09901 \u001b[0m | \u001b[0m 8.082   \u001b[0m | \u001b[0m 26.03   \u001b[0m | \u001b[0m 0.003523\u001b[0m | \u001b[0m 33.18   \u001b[0m |\n",
      "| \u001b[0m 173     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.9642  \u001b[0m | \u001b[0m 9.001   \u001b[0m | \u001b[0m 0.5862  \u001b[0m | \u001b[0m 0.9762  \u001b[0m | \u001b[0m 1.678   \u001b[0m | \u001b[0m 2.545   \u001b[0m | \u001b[0m 0.06676 \u001b[0m | \u001b[0m 64.92   \u001b[0m | \u001b[0m 73.18   \u001b[0m | \u001b[0m 0.007117\u001b[0m | \u001b[0m 41.14   \u001b[0m |\n",
      "| \u001b[0m 174     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.5194  \u001b[0m | \u001b[0m 6.314   \u001b[0m | \u001b[0m 0.6706  \u001b[0m | \u001b[0m 0.4291  \u001b[0m | \u001b[0m 1.51    \u001b[0m | \u001b[0m 1.932   \u001b[0m | \u001b[0m 0.02693 \u001b[0m | \u001b[0m 12.61   \u001b[0m | \u001b[0m 69.88   \u001b[0m | \u001b[0m 0.001067\u001b[0m | \u001b[0m 59.88   \u001b[0m |\n",
      "| \u001b[0m 175     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.6073  \u001b[0m | \u001b[0m 1.541   \u001b[0m | \u001b[0m 0.5799  \u001b[0m | \u001b[0m 0.6897  \u001b[0m | \u001b[0m 1.845   \u001b[0m | \u001b[0m 0.7484  \u001b[0m | \u001b[0m 0.1187  \u001b[0m | \u001b[0m 60.09   \u001b[0m | \u001b[0m 84.07   \u001b[0m | \u001b[0m 0.001089\u001b[0m | \u001b[0m 56.36   \u001b[0m |\n",
      "| \u001b[0m 176     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 5.025   \u001b[0m | \u001b[0m 0.8313  \u001b[0m | \u001b[0m 0.4352  \u001b[0m | \u001b[0m 2.428   \u001b[0m | \u001b[0m 0.04572 \u001b[0m | \u001b[0m 0.03941 \u001b[0m | \u001b[0m 84.09   \u001b[0m | \u001b[0m 51.52   \u001b[0m | \u001b[0m 0.009662\u001b[0m | \u001b[0m 86.95   \u001b[0m |\n",
      "| \u001b[0m 177     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.9176  \u001b[0m | \u001b[0m 6.073   \u001b[0m | \u001b[0m 0.8137  \u001b[0m | \u001b[0m 0.505   \u001b[0m | \u001b[0m 2.241   \u001b[0m | \u001b[0m 0.3747  \u001b[0m | \u001b[0m 0.1249  \u001b[0m | \u001b[0m 22.87   \u001b[0m | \u001b[0m 54.45   \u001b[0m | \u001b[0m 0.009475\u001b[0m | \u001b[0m 93.87   \u001b[0m |\n",
      "| \u001b[0m 178     \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.7902  \u001b[0m | \u001b[0m 2.441   \u001b[0m | \u001b[0m 0.5615  \u001b[0m | \u001b[0m 0.8082  \u001b[0m | \u001b[0m 1.328   \u001b[0m | \u001b[0m 0.307   \u001b[0m | \u001b[0m 0.07262 \u001b[0m | \u001b[0m 50.93   \u001b[0m | \u001b[0m 75.9    \u001b[0m | \u001b[0m 0.007424\u001b[0m | \u001b[0m 86.87   \u001b[0m |\n",
      "| \u001b[0m 179     \u001b[0m | \u001b[0m 0.9337  \u001b[0m | \u001b[0m 0.9705  \u001b[0m | \u001b[0m 9.065   \u001b[0m | \u001b[0m 0.538   \u001b[0m | \u001b[0m 0.5596  \u001b[0m | \u001b[0m 2.523   \u001b[0m | \u001b[0m 2.409   \u001b[0m | \u001b[0m 0.06912 \u001b[0m | \u001b[0m 21.19   \u001b[0m | \u001b[0m 62.14   \u001b[0m | \u001b[0m 0.000570\u001b[0m | \u001b[0m 32.82   \u001b[0m |\n",
      "| \u001b[0m 180     \u001b[0m | \u001b[0m 0.94    \u001b[0m | \u001b[0m 0.5683  \u001b[0m | \u001b[0m 8.837   \u001b[0m | \u001b[0m 0.6846  \u001b[0m | \u001b[0m 0.9924  \u001b[0m | \u001b[0m 1.916   \u001b[0m | \u001b[0m 2.461   \u001b[0m | \u001b[0m 0.1585  \u001b[0m | \u001b[0m 18.03   \u001b[0m | \u001b[0m 23.47   \u001b[0m | \u001b[0m 0.0046  \u001b[0m | \u001b[0m 49.71   \u001b[0m |\n",
      "| \u001b[0m 181     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.878   \u001b[0m | \u001b[0m 3.055   \u001b[0m | \u001b[0m 0.4086  \u001b[0m | \u001b[0m 0.7305  \u001b[0m | \u001b[0m 1.582   \u001b[0m | \u001b[0m 0.3472  \u001b[0m | \u001b[0m 0.1272  \u001b[0m | \u001b[0m 68.96   \u001b[0m | \u001b[0m 72.06   \u001b[0m | \u001b[0m 0.008655\u001b[0m | \u001b[0m 36.9    \u001b[0m |\n",
      "| \u001b[0m 182     \u001b[0m | \u001b[0m 0.9402  \u001b[0m | \u001b[0m 0.5035  \u001b[0m | \u001b[0m 5.334   \u001b[0m | \u001b[0m 0.6888  \u001b[0m | \u001b[0m 0.9162  \u001b[0m | \u001b[0m 2.808   \u001b[0m | \u001b[0m 0.7758  \u001b[0m | \u001b[0m 0.02053 \u001b[0m | \u001b[0m 34.15   \u001b[0m | \u001b[0m 12.52   \u001b[0m | \u001b[0m 0.007954\u001b[0m | \u001b[0m 25.4    \u001b[0m |\n",
      "| \u001b[0m 183     \u001b[0m | \u001b[0m 0.9443  \u001b[0m | \u001b[0m 0.9372  \u001b[0m | \u001b[0m 5.757   \u001b[0m | \u001b[0m 0.6413  \u001b[0m | \u001b[0m 0.6194  \u001b[0m | \u001b[0m 1.989   \u001b[0m | \u001b[0m 0.6131  \u001b[0m | \u001b[0m 0.02482 \u001b[0m | \u001b[0m 40.93   \u001b[0m | \u001b[0m 62.94   \u001b[0m | \u001b[0m 0.002178\u001b[0m | \u001b[0m 60.07   \u001b[0m |\n",
      "| \u001b[0m 184     \u001b[0m | \u001b[0m 0.9379  \u001b[0m | \u001b[0m 0.9888  \u001b[0m | \u001b[0m 8.108   \u001b[0m | \u001b[0m 0.8903  \u001b[0m | \u001b[0m 0.9845  \u001b[0m | \u001b[0m 1.232   \u001b[0m | \u001b[0m 1.727   \u001b[0m | \u001b[0m 0.0929  \u001b[0m | \u001b[0m 80.97   \u001b[0m | \u001b[0m 56.24   \u001b[0m | \u001b[0m 0.003623\u001b[0m | \u001b[0m 64.03   \u001b[0m |\n",
      "| \u001b[0m 185     \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 7.083   \u001b[0m | \u001b[0m 0.5946  \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.4092  \u001b[0m | \u001b[0m 2.612   \u001b[0m | \u001b[0m 0.08024 \u001b[0m | \u001b[0m 61.1    \u001b[0m | \u001b[0m 90.23   \u001b[0m | \u001b[0m 0.009504\u001b[0m | \u001b[0m 46.1    \u001b[0m |\n",
      "| \u001b[0m 186     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.8291  \u001b[0m | \u001b[0m 4.078   \u001b[0m | \u001b[0m 0.7711  \u001b[0m | \u001b[0m 0.8854  \u001b[0m | \u001b[0m 2.918   \u001b[0m | \u001b[0m 1.715   \u001b[0m | \u001b[0m 0.1934  \u001b[0m | \u001b[0m 76.01   \u001b[0m | \u001b[0m 82.55   \u001b[0m | \u001b[0m 0.001683\u001b[0m | \u001b[0m 74.87   \u001b[0m |\n",
      "| \u001b[0m 187     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.4753  \u001b[0m | \u001b[0m 8.221   \u001b[0m | \u001b[0m 0.7662  \u001b[0m | \u001b[0m 0.53    \u001b[0m | \u001b[0m 0.4645  \u001b[0m | \u001b[0m 2.711   \u001b[0m | \u001b[0m 0.01528 \u001b[0m | \u001b[0m 36.61   \u001b[0m | \u001b[0m 88.12   \u001b[0m | \u001b[0m 0.003978\u001b[0m | \u001b[0m 83.89   \u001b[0m |\n",
      "| \u001b[0m 188     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.7001  \u001b[0m | \u001b[0m 7.23    \u001b[0m | \u001b[0m 0.7378  \u001b[0m | \u001b[0m 0.8145  \u001b[0m | \u001b[0m 0.2585  \u001b[0m | \u001b[0m 1.703   \u001b[0m | \u001b[0m 0.1973  \u001b[0m | \u001b[0m 15.68   \u001b[0m | \u001b[0m 89.95   \u001b[0m | \u001b[0m 0.009483\u001b[0m | \u001b[0m 68.54   \u001b[0m |\n",
      "| \u001b[0m 189     \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.6527  \u001b[0m | \u001b[0m 6.471   \u001b[0m | \u001b[0m 0.7966  \u001b[0m | \u001b[0m 0.6999  \u001b[0m | \u001b[0m 1.559   \u001b[0m | \u001b[0m 2.584   \u001b[0m | \u001b[0m 0.01456 \u001b[0m | \u001b[0m 9.633   \u001b[0m | \u001b[0m 41.33   \u001b[0m | \u001b[0m 0.004625\u001b[0m | \u001b[0m 54.56   \u001b[0m |\n",
      "| \u001b[0m 190     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.7069  \u001b[0m | \u001b[0m 3.675   \u001b[0m | \u001b[0m 0.5015  \u001b[0m | \u001b[0m 0.7328  \u001b[0m | \u001b[0m 1.501   \u001b[0m | \u001b[0m 0.0989  \u001b[0m | \u001b[0m 0.1386  \u001b[0m | \u001b[0m 67.46   \u001b[0m | \u001b[0m 79.64   \u001b[0m | \u001b[0m 0.000541\u001b[0m | \u001b[0m 26.2    \u001b[0m |\n",
      "| \u001b[0m 191     \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.8338  \u001b[0m | \u001b[0m 5.654   \u001b[0m | \u001b[0m 0.5812  \u001b[0m | \u001b[0m 0.9344  \u001b[0m | \u001b[0m 0.4397  \u001b[0m | \u001b[0m 0.11    \u001b[0m | \u001b[0m 0.04856 \u001b[0m | \u001b[0m 49.26   \u001b[0m | \u001b[0m 22.87   \u001b[0m | \u001b[0m 0.008249\u001b[0m | \u001b[0m 21.31   \u001b[0m |\n",
      "| \u001b[0m 192     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.9006  \u001b[0m | \u001b[0m 8.17    \u001b[0m | \u001b[0m 0.7776  \u001b[0m | \u001b[0m 0.6848  \u001b[0m | \u001b[0m 1.499   \u001b[0m | \u001b[0m 1.361   \u001b[0m | \u001b[0m 0.017   \u001b[0m | \u001b[0m 78.31   \u001b[0m | \u001b[0m 58.15   \u001b[0m | \u001b[0m 0.002207\u001b[0m | \u001b[0m 67.17   \u001b[0m |\n",
      "| \u001b[0m 193     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.6937  \u001b[0m | \u001b[0m 5.771   \u001b[0m | \u001b[0m 0.7626  \u001b[0m | \u001b[0m 0.7373  \u001b[0m | \u001b[0m 2.187   \u001b[0m | \u001b[0m 0.5852  \u001b[0m | \u001b[0m 0.1662  \u001b[0m | \u001b[0m 97.82   \u001b[0m | \u001b[0m 23.18   \u001b[0m | \u001b[0m 0.003863\u001b[0m | \u001b[0m 82.58   \u001b[0m |\n",
      "| \u001b[0m 194     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.9806  \u001b[0m | \u001b[0m 3.507   \u001b[0m | \u001b[0m 0.5908  \u001b[0m | \u001b[0m 0.6262  \u001b[0m | \u001b[0m 2.772   \u001b[0m | \u001b[0m 0.2735  \u001b[0m | \u001b[0m 0.03925 \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 85.28   \u001b[0m | \u001b[0m 0.000476\u001b[0m | \u001b[0m 23.91   \u001b[0m |\n",
      "| \u001b[0m 195     \u001b[0m | \u001b[0m 0.9443  \u001b[0m | \u001b[0m 0.7376  \u001b[0m | \u001b[0m 5.022   \u001b[0m | \u001b[0m 0.5716  \u001b[0m | \u001b[0m 0.6414  \u001b[0m | \u001b[0m 2.176   \u001b[0m | \u001b[0m 2.739   \u001b[0m | \u001b[0m 0.1287  \u001b[0m | \u001b[0m 94.16   \u001b[0m | \u001b[0m 88.91   \u001b[0m | \u001b[0m 0.008132\u001b[0m | \u001b[0m 62.54   \u001b[0m |\n",
      "| \u001b[0m 196     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.4138  \u001b[0m | \u001b[0m 9.635   \u001b[0m | \u001b[0m 0.6813  \u001b[0m | \u001b[0m 0.7591  \u001b[0m | \u001b[0m 1.275   \u001b[0m | \u001b[0m 2.856   \u001b[0m | \u001b[0m 0.1297  \u001b[0m | \u001b[0m 57.36   \u001b[0m | \u001b[0m 62.23   \u001b[0m | \u001b[0m 0.004057\u001b[0m | \u001b[0m 46.01   \u001b[0m |\n",
      "| \u001b[0m 197     \u001b[0m | \u001b[0m 0.9343  \u001b[0m | \u001b[0m 0.8719  \u001b[0m | \u001b[0m 6.026   \u001b[0m | \u001b[0m 0.4216  \u001b[0m | \u001b[0m 0.4994  \u001b[0m | \u001b[0m 1.781   \u001b[0m | \u001b[0m 2.844   \u001b[0m | \u001b[0m 0.1069  \u001b[0m | \u001b[0m 50.92   \u001b[0m | \u001b[0m 82.84   \u001b[0m | \u001b[0m 0.004562\u001b[0m | \u001b[0m 53.13   \u001b[0m |\n",
      "| \u001b[0m 198     \u001b[0m | \u001b[0m 0.9345  \u001b[0m | \u001b[0m 0.8871  \u001b[0m | \u001b[0m 7.7     \u001b[0m | \u001b[0m 0.9923  \u001b[0m | \u001b[0m 0.9448  \u001b[0m | \u001b[0m 1.226   \u001b[0m | \u001b[0m 1.739   \u001b[0m | \u001b[0m 0.151   \u001b[0m | \u001b[0m 47.15   \u001b[0m | \u001b[0m 50.13   \u001b[0m | \u001b[0m 0.004042\u001b[0m | \u001b[0m 38.83   \u001b[0m |\n",
      "| \u001b[0m 199     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.4802  \u001b[0m | \u001b[0m 7.732   \u001b[0m | \u001b[0m 0.4853  \u001b[0m | \u001b[0m 0.6473  \u001b[0m | \u001b[0m 1.764   \u001b[0m | \u001b[0m 0.3733  \u001b[0m | \u001b[0m 0.04077 \u001b[0m | \u001b[0m 41.12   \u001b[0m | \u001b[0m 43.04   \u001b[0m | \u001b[0m 0.004769\u001b[0m | \u001b[0m 28.28   \u001b[0m |\n",
      "| \u001b[0m 200     \u001b[0m | \u001b[0m 0.9384  \u001b[0m | \u001b[0m 0.8649  \u001b[0m | \u001b[0m 1.098   \u001b[0m | \u001b[0m 0.8968  \u001b[0m | \u001b[0m 0.9847  \u001b[0m | \u001b[0m 0.7355  \u001b[0m | \u001b[0m 1.661   \u001b[0m | \u001b[0m 0.1151  \u001b[0m | \u001b[0m 40.28   \u001b[0m | \u001b[0m 38.07   \u001b[0m | \u001b[0m 0.003874\u001b[0m | \u001b[0m 93.77   \u001b[0m |\n",
      "| \u001b[0m 201     \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.9326  \u001b[0m | \u001b[0m 4.071   \u001b[0m | \u001b[0m 0.8856  \u001b[0m | \u001b[0m 0.5289  \u001b[0m | \u001b[0m 0.196   \u001b[0m | \u001b[0m 0.8214  \u001b[0m | \u001b[0m 0.06573 \u001b[0m | \u001b[0m 41.81   \u001b[0m | \u001b[0m 25.56   \u001b[0m | \u001b[0m 0.006463\u001b[0m | \u001b[0m 61.1    \u001b[0m |\n",
      "| \u001b[0m 202     \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.4419  \u001b[0m | \u001b[0m 9.617   \u001b[0m | \u001b[0m 0.9146  \u001b[0m | \u001b[0m 0.862   \u001b[0m | \u001b[0m 1.92    \u001b[0m | \u001b[0m 0.1355  \u001b[0m | \u001b[0m 0.02088 \u001b[0m | \u001b[0m 74.58   \u001b[0m | \u001b[0m 54.22   \u001b[0m | \u001b[0m 0.000810\u001b[0m | \u001b[0m 42.85   \u001b[0m |\n",
      "| \u001b[0m 203     \u001b[0m | \u001b[0m 0.9406  \u001b[0m | \u001b[0m 0.9686  \u001b[0m | \u001b[0m 1.345   \u001b[0m | \u001b[0m 0.8389  \u001b[0m | \u001b[0m 0.9103  \u001b[0m | \u001b[0m 1.032   \u001b[0m | \u001b[0m 2.72    \u001b[0m | \u001b[0m 0.07215 \u001b[0m | \u001b[0m 97.9    \u001b[0m | \u001b[0m 19.13   \u001b[0m | \u001b[0m 0.006107\u001b[0m | \u001b[0m 68.27   \u001b[0m |\n",
      "| \u001b[0m 204     \u001b[0m | \u001b[0m 0.9378  \u001b[0m | \u001b[0m 0.9629  \u001b[0m | \u001b[0m 7.468   \u001b[0m | \u001b[0m 0.8848  \u001b[0m | \u001b[0m 0.9874  \u001b[0m | \u001b[0m 1.015   \u001b[0m | \u001b[0m 1.496   \u001b[0m | \u001b[0m 0.1255  \u001b[0m | \u001b[0m 17.26   \u001b[0m | \u001b[0m 71.87   \u001b[0m | \u001b[0m 0.006884\u001b[0m | \u001b[0m 40.17   \u001b[0m |\n",
      "| \u001b[0m 205     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.9994  \u001b[0m | \u001b[0m 4.213   \u001b[0m | \u001b[0m 0.5729  \u001b[0m | \u001b[0m 0.9366  \u001b[0m | \u001b[0m 0.9386  \u001b[0m | \u001b[0m 1.128   \u001b[0m | \u001b[0m 0.03533 \u001b[0m | \u001b[0m 64.38   \u001b[0m | \u001b[0m 58.6    \u001b[0m | \u001b[0m 0.008654\u001b[0m | \u001b[0m 51.67   \u001b[0m |\n",
      "| \u001b[0m 206     \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.7734  \u001b[0m | \u001b[0m 8.14    \u001b[0m | \u001b[0m 0.5144  \u001b[0m | \u001b[0m 0.6092  \u001b[0m | \u001b[0m 0.5121  \u001b[0m | \u001b[0m 2.212   \u001b[0m | \u001b[0m 0.1553  \u001b[0m | \u001b[0m 15.72   \u001b[0m | \u001b[0m 12.03   \u001b[0m | \u001b[0m 0.008245\u001b[0m | \u001b[0m 30.81   \u001b[0m |\n",
      "| \u001b[0m 207     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.4158  \u001b[0m | \u001b[0m 2.69    \u001b[0m | \u001b[0m 0.7951  \u001b[0m | \u001b[0m 0.5556  \u001b[0m | \u001b[0m 0.7058  \u001b[0m | \u001b[0m 0.3295  \u001b[0m | \u001b[0m 0.1554  \u001b[0m | \u001b[0m 54.95   \u001b[0m | \u001b[0m 53.42   \u001b[0m | \u001b[0m 0.004788\u001b[0m | \u001b[0m 61.5    \u001b[0m |\n",
      "| \u001b[0m 208     \u001b[0m | \u001b[0m 0.9406  \u001b[0m | \u001b[0m 0.664   \u001b[0m | \u001b[0m 1.932   \u001b[0m | \u001b[0m 0.6532  \u001b[0m | \u001b[0m 0.814   \u001b[0m | \u001b[0m 2.439   \u001b[0m | \u001b[0m 1.455   \u001b[0m | \u001b[0m 0.1435  \u001b[0m | \u001b[0m 43.76   \u001b[0m | \u001b[0m 72.52   \u001b[0m | \u001b[0m 0.002073\u001b[0m | \u001b[0m 33.6    \u001b[0m |\n",
      "| \u001b[0m 209     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.6949  \u001b[0m | \u001b[0m 1.361   \u001b[0m | \u001b[0m 0.5942  \u001b[0m | \u001b[0m 0.9182  \u001b[0m | \u001b[0m 0.3694  \u001b[0m | \u001b[0m 1.188   \u001b[0m | \u001b[0m 0.08713 \u001b[0m | \u001b[0m 26.44   \u001b[0m | \u001b[0m 17.5    \u001b[0m | \u001b[0m 0.002075\u001b[0m | \u001b[0m 77.01   \u001b[0m |\n",
      "| \u001b[0m 210     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.405   \u001b[0m | \u001b[0m 2.909   \u001b[0m | \u001b[0m 0.9073  \u001b[0m | \u001b[0m 0.6068  \u001b[0m | \u001b[0m 1.009   \u001b[0m | \u001b[0m 1.973   \u001b[0m | \u001b[0m 0.1281  \u001b[0m | \u001b[0m 22.15   \u001b[0m | \u001b[0m 68.68   \u001b[0m | \u001b[0m 0.004343\u001b[0m | \u001b[0m 63.3    \u001b[0m |\n",
      "| \u001b[0m 211     \u001b[0m | \u001b[0m 0.9416  \u001b[0m | \u001b[0m 0.5422  \u001b[0m | \u001b[0m 1.676   \u001b[0m | \u001b[0m 0.6656  \u001b[0m | \u001b[0m 0.6099  \u001b[0m | \u001b[0m 1.926   \u001b[0m | \u001b[0m 1.262   \u001b[0m | \u001b[0m 0.07227 \u001b[0m | \u001b[0m 92.03   \u001b[0m | \u001b[0m 70.84   \u001b[0m | \u001b[0m 0.005244\u001b[0m | \u001b[0m 26.83   \u001b[0m |\n",
      "| \u001b[0m 212     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.9028  \u001b[0m | \u001b[0m 2.494   \u001b[0m | \u001b[0m 0.8964  \u001b[0m | \u001b[0m 0.4359  \u001b[0m | \u001b[0m 0.6455  \u001b[0m | \u001b[0m 2.414   \u001b[0m | \u001b[0m 0.02935 \u001b[0m | \u001b[0m 24.38   \u001b[0m | \u001b[0m 72.98   \u001b[0m | \u001b[0m 0.005748\u001b[0m | \u001b[0m 76.98   \u001b[0m |\n",
      "| \u001b[0m 213     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.8304  \u001b[0m | \u001b[0m 2.129   \u001b[0m | \u001b[0m 0.5626  \u001b[0m | \u001b[0m 0.9228  \u001b[0m | \u001b[0m 2.17    \u001b[0m | \u001b[0m 1.888   \u001b[0m | \u001b[0m 0.08368 \u001b[0m | \u001b[0m 19.33   \u001b[0m | \u001b[0m 97.78   \u001b[0m | \u001b[0m 0.005913\u001b[0m | \u001b[0m 99.61   \u001b[0m |\n",
      "| \u001b[0m 214     \u001b[0m | \u001b[0m 0.9411  \u001b[0m | \u001b[0m 0.5865  \u001b[0m | \u001b[0m 9.176   \u001b[0m | \u001b[0m 0.8244  \u001b[0m | \u001b[0m 0.6092  \u001b[0m | \u001b[0m 2.762   \u001b[0m | \u001b[0m 1.679   \u001b[0m | \u001b[0m 0.1444  \u001b[0m | \u001b[0m 36.28   \u001b[0m | \u001b[0m 76.24   \u001b[0m | \u001b[0m 0.001577\u001b[0m | \u001b[0m 95.81   \u001b[0m |\n",
      "| \u001b[0m 215     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.4474  \u001b[0m | \u001b[0m 2.705   \u001b[0m | \u001b[0m 0.6429  \u001b[0m | \u001b[0m 0.5079  \u001b[0m | \u001b[0m 2.838   \u001b[0m | \u001b[0m 2.733   \u001b[0m | \u001b[0m 0.1211  \u001b[0m | \u001b[0m 87.0    \u001b[0m | \u001b[0m 74.12   \u001b[0m | \u001b[0m 0.006324\u001b[0m | \u001b[0m 36.29   \u001b[0m |\n",
      "| \u001b[0m 216     \u001b[0m | \u001b[0m 0.9313  \u001b[0m | \u001b[0m 0.9087  \u001b[0m | \u001b[0m 6.521   \u001b[0m | \u001b[0m 0.4649  \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 2.319   \u001b[0m | \u001b[0m 2.735   \u001b[0m | \u001b[0m 0.1267  \u001b[0m | \u001b[0m 74.21   \u001b[0m | \u001b[0m 13.6    \u001b[0m | \u001b[0m 0.002961\u001b[0m | \u001b[0m 21.23   \u001b[0m |\n",
      "| \u001b[0m 217     \u001b[0m | \u001b[0m 0.9384  \u001b[0m | \u001b[0m 0.4323  \u001b[0m | \u001b[0m 3.79    \u001b[0m | \u001b[0m 0.5756  \u001b[0m | \u001b[0m 0.968   \u001b[0m | \u001b[0m 2.976   \u001b[0m | \u001b[0m 0.7728  \u001b[0m | \u001b[0m 0.06985 \u001b[0m | \u001b[0m 50.83   \u001b[0m | \u001b[0m 52.59   \u001b[0m | \u001b[0m 0.009167\u001b[0m | \u001b[0m 28.63   \u001b[0m |\n",
      "| \u001b[0m 218     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.8972  \u001b[0m | \u001b[0m 3.167   \u001b[0m | \u001b[0m 0.6586  \u001b[0m | \u001b[0m 0.5876  \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 0.5057  \u001b[0m | \u001b[0m 0.1855  \u001b[0m | \u001b[0m 90.7    \u001b[0m | \u001b[0m 59.25   \u001b[0m | \u001b[0m 0.009226\u001b[0m | \u001b[0m 25.28   \u001b[0m |\n",
      "| \u001b[0m 219     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.8496  \u001b[0m | \u001b[0m 3.497   \u001b[0m | \u001b[0m 0.9368  \u001b[0m | \u001b[0m 0.8252  \u001b[0m | \u001b[0m 2.188   \u001b[0m | \u001b[0m 2.882   \u001b[0m | \u001b[0m 0.1037  \u001b[0m | \u001b[0m 90.19   \u001b[0m | \u001b[0m 98.3    \u001b[0m | \u001b[0m 0.008195\u001b[0m | \u001b[0m 24.59   \u001b[0m |\n",
      "| \u001b[0m 220     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.9526  \u001b[0m | \u001b[0m 3.424   \u001b[0m | \u001b[0m 0.4435  \u001b[0m | \u001b[0m 0.9329  \u001b[0m | \u001b[0m 1.581   \u001b[0m | \u001b[0m 1.871   \u001b[0m | \u001b[0m 0.01791 \u001b[0m | \u001b[0m 80.96   \u001b[0m | \u001b[0m 82.32   \u001b[0m | \u001b[0m 0.001731\u001b[0m | \u001b[0m 78.24   \u001b[0m |\n",
      "| \u001b[0m 221     \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.434   \u001b[0m | \u001b[0m 6.437   \u001b[0m | \u001b[0m 0.4385  \u001b[0m | \u001b[0m 0.9394  \u001b[0m | \u001b[0m 0.6139  \u001b[0m | \u001b[0m 1.82    \u001b[0m | \u001b[0m 0.1841  \u001b[0m | \u001b[0m 71.07   \u001b[0m | \u001b[0m 65.22   \u001b[0m | \u001b[0m 0.003596\u001b[0m | \u001b[0m 87.82   \u001b[0m |\n",
      "| \u001b[0m 222     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.6877  \u001b[0m | \u001b[0m 5.116   \u001b[0m | \u001b[0m 0.5328  \u001b[0m | \u001b[0m 0.6015  \u001b[0m | \u001b[0m 2.547   \u001b[0m | \u001b[0m 1.667   \u001b[0m | \u001b[0m 0.09732 \u001b[0m | \u001b[0m 70.19   \u001b[0m | \u001b[0m 43.03   \u001b[0m | \u001b[0m 0.001781\u001b[0m | \u001b[0m 45.32   \u001b[0m |\n",
      "| \u001b[0m 223     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.837   \u001b[0m | \u001b[0m 1.715   \u001b[0m | \u001b[0m 0.8951  \u001b[0m | \u001b[0m 0.7367  \u001b[0m | \u001b[0m 0.2376  \u001b[0m | \u001b[0m 0.7246  \u001b[0m | \u001b[0m 0.1393  \u001b[0m | \u001b[0m 15.16   \u001b[0m | \u001b[0m 38.47   \u001b[0m | \u001b[0m 0.004553\u001b[0m | \u001b[0m 32.22   \u001b[0m |\n",
      "| \u001b[0m 224     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.9892  \u001b[0m | \u001b[0m 6.197   \u001b[0m | \u001b[0m 0.8987  \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.4543  \u001b[0m | \u001b[0m 1.31    \u001b[0m | \u001b[0m 0.09135 \u001b[0m | \u001b[0m 81.69   \u001b[0m | \u001b[0m 29.9    \u001b[0m | \u001b[0m 0.000402\u001b[0m | \u001b[0m 80.11   \u001b[0m |\n",
      "| \u001b[0m 225     \u001b[0m | \u001b[0m 0.9351  \u001b[0m | \u001b[0m 0.8266  \u001b[0m | \u001b[0m 5.702   \u001b[0m | \u001b[0m 0.5913  \u001b[0m | \u001b[0m 0.5559  \u001b[0m | \u001b[0m 2.554   \u001b[0m | \u001b[0m 0.9883  \u001b[0m | \u001b[0m 0.1617  \u001b[0m | \u001b[0m 76.27   \u001b[0m | \u001b[0m 92.56   \u001b[0m | \u001b[0m 0.006949\u001b[0m | \u001b[0m 81.38   \u001b[0m |\n",
      "| \u001b[0m 226     \u001b[0m | \u001b[0m 0.9445  \u001b[0m | \u001b[0m 0.9115  \u001b[0m | \u001b[0m 9.177   \u001b[0m | \u001b[0m 0.9848  \u001b[0m | \u001b[0m 0.4028  \u001b[0m | \u001b[0m 2.753   \u001b[0m | \u001b[0m 1.023   \u001b[0m | \u001b[0m 0.03831 \u001b[0m | \u001b[0m 27.72   \u001b[0m | \u001b[0m 71.34   \u001b[0m | \u001b[0m 0.007209\u001b[0m | \u001b[0m 90.98   \u001b[0m |\n",
      "| \u001b[0m 227     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 8.549   \u001b[0m | \u001b[0m 0.5986  \u001b[0m | \u001b[0m 0.9923  \u001b[0m | \u001b[0m 0.5727  \u001b[0m | \u001b[0m 0.1465  \u001b[0m | \u001b[0m 0.1494  \u001b[0m | \u001b[0m 20.65   \u001b[0m | \u001b[0m 82.56   \u001b[0m | \u001b[0m 0.0079  \u001b[0m | \u001b[0m 95.17   \u001b[0m |\n",
      "| \u001b[0m 228     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.9062  \u001b[0m | \u001b[0m 6.026   \u001b[0m | \u001b[0m 0.9734  \u001b[0m | \u001b[0m 0.6962  \u001b[0m | \u001b[0m 1.213   \u001b[0m | \u001b[0m 1.678   \u001b[0m | \u001b[0m 0.1578  \u001b[0m | \u001b[0m 17.49   \u001b[0m | \u001b[0m 13.94   \u001b[0m | \u001b[0m 0.00913 \u001b[0m | \u001b[0m 43.86   \u001b[0m |\n",
      "| \u001b[0m 229     \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.9296  \u001b[0m | \u001b[0m 1.804   \u001b[0m | \u001b[0m 0.7287  \u001b[0m | \u001b[0m 0.8982  \u001b[0m | \u001b[0m 1.174   \u001b[0m | \u001b[0m 2.3     \u001b[0m | \u001b[0m 0.1292  \u001b[0m | \u001b[0m 47.46   \u001b[0m | \u001b[0m 31.88   \u001b[0m | \u001b[0m 0.001744\u001b[0m | \u001b[0m 41.2    \u001b[0m |\n",
      "| \u001b[0m 230     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.5227  \u001b[0m | \u001b[0m 4.747   \u001b[0m | \u001b[0m 0.7582  \u001b[0m | \u001b[0m 0.8546  \u001b[0m | \u001b[0m 1.966   \u001b[0m | \u001b[0m 0.1654  \u001b[0m | \u001b[0m 0.04218 \u001b[0m | \u001b[0m 42.2    \u001b[0m | \u001b[0m 21.52   \u001b[0m | \u001b[0m 0.00456 \u001b[0m | \u001b[0m 20.13   \u001b[0m |\n",
      "| \u001b[0m 231     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.7729  \u001b[0m | \u001b[0m 1.776   \u001b[0m | \u001b[0m 0.9394  \u001b[0m | \u001b[0m 0.7031  \u001b[0m | \u001b[0m 2.432   \u001b[0m | \u001b[0m 1.327   \u001b[0m | \u001b[0m 0.1478  \u001b[0m | \u001b[0m 11.13   \u001b[0m | \u001b[0m 77.84   \u001b[0m | \u001b[0m 0.008284\u001b[0m | \u001b[0m 77.48   \u001b[0m |\n",
      "| \u001b[0m 232     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.8004  \u001b[0m | \u001b[0m 6.216   \u001b[0m | \u001b[0m 0.7464  \u001b[0m | \u001b[0m 0.8925  \u001b[0m | \u001b[0m 0.9667  \u001b[0m | \u001b[0m 0.2984  \u001b[0m | \u001b[0m 0.1499  \u001b[0m | \u001b[0m 46.91   \u001b[0m | \u001b[0m 95.69   \u001b[0m | \u001b[0m 0.00051 \u001b[0m | \u001b[0m 21.25   \u001b[0m |\n",
      "| \u001b[95m 233     \u001b[0m | \u001b[95m 0.9448  \u001b[0m | \u001b[95m 0.4689  \u001b[0m | \u001b[95m 3.327   \u001b[0m | \u001b[95m 0.5142  \u001b[0m | \u001b[95m 0.5765  \u001b[0m | \u001b[95m 0.1057  \u001b[0m | \u001b[95m 1.772   \u001b[0m | \u001b[95m 0.1851  \u001b[0m | \u001b[95m 74.65   \u001b[0m | \u001b[95m 32.66   \u001b[0m | \u001b[95m 0.0012  \u001b[0m | \u001b[95m 27.55   \u001b[0m |\n",
      "| \u001b[0m 234     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.761   \u001b[0m | \u001b[0m 5.901   \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 0.799   \u001b[0m | \u001b[0m 1.518   \u001b[0m | \u001b[0m 2.006   \u001b[0m | \u001b[0m 0.1896  \u001b[0m | \u001b[0m 24.58   \u001b[0m | \u001b[0m 40.17   \u001b[0m | \u001b[0m 0.003465\u001b[0m | \u001b[0m 31.26   \u001b[0m |\n",
      "| \u001b[0m 235     \u001b[0m | \u001b[0m 0.9404  \u001b[0m | \u001b[0m 0.9389  \u001b[0m | \u001b[0m 7.209   \u001b[0m | \u001b[0m 0.4218  \u001b[0m | \u001b[0m 0.9468  \u001b[0m | \u001b[0m 1.566   \u001b[0m | \u001b[0m 2.142   \u001b[0m | \u001b[0m 0.09338 \u001b[0m | \u001b[0m 80.09   \u001b[0m | \u001b[0m 59.52   \u001b[0m | \u001b[0m 0.005251\u001b[0m | \u001b[0m 82.86   \u001b[0m |\n",
      "| \u001b[0m 236     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.5836  \u001b[0m | \u001b[0m 8.828   \u001b[0m | \u001b[0m 0.4468  \u001b[0m | \u001b[0m 0.8844  \u001b[0m | \u001b[0m 2.193   \u001b[0m | \u001b[0m 1.287   \u001b[0m | \u001b[0m 0.1564  \u001b[0m | \u001b[0m 84.96   \u001b[0m | \u001b[0m 22.91   \u001b[0m | \u001b[0m 0.008229\u001b[0m | \u001b[0m 98.99   \u001b[0m |\n",
      "| \u001b[0m 237     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.884   \u001b[0m | \u001b[0m 1.223   \u001b[0m | \u001b[0m 0.7527  \u001b[0m | \u001b[0m 0.8405  \u001b[0m | \u001b[0m 0.7612  \u001b[0m | \u001b[0m 1.714   \u001b[0m | \u001b[0m 0.1914  \u001b[0m | \u001b[0m 66.39   \u001b[0m | \u001b[0m 67.87   \u001b[0m | \u001b[0m 0.004751\u001b[0m | \u001b[0m 50.16   \u001b[0m |\n",
      "| \u001b[0m 238     \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 0.7061  \u001b[0m | \u001b[0m 2.46    \u001b[0m | \u001b[0m 0.792   \u001b[0m | \u001b[0m 0.8419  \u001b[0m | \u001b[0m 2.765   \u001b[0m | \u001b[0m 2.435   \u001b[0m | \u001b[0m 0.1102  \u001b[0m | \u001b[0m 79.0    \u001b[0m | \u001b[0m 45.47   \u001b[0m | \u001b[0m 0.007898\u001b[0m | \u001b[0m 80.9    \u001b[0m |\n",
      "| \u001b[0m 239     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.6043  \u001b[0m | \u001b[0m 7.911   \u001b[0m | \u001b[0m 0.9153  \u001b[0m | \u001b[0m 0.6017  \u001b[0m | \u001b[0m 0.2972  \u001b[0m | \u001b[0m 1.551   \u001b[0m | \u001b[0m 0.1565  \u001b[0m | \u001b[0m 90.73   \u001b[0m | \u001b[0m 78.16   \u001b[0m | \u001b[0m 0.009813\u001b[0m | \u001b[0m 96.0    \u001b[0m |\n",
      "| \u001b[0m 240     \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.6074  \u001b[0m | \u001b[0m 6.811   \u001b[0m | \u001b[0m 0.4865  \u001b[0m | \u001b[0m 0.6927  \u001b[0m | \u001b[0m 2.93    \u001b[0m | \u001b[0m 1.21    \u001b[0m | \u001b[0m 0.03583 \u001b[0m | \u001b[0m 47.92   \u001b[0m | \u001b[0m 99.93   \u001b[0m | \u001b[0m 0.005684\u001b[0m | \u001b[0m 88.38   \u001b[0m |\n",
      "| \u001b[0m 241     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.8072  \u001b[0m | \u001b[0m 3.791   \u001b[0m | \u001b[0m 0.8229  \u001b[0m | \u001b[0m 0.704   \u001b[0m | \u001b[0m 0.8335  \u001b[0m | \u001b[0m 0.3121  \u001b[0m | \u001b[0m 0.02203 \u001b[0m | \u001b[0m 31.18   \u001b[0m | \u001b[0m 42.24   \u001b[0m | \u001b[0m 0.006606\u001b[0m | \u001b[0m 37.86   \u001b[0m |\n",
      "| \u001b[0m 242     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.9986  \u001b[0m | \u001b[0m 9.263   \u001b[0m | \u001b[0m 0.7085  \u001b[0m | \u001b[0m 0.6924  \u001b[0m | \u001b[0m 1.035   \u001b[0m | \u001b[0m 2.172   \u001b[0m | \u001b[0m 0.05999 \u001b[0m | \u001b[0m 24.6    \u001b[0m | \u001b[0m 59.8    \u001b[0m | \u001b[0m 0.001702\u001b[0m | \u001b[0m 50.65   \u001b[0m |\n",
      "| \u001b[0m 243     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.8189  \u001b[0m | \u001b[0m 7.574   \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.5232  \u001b[0m | \u001b[0m 0.5486  \u001b[0m | \u001b[0m 0.408   \u001b[0m | \u001b[0m 0.01154 \u001b[0m | \u001b[0m 32.96   \u001b[0m | \u001b[0m 25.8    \u001b[0m | \u001b[0m 0.006782\u001b[0m | \u001b[0m 69.36   \u001b[0m |\n",
      "| \u001b[0m 244     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.7976  \u001b[0m | \u001b[0m 5.396   \u001b[0m | \u001b[0m 0.6209  \u001b[0m | \u001b[0m 0.8446  \u001b[0m | \u001b[0m 0.6516  \u001b[0m | \u001b[0m 1.48    \u001b[0m | \u001b[0m 0.05185 \u001b[0m | \u001b[0m 42.58   \u001b[0m | \u001b[0m 58.5    \u001b[0m | \u001b[0m 0.00237 \u001b[0m | \u001b[0m 62.0    \u001b[0m |\n",
      "| \u001b[0m 245     \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.537   \u001b[0m | \u001b[0m 3.595   \u001b[0m | \u001b[0m 0.9884  \u001b[0m | \u001b[0m 0.7924  \u001b[0m | \u001b[0m 0.5073  \u001b[0m | \u001b[0m 0.146   \u001b[0m | \u001b[0m 0.01978 \u001b[0m | \u001b[0m 39.95   \u001b[0m | \u001b[0m 67.46   \u001b[0m | \u001b[0m 0.002299\u001b[0m | \u001b[0m 87.69   \u001b[0m |\n",
      "| \u001b[0m 246     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.5661  \u001b[0m | \u001b[0m 1.702   \u001b[0m | \u001b[0m 0.8228  \u001b[0m | \u001b[0m 0.8126  \u001b[0m | \u001b[0m 1.011   \u001b[0m | \u001b[0m 0.8386  \u001b[0m | \u001b[0m 0.1259  \u001b[0m | \u001b[0m 38.76   \u001b[0m | \u001b[0m 56.67   \u001b[0m | \u001b[0m 0.000711\u001b[0m | \u001b[0m 68.07   \u001b[0m |\n",
      "| \u001b[0m 247     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.8208  \u001b[0m | \u001b[0m 6.389   \u001b[0m | \u001b[0m 0.5192  \u001b[0m | \u001b[0m 0.4169  \u001b[0m | \u001b[0m 1.114   \u001b[0m | \u001b[0m 0.6282  \u001b[0m | \u001b[0m 0.1388  \u001b[0m | \u001b[0m 46.68   \u001b[0m | \u001b[0m 59.4    \u001b[0m | \u001b[0m 0.004461\u001b[0m | \u001b[0m 40.3    \u001b[0m |\n",
      "| \u001b[0m 248     \u001b[0m | \u001b[0m 0.9411  \u001b[0m | \u001b[0m 0.614   \u001b[0m | \u001b[0m 9.36    \u001b[0m | \u001b[0m 0.7004  \u001b[0m | \u001b[0m 0.4469  \u001b[0m | \u001b[0m 2.919   \u001b[0m | \u001b[0m 0.5705  \u001b[0m | \u001b[0m 0.06658 \u001b[0m | \u001b[0m 76.24   \u001b[0m | \u001b[0m 89.25   \u001b[0m | \u001b[0m 0.007615\u001b[0m | \u001b[0m 72.42   \u001b[0m |\n",
      "| \u001b[0m 249     \u001b[0m | \u001b[0m 0.9401  \u001b[0m | \u001b[0m 0.5094  \u001b[0m | \u001b[0m 7.315   \u001b[0m | \u001b[0m 0.6477  \u001b[0m | \u001b[0m 0.4719  \u001b[0m | \u001b[0m 2.649   \u001b[0m | \u001b[0m 2.182   \u001b[0m | \u001b[0m 0.06917 \u001b[0m | \u001b[0m 56.53   \u001b[0m | \u001b[0m 52.32   \u001b[0m | \u001b[0m 0.001955\u001b[0m | \u001b[0m 53.58   \u001b[0m |\n",
      "| \u001b[0m 250     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.5575  \u001b[0m | \u001b[0m 2.942   \u001b[0m | \u001b[0m 0.6805  \u001b[0m | \u001b[0m 0.6278  \u001b[0m | \u001b[0m 0.8513  \u001b[0m | \u001b[0m 1.742   \u001b[0m | \u001b[0m 0.1235  \u001b[0m | \u001b[0m 77.47   \u001b[0m | \u001b[0m 50.79   \u001b[0m | \u001b[0m 0.008411\u001b[0m | \u001b[0m 75.05   \u001b[0m |\n",
      "| \u001b[0m 251     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.6278  \u001b[0m | \u001b[0m 9.468   \u001b[0m | \u001b[0m 0.6178  \u001b[0m | \u001b[0m 0.6135  \u001b[0m | \u001b[0m 0.7546  \u001b[0m | \u001b[0m 0.02133 \u001b[0m | \u001b[0m 0.08432 \u001b[0m | \u001b[0m 60.87   \u001b[0m | \u001b[0m 19.26   \u001b[0m | \u001b[0m 0.00279 \u001b[0m | \u001b[0m 91.88   \u001b[0m |\n",
      "| \u001b[0m 252     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.5346  \u001b[0m | \u001b[0m 4.297   \u001b[0m | \u001b[0m 0.863   \u001b[0m | \u001b[0m 0.4176  \u001b[0m | \u001b[0m 0.8447  \u001b[0m | \u001b[0m 1.424   \u001b[0m | \u001b[0m 0.1739  \u001b[0m | \u001b[0m 60.14   \u001b[0m | \u001b[0m 86.67   \u001b[0m | \u001b[0m 0.003483\u001b[0m | \u001b[0m 20.31   \u001b[0m |\n",
      "| \u001b[0m 253     \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.889   \u001b[0m | \u001b[0m 9.394   \u001b[0m | \u001b[0m 0.7427  \u001b[0m | \u001b[0m 0.7631  \u001b[0m | \u001b[0m 0.5829  \u001b[0m | \u001b[0m 2.841   \u001b[0m | \u001b[0m 0.1361  \u001b[0m | \u001b[0m 22.1    \u001b[0m | \u001b[0m 84.74   \u001b[0m | \u001b[0m 0.001627\u001b[0m | \u001b[0m 49.16   \u001b[0m |\n",
      "| \u001b[0m 254     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.6069  \u001b[0m | \u001b[0m 2.626   \u001b[0m | \u001b[0m 0.8689  \u001b[0m | \u001b[0m 0.5372  \u001b[0m | \u001b[0m 2.394   \u001b[0m | \u001b[0m 2.217   \u001b[0m | \u001b[0m 0.1488  \u001b[0m | \u001b[0m 50.17   \u001b[0m | \u001b[0m 64.95   \u001b[0m | \u001b[0m 0.000657\u001b[0m | \u001b[0m 99.81   \u001b[0m |\n",
      "| \u001b[0m 255     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.4893  \u001b[0m | \u001b[0m 8.96    \u001b[0m | \u001b[0m 0.6351  \u001b[0m | \u001b[0m 0.5556  \u001b[0m | \u001b[0m 2.333   \u001b[0m | \u001b[0m 2.723   \u001b[0m | \u001b[0m 0.1133  \u001b[0m | \u001b[0m 13.38   \u001b[0m | \u001b[0m 68.32   \u001b[0m | \u001b[0m 0.006496\u001b[0m | \u001b[0m 47.81   \u001b[0m |\n",
      "| \u001b[0m 256     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.5647  \u001b[0m | \u001b[0m 7.778   \u001b[0m | \u001b[0m 0.9155  \u001b[0m | \u001b[0m 0.798   \u001b[0m | \u001b[0m 2.848   \u001b[0m | \u001b[0m 1.399   \u001b[0m | \u001b[0m 0.1612  \u001b[0m | \u001b[0m 65.77   \u001b[0m | \u001b[0m 74.59   \u001b[0m | \u001b[0m 0.009423\u001b[0m | \u001b[0m 29.32   \u001b[0m |\n",
      "| \u001b[0m 257     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.7122  \u001b[0m | \u001b[0m 4.006   \u001b[0m | \u001b[0m 0.6773  \u001b[0m | \u001b[0m 0.9208  \u001b[0m | \u001b[0m 2.329   \u001b[0m | \u001b[0m 2.106   \u001b[0m | \u001b[0m 0.02798 \u001b[0m | \u001b[0m 8.91    \u001b[0m | \u001b[0m 42.35   \u001b[0m | \u001b[0m 0.00184 \u001b[0m | \u001b[0m 52.19   \u001b[0m |\n",
      "| \u001b[0m 258     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.9639  \u001b[0m | \u001b[0m 7.357   \u001b[0m | \u001b[0m 0.8123  \u001b[0m | \u001b[0m 0.7495  \u001b[0m | \u001b[0m 0.6392  \u001b[0m | \u001b[0m 2.248   \u001b[0m | \u001b[0m 0.06422 \u001b[0m | \u001b[0m 72.52   \u001b[0m | \u001b[0m 40.42   \u001b[0m | \u001b[0m 0.005118\u001b[0m | \u001b[0m 23.21   \u001b[0m |\n",
      "| \u001b[0m 259     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.6201  \u001b[0m | \u001b[0m 4.64    \u001b[0m | \u001b[0m 0.9287  \u001b[0m | \u001b[0m 0.4679  \u001b[0m | \u001b[0m 0.9532  \u001b[0m | \u001b[0m 1.14    \u001b[0m | \u001b[0m 0.1955  \u001b[0m | \u001b[0m 99.68   \u001b[0m | \u001b[0m 86.04   \u001b[0m | \u001b[0m 0.00755 \u001b[0m | \u001b[0m 79.56   \u001b[0m |\n",
      "| \u001b[0m 260     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.664   \u001b[0m | \u001b[0m 8.518   \u001b[0m | \u001b[0m 0.7144  \u001b[0m | \u001b[0m 0.431   \u001b[0m | \u001b[0m 1.598   \u001b[0m | \u001b[0m 0.9443  \u001b[0m | \u001b[0m 0.1791  \u001b[0m | \u001b[0m 20.27   \u001b[0m | \u001b[0m 68.53   \u001b[0m | \u001b[0m 0.001299\u001b[0m | \u001b[0m 85.33   \u001b[0m |\n",
      "| \u001b[0m 261     \u001b[0m | \u001b[0m 0.9404  \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 1.773   \u001b[0m | \u001b[0m 0.6156  \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 2.322   \u001b[0m | \u001b[0m 1.817   \u001b[0m | \u001b[0m 0.08813 \u001b[0m | \u001b[0m 64.69   \u001b[0m | \u001b[0m 41.56   \u001b[0m | \u001b[0m 0.006462\u001b[0m | \u001b[0m 44.21   \u001b[0m |\n",
      "| \u001b[0m 262     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.7808  \u001b[0m | \u001b[0m 8.51    \u001b[0m | \u001b[0m 0.5906  \u001b[0m | \u001b[0m 0.9655  \u001b[0m | \u001b[0m 0.8962  \u001b[0m | \u001b[0m 2.966   \u001b[0m | \u001b[0m 0.04199 \u001b[0m | \u001b[0m 87.57   \u001b[0m | \u001b[0m 87.1    \u001b[0m | \u001b[0m 0.008298\u001b[0m | \u001b[0m 41.15   \u001b[0m |\n",
      "| \u001b[0m 263     \u001b[0m | \u001b[0m 0.9349  \u001b[0m | \u001b[0m 0.9049  \u001b[0m | \u001b[0m 7.509   \u001b[0m | \u001b[0m 0.4563  \u001b[0m | \u001b[0m 0.9965  \u001b[0m | \u001b[0m 0.1748  \u001b[0m | \u001b[0m 0.4626  \u001b[0m | \u001b[0m 0.1382  \u001b[0m | \u001b[0m 38.28   \u001b[0m | \u001b[0m 23.14   \u001b[0m | \u001b[0m 0.002024\u001b[0m | \u001b[0m 60.87   \u001b[0m |\n",
      "| \u001b[0m 264     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.4409  \u001b[0m | \u001b[0m 9.359   \u001b[0m | \u001b[0m 0.7031  \u001b[0m | \u001b[0m 0.5564  \u001b[0m | \u001b[0m 1.864   \u001b[0m | \u001b[0m 1.695   \u001b[0m | \u001b[0m 0.1378  \u001b[0m | \u001b[0m 58.49   \u001b[0m | \u001b[0m 42.67   \u001b[0m | \u001b[0m 0.004156\u001b[0m | \u001b[0m 53.06   \u001b[0m |\n",
      "| \u001b[0m 265     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.6163  \u001b[0m | \u001b[0m 2.603   \u001b[0m | \u001b[0m 0.7262  \u001b[0m | \u001b[0m 0.4442  \u001b[0m | \u001b[0m 2.548   \u001b[0m | \u001b[0m 2.617   \u001b[0m | \u001b[0m 0.1442  \u001b[0m | \u001b[0m 29.17   \u001b[0m | \u001b[0m 64.34   \u001b[0m | \u001b[0m 0.007976\u001b[0m | \u001b[0m 59.31   \u001b[0m |\n",
      "| \u001b[0m 266     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.5648  \u001b[0m | \u001b[0m 4.122   \u001b[0m | \u001b[0m 0.7968  \u001b[0m | \u001b[0m 0.5524  \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 2.539   \u001b[0m | \u001b[0m 0.07271 \u001b[0m | \u001b[0m 19.85   \u001b[0m | \u001b[0m 40.38   \u001b[0m | \u001b[0m 0.001577\u001b[0m | \u001b[0m 30.1    \u001b[0m |\n",
      "| \u001b[0m 267     \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.5335  \u001b[0m | \u001b[0m 4.07    \u001b[0m | \u001b[0m 0.7567  \u001b[0m | \u001b[0m 0.7384  \u001b[0m | \u001b[0m 1.997   \u001b[0m | \u001b[0m 0.8024  \u001b[0m | \u001b[0m 0.1882  \u001b[0m | \u001b[0m 70.3    \u001b[0m | \u001b[0m 58.92   \u001b[0m | \u001b[0m 0.009282\u001b[0m | \u001b[0m 36.16   \u001b[0m |\n",
      "| \u001b[0m 268     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.8249  \u001b[0m | \u001b[0m 2.148   \u001b[0m | \u001b[0m 0.4417  \u001b[0m | \u001b[0m 0.4103  \u001b[0m | \u001b[0m 1.5     \u001b[0m | \u001b[0m 2.871   \u001b[0m | \u001b[0m 0.0295  \u001b[0m | \u001b[0m 77.07   \u001b[0m | \u001b[0m 13.81   \u001b[0m | \u001b[0m 0.009068\u001b[0m | \u001b[0m 81.89   \u001b[0m |\n",
      "| \u001b[0m 269     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.9662  \u001b[0m | \u001b[0m 3.794   \u001b[0m | \u001b[0m 0.8487  \u001b[0m | \u001b[0m 0.4132  \u001b[0m | \u001b[0m 1.006   \u001b[0m | \u001b[0m 1.212   \u001b[0m | \u001b[0m 0.1257  \u001b[0m | \u001b[0m 46.04   \u001b[0m | \u001b[0m 95.67   \u001b[0m | \u001b[0m 0.008889\u001b[0m | \u001b[0m 89.15   \u001b[0m |\n",
      "| \u001b[0m 270     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.7594  \u001b[0m | \u001b[0m 4.218   \u001b[0m | \u001b[0m 0.9497  \u001b[0m | \u001b[0m 0.8459  \u001b[0m | \u001b[0m 1.897   \u001b[0m | \u001b[0m 0.01614 \u001b[0m | \u001b[0m 0.0324  \u001b[0m | \u001b[0m 49.94   \u001b[0m | \u001b[0m 62.56   \u001b[0m | \u001b[0m 0.00991 \u001b[0m | \u001b[0m 55.88   \u001b[0m |\n",
      "| \u001b[0m 271     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.6652  \u001b[0m | \u001b[0m 1.324   \u001b[0m | \u001b[0m 0.8748  \u001b[0m | \u001b[0m 0.8314  \u001b[0m | \u001b[0m 0.7429  \u001b[0m | \u001b[0m 1.771   \u001b[0m | \u001b[0m 0.01321 \u001b[0m | \u001b[0m 26.68   \u001b[0m | \u001b[0m 77.84   \u001b[0m | \u001b[0m 0.002054\u001b[0m | \u001b[0m 74.5    \u001b[0m |\n",
      "| \u001b[0m 272     \u001b[0m | \u001b[0m 0.9408  \u001b[0m | \u001b[0m 0.6865  \u001b[0m | \u001b[0m 4.635   \u001b[0m | \u001b[0m 0.6294  \u001b[0m | \u001b[0m 0.9334  \u001b[0m | \u001b[0m 2.452   \u001b[0m | \u001b[0m 1.702   \u001b[0m | \u001b[0m 0.01099 \u001b[0m | \u001b[0m 74.61   \u001b[0m | \u001b[0m 72.08   \u001b[0m | \u001b[0m 0.008124\u001b[0m | \u001b[0m 60.06   \u001b[0m |\n",
      "| \u001b[0m 273     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.87    \u001b[0m | \u001b[0m 6.336   \u001b[0m | \u001b[0m 0.6226  \u001b[0m | \u001b[0m 0.6667  \u001b[0m | \u001b[0m 1.402   \u001b[0m | \u001b[0m 1.719   \u001b[0m | \u001b[0m 0.1744  \u001b[0m | \u001b[0m 26.47   \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.009017\u001b[0m | \u001b[0m 93.75   \u001b[0m |\n",
      "| \u001b[0m 274     \u001b[0m | \u001b[0m 0.9401  \u001b[0m | \u001b[0m 0.4071  \u001b[0m | \u001b[0m 4.432   \u001b[0m | \u001b[0m 0.5411  \u001b[0m | \u001b[0m 0.4056  \u001b[0m | \u001b[0m 2.309   \u001b[0m | \u001b[0m 2.966   \u001b[0m | \u001b[0m 0.1115  \u001b[0m | \u001b[0m 35.51   \u001b[0m | \u001b[0m 83.2    \u001b[0m | \u001b[0m 0.002683\u001b[0m | \u001b[0m 70.68   \u001b[0m |\n",
      "| \u001b[0m 275     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.4634  \u001b[0m | \u001b[0m 4.069   \u001b[0m | \u001b[0m 0.6924  \u001b[0m | \u001b[0m 0.5033  \u001b[0m | \u001b[0m 1.508   \u001b[0m | \u001b[0m 0.1758  \u001b[0m | \u001b[0m 0.08468 \u001b[0m | \u001b[0m 60.88   \u001b[0m | \u001b[0m 89.55   \u001b[0m | \u001b[0m 0.008117\u001b[0m | \u001b[0m 48.22   \u001b[0m |\n",
      "| \u001b[0m 276     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.659   \u001b[0m | \u001b[0m 8.725   \u001b[0m | \u001b[0m 0.7632  \u001b[0m | \u001b[0m 0.8468  \u001b[0m | \u001b[0m 0.5554  \u001b[0m | \u001b[0m 1.569   \u001b[0m | \u001b[0m 0.1166  \u001b[0m | \u001b[0m 8.354   \u001b[0m | \u001b[0m 49.24   \u001b[0m | \u001b[0m 0.000371\u001b[0m | \u001b[0m 47.07   \u001b[0m |\n",
      "| \u001b[0m 277     \u001b[0m | \u001b[0m 0.9396  \u001b[0m | \u001b[0m 0.4638  \u001b[0m | \u001b[0m 1.518   \u001b[0m | \u001b[0m 0.7355  \u001b[0m | \u001b[0m 0.9632  \u001b[0m | \u001b[0m 2.911   \u001b[0m | \u001b[0m 2.729   \u001b[0m | \u001b[0m 0.1016  \u001b[0m | \u001b[0m 22.79   \u001b[0m | \u001b[0m 77.63   \u001b[0m | \u001b[0m 0.009436\u001b[0m | \u001b[0m 32.1    \u001b[0m |\n",
      "| \u001b[0m 278     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.9712  \u001b[0m | \u001b[0m 4.854   \u001b[0m | \u001b[0m 0.4142  \u001b[0m | \u001b[0m 0.9511  \u001b[0m | \u001b[0m 0.4144  \u001b[0m | \u001b[0m 0.02119 \u001b[0m | \u001b[0m 0.1145  \u001b[0m | \u001b[0m 47.93   \u001b[0m | \u001b[0m 77.95   \u001b[0m | \u001b[0m 0.009367\u001b[0m | \u001b[0m 75.97   \u001b[0m |\n",
      "| \u001b[0m 279     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.9131  \u001b[0m | \u001b[0m 2.445   \u001b[0m | \u001b[0m 0.478   \u001b[0m | \u001b[0m 0.9918  \u001b[0m | \u001b[0m 0.6026  \u001b[0m | \u001b[0m 1.169   \u001b[0m | \u001b[0m 0.1105  \u001b[0m | \u001b[0m 46.22   \u001b[0m | \u001b[0m 58.36   \u001b[0m | \u001b[0m 0.008673\u001b[0m | \u001b[0m 24.47   \u001b[0m |\n",
      "| \u001b[0m 280     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.7087  \u001b[0m | \u001b[0m 5.672   \u001b[0m | \u001b[0m 0.6816  \u001b[0m | \u001b[0m 0.6451  \u001b[0m | \u001b[0m 0.4183  \u001b[0m | \u001b[0m 0.3486  \u001b[0m | \u001b[0m 0.1984  \u001b[0m | \u001b[0m 33.33   \u001b[0m | \u001b[0m 12.14   \u001b[0m | \u001b[0m 0.003077\u001b[0m | \u001b[0m 92.41   \u001b[0m |\n",
      "| \u001b[0m 281     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.7444  \u001b[0m | \u001b[0m 2.847   \u001b[0m | \u001b[0m 0.6903  \u001b[0m | \u001b[0m 0.7903  \u001b[0m | \u001b[0m 1.522   \u001b[0m | \u001b[0m 1.363   \u001b[0m | \u001b[0m 0.04732 \u001b[0m | \u001b[0m 60.8    \u001b[0m | \u001b[0m 98.38   \u001b[0m | \u001b[0m 0.002352\u001b[0m | \u001b[0m 31.72   \u001b[0m |\n",
      "| \u001b[0m 282     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.5481  \u001b[0m | \u001b[0m 3.017   \u001b[0m | \u001b[0m 0.9759  \u001b[0m | \u001b[0m 0.896   \u001b[0m | \u001b[0m 0.7397  \u001b[0m | \u001b[0m 1.374   \u001b[0m | \u001b[0m 0.1488  \u001b[0m | \u001b[0m 17.11   \u001b[0m | \u001b[0m 47.64   \u001b[0m | \u001b[0m 0.000775\u001b[0m | \u001b[0m 62.13   \u001b[0m |\n",
      "| \u001b[0m 283     \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.4681  \u001b[0m | \u001b[0m 5.65    \u001b[0m | \u001b[0m 0.9969  \u001b[0m | \u001b[0m 0.6661  \u001b[0m | \u001b[0m 2.511   \u001b[0m | \u001b[0m 1.131   \u001b[0m | \u001b[0m 0.1018  \u001b[0m | \u001b[0m 96.31   \u001b[0m | \u001b[0m 21.85   \u001b[0m | \u001b[0m 0.008527\u001b[0m | \u001b[0m 68.26   \u001b[0m |\n",
      "| \u001b[0m 284     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.7333  \u001b[0m | \u001b[0m 8.536   \u001b[0m | \u001b[0m 0.9808  \u001b[0m | \u001b[0m 0.755   \u001b[0m | \u001b[0m 0.364   \u001b[0m | \u001b[0m 2.123   \u001b[0m | \u001b[0m 0.06983 \u001b[0m | \u001b[0m 20.87   \u001b[0m | \u001b[0m 36.77   \u001b[0m | \u001b[0m 0.004708\u001b[0m | \u001b[0m 94.06   \u001b[0m |\n",
      "| \u001b[0m 285     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.6093  \u001b[0m | \u001b[0m 9.361   \u001b[0m | \u001b[0m 0.6557  \u001b[0m | \u001b[0m 0.5216  \u001b[0m | \u001b[0m 2.196   \u001b[0m | \u001b[0m 0.7159  \u001b[0m | \u001b[0m 0.06195 \u001b[0m | \u001b[0m 78.72   \u001b[0m | \u001b[0m 45.13   \u001b[0m | \u001b[0m 0.000162\u001b[0m | \u001b[0m 57.2    \u001b[0m |\n",
      "| \u001b[0m 286     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.8131  \u001b[0m | \u001b[0m 8.761   \u001b[0m | \u001b[0m 0.4351  \u001b[0m | \u001b[0m 0.9188  \u001b[0m | \u001b[0m 0.927   \u001b[0m | \u001b[0m 1.19    \u001b[0m | \u001b[0m 0.03127 \u001b[0m | \u001b[0m 77.87   \u001b[0m | \u001b[0m 82.89   \u001b[0m | \u001b[0m 0.003164\u001b[0m | \u001b[0m 52.64   \u001b[0m |\n",
      "| \u001b[0m 287     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.7983  \u001b[0m | \u001b[0m 9.979   \u001b[0m | \u001b[0m 0.4583  \u001b[0m | \u001b[0m 0.6439  \u001b[0m | \u001b[0m 1.493   \u001b[0m | \u001b[0m 0.9214  \u001b[0m | \u001b[0m 0.04832 \u001b[0m | \u001b[0m 93.11   \u001b[0m | \u001b[0m 66.38   \u001b[0m | \u001b[0m 0.002908\u001b[0m | \u001b[0m 92.84   \u001b[0m |\n",
      "| \u001b[0m 288     \u001b[0m | \u001b[0m 0.9326  \u001b[0m | \u001b[0m 0.9123  \u001b[0m | \u001b[0m 6.928   \u001b[0m | \u001b[0m 0.4294  \u001b[0m | \u001b[0m 0.5423  \u001b[0m | \u001b[0m 0.7509  \u001b[0m | \u001b[0m 2.951   \u001b[0m | \u001b[0m 0.09685 \u001b[0m | \u001b[0m 93.64   \u001b[0m | \u001b[0m 37.59   \u001b[0m | \u001b[0m 0.009288\u001b[0m | \u001b[0m 54.09   \u001b[0m |\n",
      "| \u001b[0m 289     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.4918  \u001b[0m | \u001b[0m 7.839   \u001b[0m | \u001b[0m 0.4573  \u001b[0m | \u001b[0m 0.6954  \u001b[0m | \u001b[0m 1.685   \u001b[0m | \u001b[0m 2.707   \u001b[0m | \u001b[0m 0.1386  \u001b[0m | \u001b[0m 74.51   \u001b[0m | \u001b[0m 35.88   \u001b[0m | \u001b[0m 0.0093  \u001b[0m | \u001b[0m 42.05   \u001b[0m |\n",
      "| \u001b[0m 290     \u001b[0m | \u001b[0m 0.9392  \u001b[0m | \u001b[0m 0.9472  \u001b[0m | \u001b[0m 2.064   \u001b[0m | \u001b[0m 0.9201  \u001b[0m | \u001b[0m 0.4968  \u001b[0m | \u001b[0m 0.4947  \u001b[0m | \u001b[0m 0.1136  \u001b[0m | \u001b[0m 0.1853  \u001b[0m | \u001b[0m 18.03   \u001b[0m | \u001b[0m 36.59   \u001b[0m | \u001b[0m 0.000279\u001b[0m | \u001b[0m 60.35   \u001b[0m |\n",
      "| \u001b[0m 291     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.9742  \u001b[0m | \u001b[0m 7.191   \u001b[0m | \u001b[0m 0.6474  \u001b[0m | \u001b[0m 0.6752  \u001b[0m | \u001b[0m 1.213   \u001b[0m | \u001b[0m 0.04203 \u001b[0m | \u001b[0m 0.16    \u001b[0m | \u001b[0m 68.96   \u001b[0m | \u001b[0m 84.7    \u001b[0m | \u001b[0m 0.006227\u001b[0m | \u001b[0m 57.55   \u001b[0m |\n",
      "| \u001b[0m 292     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.8897  \u001b[0m | \u001b[0m 2.887   \u001b[0m | \u001b[0m 0.9294  \u001b[0m | \u001b[0m 0.807   \u001b[0m | \u001b[0m 1.478   \u001b[0m | \u001b[0m 1.201   \u001b[0m | \u001b[0m 0.04727 \u001b[0m | \u001b[0m 32.06   \u001b[0m | \u001b[0m 65.01   \u001b[0m | \u001b[0m 0.004382\u001b[0m | \u001b[0m 46.27   \u001b[0m |\n",
      "| \u001b[0m 293     \u001b[0m | \u001b[0m 0.9384  \u001b[0m | \u001b[0m 0.4729  \u001b[0m | \u001b[0m 6.031   \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.5805  \u001b[0m | \u001b[0m 2.786   \u001b[0m | \u001b[0m 2.32    \u001b[0m | \u001b[0m 0.08098 \u001b[0m | \u001b[0m 31.37   \u001b[0m | \u001b[0m 41.46   \u001b[0m | \u001b[0m 0.007193\u001b[0m | \u001b[0m 43.34   \u001b[0m |\n",
      "| \u001b[0m 294     \u001b[0m | \u001b[0m 0.9389  \u001b[0m | \u001b[0m 0.4909  \u001b[0m | \u001b[0m 6.023   \u001b[0m | \u001b[0m 0.9151  \u001b[0m | \u001b[0m 0.916   \u001b[0m | \u001b[0m 1.656   \u001b[0m | \u001b[0m 0.09907 \u001b[0m | \u001b[0m 0.06682 \u001b[0m | \u001b[0m 91.72   \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 0.000369\u001b[0m | \u001b[0m 55.66   \u001b[0m |\n",
      "| \u001b[0m 295     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.8398  \u001b[0m | \u001b[0m 4.265   \u001b[0m | \u001b[0m 0.5215  \u001b[0m | \u001b[0m 0.8364  \u001b[0m | \u001b[0m 0.5563  \u001b[0m | \u001b[0m 1.341   \u001b[0m | \u001b[0m 0.08586 \u001b[0m | \u001b[0m 17.19   \u001b[0m | \u001b[0m 31.71   \u001b[0m | \u001b[0m 0.00538 \u001b[0m | \u001b[0m 62.4    \u001b[0m |\n",
      "| \u001b[0m 296     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.8853  \u001b[0m | \u001b[0m 9.182   \u001b[0m | \u001b[0m 0.5133  \u001b[0m | \u001b[0m 0.9247  \u001b[0m | \u001b[0m 0.9816  \u001b[0m | \u001b[0m 0.1799  \u001b[0m | \u001b[0m 0.1435  \u001b[0m | \u001b[0m 93.32   \u001b[0m | \u001b[0m 75.23   \u001b[0m | \u001b[0m 0.002091\u001b[0m | \u001b[0m 72.35   \u001b[0m |\n",
      "| \u001b[0m 297     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.7864  \u001b[0m | \u001b[0m 7.545   \u001b[0m | \u001b[0m 0.6965  \u001b[0m | \u001b[0m 0.6469  \u001b[0m | \u001b[0m 2.676   \u001b[0m | \u001b[0m 0.8787  \u001b[0m | \u001b[0m 0.09707 \u001b[0m | \u001b[0m 29.62   \u001b[0m | \u001b[0m 47.31   \u001b[0m | \u001b[0m 0.001972\u001b[0m | \u001b[0m 98.94   \u001b[0m |\n",
      "| \u001b[0m 298     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.7329  \u001b[0m | \u001b[0m 8.572   \u001b[0m | \u001b[0m 0.4784  \u001b[0m | \u001b[0m 0.9124  \u001b[0m | \u001b[0m 1.95    \u001b[0m | \u001b[0m 1.225   \u001b[0m | \u001b[0m 0.05122 \u001b[0m | \u001b[0m 22.79   \u001b[0m | \u001b[0m 33.58   \u001b[0m | \u001b[0m 0.004158\u001b[0m | \u001b[0m 43.32   \u001b[0m |\n",
      "| \u001b[0m 299     \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.4093  \u001b[0m | \u001b[0m 5.412   \u001b[0m | \u001b[0m 0.6021  \u001b[0m | \u001b[0m 0.8428  \u001b[0m | \u001b[0m 1.863   \u001b[0m | \u001b[0m 1.616   \u001b[0m | \u001b[0m 0.1538  \u001b[0m | \u001b[0m 99.66   \u001b[0m | \u001b[0m 68.66   \u001b[0m | \u001b[0m 0.001965\u001b[0m | \u001b[0m 53.12   \u001b[0m |\n",
      "| \u001b[0m 300     \u001b[0m | \u001b[0m 0.9416  \u001b[0m | \u001b[0m 0.7769  \u001b[0m | \u001b[0m 6.436   \u001b[0m | \u001b[0m 0.5945  \u001b[0m | \u001b[0m 0.8609  \u001b[0m | \u001b[0m 2.669   \u001b[0m | \u001b[0m 0.7711  \u001b[0m | \u001b[0m 0.09108 \u001b[0m | \u001b[0m 47.57   \u001b[0m | \u001b[0m 43.28   \u001b[0m | \u001b[0m 0.004873\u001b[0m | \u001b[0m 85.26   \u001b[0m |\n",
      "| \u001b[0m 301     \u001b[0m | \u001b[0m 0.9447  \u001b[0m | \u001b[0m 0.6137  \u001b[0m | \u001b[0m 2.761   \u001b[0m | \u001b[0m 0.6248  \u001b[0m | \u001b[0m 0.6311  \u001b[0m | \u001b[0m 0.6976  \u001b[0m | \u001b[0m 0.3626  \u001b[0m | \u001b[0m 0.1278  \u001b[0m | \u001b[0m 31.34   \u001b[0m | \u001b[0m 34.51   \u001b[0m | \u001b[0m 0.007437\u001b[0m | \u001b[0m 95.29   \u001b[0m |\n",
      "| \u001b[0m 302     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.9817  \u001b[0m | \u001b[0m 3.309   \u001b[0m | \u001b[0m 0.8107  \u001b[0m | \u001b[0m 0.617   \u001b[0m | \u001b[0m 0.205   \u001b[0m | \u001b[0m 0.6184  \u001b[0m | \u001b[0m 0.1209  \u001b[0m | \u001b[0m 79.68   \u001b[0m | \u001b[0m 44.51   \u001b[0m | \u001b[0m 0.000390\u001b[0m | \u001b[0m 54.59   \u001b[0m |\n",
      "| \u001b[0m 303     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.4907  \u001b[0m | \u001b[0m 7.231   \u001b[0m | \u001b[0m 0.7531  \u001b[0m | \u001b[0m 0.683   \u001b[0m | \u001b[0m 1.807   \u001b[0m | \u001b[0m 0.03101 \u001b[0m | \u001b[0m 0.05114 \u001b[0m | \u001b[0m 64.03   \u001b[0m | \u001b[0m 60.12   \u001b[0m | \u001b[0m 0.001354\u001b[0m | \u001b[0m 83.54   \u001b[0m |\n",
      "| \u001b[0m 304     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.8793  \u001b[0m | \u001b[0m 9.619   \u001b[0m | \u001b[0m 0.4243  \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 2.175   \u001b[0m | \u001b[0m 0.6106  \u001b[0m | \u001b[0m 0.149   \u001b[0m | \u001b[0m 30.93   \u001b[0m | \u001b[0m 43.04   \u001b[0m | \u001b[0m 0.002465\u001b[0m | \u001b[0m 76.0    \u001b[0m |\n",
      "| \u001b[0m 305     \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.4532  \u001b[0m | \u001b[0m 5.363   \u001b[0m | \u001b[0m 0.756   \u001b[0m | \u001b[0m 0.7071  \u001b[0m | \u001b[0m 2.812   \u001b[0m | \u001b[0m 0.9772  \u001b[0m | \u001b[0m 0.09794 \u001b[0m | \u001b[0m 69.42   \u001b[0m | \u001b[0m 29.65   \u001b[0m | \u001b[0m 0.002253\u001b[0m | \u001b[0m 74.89   \u001b[0m |\n",
      "| \u001b[0m 306     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.5033  \u001b[0m | \u001b[0m 5.653   \u001b[0m | \u001b[0m 0.6514  \u001b[0m | \u001b[0m 0.5668  \u001b[0m | \u001b[0m 1.482   \u001b[0m | \u001b[0m 2.071   \u001b[0m | \u001b[0m 0.04899 \u001b[0m | \u001b[0m 97.67   \u001b[0m | \u001b[0m 40.27   \u001b[0m | \u001b[0m 0.004762\u001b[0m | \u001b[0m 80.55   \u001b[0m |\n",
      "| \u001b[0m 307     \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 0.9454  \u001b[0m | \u001b[0m 5.883   \u001b[0m | \u001b[0m 0.6517  \u001b[0m | \u001b[0m 0.9031  \u001b[0m | \u001b[0m 1.931   \u001b[0m | \u001b[0m 2.046   \u001b[0m | \u001b[0m 0.1248  \u001b[0m | \u001b[0m 62.51   \u001b[0m | \u001b[0m 94.35   \u001b[0m | \u001b[0m 0.007292\u001b[0m | \u001b[0m 87.77   \u001b[0m |\n",
      "| \u001b[0m 308     \u001b[0m | \u001b[0m 0.9405  \u001b[0m | \u001b[0m 0.4371  \u001b[0m | \u001b[0m 4.738   \u001b[0m | \u001b[0m 0.6436  \u001b[0m | \u001b[0m 0.8442  \u001b[0m | \u001b[0m 2.046   \u001b[0m | \u001b[0m 1.381   \u001b[0m | \u001b[0m 0.02064 \u001b[0m | \u001b[0m 76.27   \u001b[0m | \u001b[0m 14.76   \u001b[0m | \u001b[0m 0.00854 \u001b[0m | \u001b[0m 29.08   \u001b[0m |\n",
      "| \u001b[0m 309     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.7275  \u001b[0m | \u001b[0m 7.127   \u001b[0m | \u001b[0m 0.7512  \u001b[0m | \u001b[0m 0.5195  \u001b[0m | \u001b[0m 0.933   \u001b[0m | \u001b[0m 2.163   \u001b[0m | \u001b[0m 0.07263 \u001b[0m | \u001b[0m 96.55   \u001b[0m | \u001b[0m 39.94   \u001b[0m | \u001b[0m 0.005163\u001b[0m | \u001b[0m 87.16   \u001b[0m |\n",
      "| \u001b[0m 310     \u001b[0m | \u001b[0m 0.9443  \u001b[0m | \u001b[0m 0.474   \u001b[0m | \u001b[0m 3.632   \u001b[0m | \u001b[0m 0.6284  \u001b[0m | \u001b[0m 0.7102  \u001b[0m | \u001b[0m 0.4205  \u001b[0m | \u001b[0m 1.851   \u001b[0m | \u001b[0m 0.157   \u001b[0m | \u001b[0m 47.12   \u001b[0m | \u001b[0m 96.05   \u001b[0m | \u001b[0m 0.006994\u001b[0m | \u001b[0m 88.04   \u001b[0m |\n",
      "| \u001b[0m 311     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.8739  \u001b[0m | \u001b[0m 2.902   \u001b[0m | \u001b[0m 0.6314  \u001b[0m | \u001b[0m 0.567   \u001b[0m | \u001b[0m 0.5416  \u001b[0m | \u001b[0m 0.4555  \u001b[0m | \u001b[0m 0.02337 \u001b[0m | \u001b[0m 31.25   \u001b[0m | \u001b[0m 33.8    \u001b[0m | \u001b[0m 0.006516\u001b[0m | \u001b[0m 94.63   \u001b[0m |\n",
      "| \u001b[0m 312     \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.449   \u001b[0m | \u001b[0m 6.089   \u001b[0m | \u001b[0m 0.6654  \u001b[0m | \u001b[0m 0.8905  \u001b[0m | \u001b[0m 1.462   \u001b[0m | \u001b[0m 2.382   \u001b[0m | \u001b[0m 0.1579  \u001b[0m | \u001b[0m 43.61   \u001b[0m | \u001b[0m 41.71   \u001b[0m | \u001b[0m 0.002954\u001b[0m | \u001b[0m 46.0    \u001b[0m |\n",
      "| \u001b[0m 313     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.4699  \u001b[0m | \u001b[0m 9.746   \u001b[0m | \u001b[0m 0.7804  \u001b[0m | \u001b[0m 0.5141  \u001b[0m | \u001b[0m 2.206   \u001b[0m | \u001b[0m 0.7497  \u001b[0m | \u001b[0m 0.02244 \u001b[0m | \u001b[0m 28.77   \u001b[0m | \u001b[0m 69.48   \u001b[0m | \u001b[0m 0.007578\u001b[0m | \u001b[0m 90.42   \u001b[0m |\n",
      "| \u001b[0m 314     \u001b[0m | \u001b[0m 0.931   \u001b[0m | \u001b[0m 0.8042  \u001b[0m | \u001b[0m 6.555   \u001b[0m | \u001b[0m 0.4519  \u001b[0m | \u001b[0m 0.4625  \u001b[0m | \u001b[0m 2.376   \u001b[0m | \u001b[0m 1.745   \u001b[0m | \u001b[0m 0.1233  \u001b[0m | \u001b[0m 93.17   \u001b[0m | \u001b[0m 90.27   \u001b[0m | \u001b[0m 0.005541\u001b[0m | \u001b[0m 62.82   \u001b[0m |\n",
      "| \u001b[0m 315     \u001b[0m | \u001b[0m 0.94    \u001b[0m | \u001b[0m 0.4683  \u001b[0m | \u001b[0m 3.348   \u001b[0m | \u001b[0m 0.722   \u001b[0m | \u001b[0m 0.9646  \u001b[0m | \u001b[0m 2.064   \u001b[0m | \u001b[0m 1.88    \u001b[0m | \u001b[0m 0.09059 \u001b[0m | \u001b[0m 80.35   \u001b[0m | \u001b[0m 82.55   \u001b[0m | \u001b[0m 0.004649\u001b[0m | \u001b[0m 78.93   \u001b[0m |\n",
      "| \u001b[0m 316     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.6563  \u001b[0m | \u001b[0m 3.767   \u001b[0m | \u001b[0m 0.9355  \u001b[0m | \u001b[0m 0.6283  \u001b[0m | \u001b[0m 1.512   \u001b[0m | \u001b[0m 1.769   \u001b[0m | \u001b[0m 0.05768 \u001b[0m | \u001b[0m 31.34   \u001b[0m | \u001b[0m 21.31   \u001b[0m | \u001b[0m 0.006636\u001b[0m | \u001b[0m 43.85   \u001b[0m |\n",
      "| \u001b[0m 317     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.8837  \u001b[0m | \u001b[0m 7.083   \u001b[0m | \u001b[0m 0.9389  \u001b[0m | \u001b[0m 0.5336  \u001b[0m | \u001b[0m 1.403   \u001b[0m | \u001b[0m 1.15    \u001b[0m | \u001b[0m 0.08507 \u001b[0m | \u001b[0m 27.58   \u001b[0m | \u001b[0m 94.04   \u001b[0m | \u001b[0m 0.00678 \u001b[0m | \u001b[0m 20.84   \u001b[0m |\n",
      "| \u001b[0m 318     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.8058  \u001b[0m | \u001b[0m 6.364   \u001b[0m | \u001b[0m 0.5229  \u001b[0m | \u001b[0m 0.9918  \u001b[0m | \u001b[0m 0.918   \u001b[0m | \u001b[0m 0.6311  \u001b[0m | \u001b[0m 0.06451 \u001b[0m | \u001b[0m 33.78   \u001b[0m | \u001b[0m 71.83   \u001b[0m | \u001b[0m 0.000449\u001b[0m | \u001b[0m 54.78   \u001b[0m |\n",
      "| \u001b[0m 319     \u001b[0m | \u001b[0m 0.9325  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 5.767   \u001b[0m | \u001b[0m 0.4735  \u001b[0m | \u001b[0m 0.4697  \u001b[0m | \u001b[0m 2.549   \u001b[0m | \u001b[0m 0.9244  \u001b[0m | \u001b[0m 0.1615  \u001b[0m | \u001b[0m 41.91   \u001b[0m | \u001b[0m 62.61   \u001b[0m | \u001b[0m 0.00608 \u001b[0m | \u001b[0m 59.54   \u001b[0m |\n",
      "| \u001b[0m 320     \u001b[0m | \u001b[0m 0.9394  \u001b[0m | \u001b[0m 0.5057  \u001b[0m | \u001b[0m 9.572   \u001b[0m | \u001b[0m 0.8652  \u001b[0m | \u001b[0m 0.892   \u001b[0m | \u001b[0m 1.561   \u001b[0m | \u001b[0m 1.283   \u001b[0m | \u001b[0m 0.1998  \u001b[0m | \u001b[0m 77.71   \u001b[0m | \u001b[0m 82.72   \u001b[0m | \u001b[0m 0.001531\u001b[0m | \u001b[0m 52.25   \u001b[0m |\n",
      "| \u001b[0m 321     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.9153  \u001b[0m | \u001b[0m 3.632   \u001b[0m | \u001b[0m 0.6104  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.08404 \u001b[0m | \u001b[0m 0.6876  \u001b[0m | \u001b[0m 0.1793  \u001b[0m | \u001b[0m 40.11   \u001b[0m | \u001b[0m 67.84   \u001b[0m | \u001b[0m 0.006217\u001b[0m | \u001b[0m 88.51   \u001b[0m |\n",
      "| \u001b[0m 322     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.8974  \u001b[0m | \u001b[0m 2.112   \u001b[0m | \u001b[0m 0.5305  \u001b[0m | \u001b[0m 0.7434  \u001b[0m | \u001b[0m 2.246   \u001b[0m | \u001b[0m 1.321   \u001b[0m | \u001b[0m 0.06842 \u001b[0m | \u001b[0m 17.77   \u001b[0m | \u001b[0m 34.87   \u001b[0m | \u001b[0m 0.00277 \u001b[0m | \u001b[0m 62.83   \u001b[0m |\n",
      "| \u001b[0m 323     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.6555  \u001b[0m | \u001b[0m 2.872   \u001b[0m | \u001b[0m 0.935   \u001b[0m | \u001b[0m 0.7802  \u001b[0m | \u001b[0m 1.537   \u001b[0m | \u001b[0m 0.5338  \u001b[0m | \u001b[0m 0.1615  \u001b[0m | \u001b[0m 15.11   \u001b[0m | \u001b[0m 78.81   \u001b[0m | \u001b[0m 0.006231\u001b[0m | \u001b[0m 58.12   \u001b[0m |\n",
      "| \u001b[0m 324     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.867   \u001b[0m | \u001b[0m 4.621   \u001b[0m | \u001b[0m 0.712   \u001b[0m | \u001b[0m 0.4599  \u001b[0m | \u001b[0m 2.76    \u001b[0m | \u001b[0m 2.213   \u001b[0m | \u001b[0m 0.1559  \u001b[0m | \u001b[0m 83.02   \u001b[0m | \u001b[0m 63.33   \u001b[0m | \u001b[0m 0.008122\u001b[0m | \u001b[0m 79.88   \u001b[0m |\n",
      "| \u001b[0m 325     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.981   \u001b[0m | \u001b[0m 3.09    \u001b[0m | \u001b[0m 0.6799  \u001b[0m | \u001b[0m 0.973   \u001b[0m | \u001b[0m 0.8007  \u001b[0m | \u001b[0m 2.201   \u001b[0m | \u001b[0m 0.101   \u001b[0m | \u001b[0m 39.29   \u001b[0m | \u001b[0m 56.58   \u001b[0m | \u001b[0m 0.000738\u001b[0m | \u001b[0m 42.05   \u001b[0m |\n",
      "| \u001b[0m 326     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.7247  \u001b[0m | \u001b[0m 5.286   \u001b[0m | \u001b[0m 0.8904  \u001b[0m | \u001b[0m 0.5743  \u001b[0m | \u001b[0m 1.745   \u001b[0m | \u001b[0m 1.121   \u001b[0m | \u001b[0m 0.01982 \u001b[0m | \u001b[0m 96.59   \u001b[0m | \u001b[0m 83.39   \u001b[0m | \u001b[0m 0.005759\u001b[0m | \u001b[0m 75.76   \u001b[0m |\n",
      "| \u001b[0m 327     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.4767  \u001b[0m | \u001b[0m 2.415   \u001b[0m | \u001b[0m 0.5666  \u001b[0m | \u001b[0m 0.6456  \u001b[0m | \u001b[0m 1.479   \u001b[0m | \u001b[0m 2.848   \u001b[0m | \u001b[0m 0.0608  \u001b[0m | \u001b[0m 88.29   \u001b[0m | \u001b[0m 82.92   \u001b[0m | \u001b[0m 0.007979\u001b[0m | \u001b[0m 35.09   \u001b[0m |\n",
      "| \u001b[0m 328     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.4969  \u001b[0m | \u001b[0m 9.905   \u001b[0m | \u001b[0m 0.8124  \u001b[0m | \u001b[0m 0.437   \u001b[0m | \u001b[0m 2.265   \u001b[0m | \u001b[0m 1.73    \u001b[0m | \u001b[0m 0.1238  \u001b[0m | \u001b[0m 27.34   \u001b[0m | \u001b[0m 58.02   \u001b[0m | \u001b[0m 0.0028  \u001b[0m | \u001b[0m 65.74   \u001b[0m |\n",
      "| \u001b[0m 329     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.4774  \u001b[0m | \u001b[0m 1.173   \u001b[0m | \u001b[0m 0.7032  \u001b[0m | \u001b[0m 0.6862  \u001b[0m | \u001b[0m 1.395   \u001b[0m | \u001b[0m 0.8985  \u001b[0m | \u001b[0m 0.06917 \u001b[0m | \u001b[0m 74.5    \u001b[0m | \u001b[0m 44.47   \u001b[0m | \u001b[0m 0.001549\u001b[0m | \u001b[0m 90.67   \u001b[0m |\n",
      "| \u001b[0m 330     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.7445  \u001b[0m | \u001b[0m 7.408   \u001b[0m | \u001b[0m 0.5836  \u001b[0m | \u001b[0m 0.6131  \u001b[0m | \u001b[0m 0.7124  \u001b[0m | \u001b[0m 1.977   \u001b[0m | \u001b[0m 0.116   \u001b[0m | \u001b[0m 16.21   \u001b[0m | \u001b[0m 89.39   \u001b[0m | \u001b[0m 0.00192 \u001b[0m | \u001b[0m 67.81   \u001b[0m |\n",
      "| \u001b[0m 331     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.7653  \u001b[0m | \u001b[0m 7.744   \u001b[0m | \u001b[0m 0.7367  \u001b[0m | \u001b[0m 0.7277  \u001b[0m | \u001b[0m 0.822   \u001b[0m | \u001b[0m 0.8136  \u001b[0m | \u001b[0m 0.01906 \u001b[0m | \u001b[0m 41.09   \u001b[0m | \u001b[0m 16.4    \u001b[0m | \u001b[0m 0.007603\u001b[0m | \u001b[0m 97.78   \u001b[0m |\n",
      "| \u001b[0m 332     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.8065  \u001b[0m | \u001b[0m 9.084   \u001b[0m | \u001b[0m 0.8456  \u001b[0m | \u001b[0m 0.4307  \u001b[0m | \u001b[0m 2.624   \u001b[0m | \u001b[0m 2.571   \u001b[0m | \u001b[0m 0.03346 \u001b[0m | \u001b[0m 82.47   \u001b[0m | \u001b[0m 58.81   \u001b[0m | \u001b[0m 0.001129\u001b[0m | \u001b[0m 33.29   \u001b[0m |\n",
      "| \u001b[0m 333     \u001b[0m | \u001b[0m 0.9339  \u001b[0m | \u001b[0m 0.7769  \u001b[0m | \u001b[0m 2.112   \u001b[0m | \u001b[0m 0.6136  \u001b[0m | \u001b[0m 0.4715  \u001b[0m | \u001b[0m 1.159   \u001b[0m | \u001b[0m 2.244   \u001b[0m | \u001b[0m 0.121   \u001b[0m | \u001b[0m 66.11   \u001b[0m | \u001b[0m 68.0    \u001b[0m | \u001b[0m 0.007476\u001b[0m | \u001b[0m 49.92   \u001b[0m |\n",
      "| \u001b[0m 334     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.7115  \u001b[0m | \u001b[0m 4.721   \u001b[0m | \u001b[0m 0.7557  \u001b[0m | \u001b[0m 0.9656  \u001b[0m | \u001b[0m 1.35    \u001b[0m | \u001b[0m 2.389   \u001b[0m | \u001b[0m 0.05954 \u001b[0m | \u001b[0m 52.92   \u001b[0m | \u001b[0m 50.13   \u001b[0m | \u001b[0m 0.000358\u001b[0m | \u001b[0m 34.12   \u001b[0m |\n",
      "| \u001b[0m 335     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.4336  \u001b[0m | \u001b[0m 5.969   \u001b[0m | \u001b[0m 0.5467  \u001b[0m | \u001b[0m 0.6564  \u001b[0m | \u001b[0m 1.988   \u001b[0m | \u001b[0m 2.596   \u001b[0m | \u001b[0m 0.1804  \u001b[0m | \u001b[0m 96.04   \u001b[0m | \u001b[0m 68.96   \u001b[0m | \u001b[0m 0.007379\u001b[0m | \u001b[0m 25.62   \u001b[0m |\n",
      "| \u001b[0m 336     \u001b[0m | \u001b[0m 0.9416  \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 3.976   \u001b[0m | \u001b[0m 0.8904  \u001b[0m | \u001b[0m 0.844   \u001b[0m | \u001b[0m 1.445   \u001b[0m | \u001b[0m 1.862   \u001b[0m | \u001b[0m 0.04505 \u001b[0m | \u001b[0m 45.44   \u001b[0m | \u001b[0m 96.82   \u001b[0m | \u001b[0m 0.004096\u001b[0m | \u001b[0m 89.23   \u001b[0m |\n",
      "| \u001b[0m 337     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 8.473   \u001b[0m | \u001b[0m 0.8542  \u001b[0m | \u001b[0m 0.5341  \u001b[0m | \u001b[0m 0.7209  \u001b[0m | \u001b[0m 2.178   \u001b[0m | \u001b[0m 0.03412 \u001b[0m | \u001b[0m 58.79   \u001b[0m | \u001b[0m 16.2    \u001b[0m | \u001b[0m 0.008378\u001b[0m | \u001b[0m 91.42   \u001b[0m |\n",
      "| \u001b[0m 338     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.5896  \u001b[0m | \u001b[0m 2.543   \u001b[0m | \u001b[0m 0.4477  \u001b[0m | \u001b[0m 0.6811  \u001b[0m | \u001b[0m 0.265   \u001b[0m | \u001b[0m 2.409   \u001b[0m | \u001b[0m 0.02969 \u001b[0m | \u001b[0m 73.47   \u001b[0m | \u001b[0m 69.35   \u001b[0m | \u001b[0m 0.005907\u001b[0m | \u001b[0m 75.39   \u001b[0m |\n",
      "| \u001b[0m 339     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.8389  \u001b[0m | \u001b[0m 1.905   \u001b[0m | \u001b[0m 0.6608  \u001b[0m | \u001b[0m 0.5337  \u001b[0m | \u001b[0m 1.853   \u001b[0m | \u001b[0m 1.653   \u001b[0m | \u001b[0m 0.1641  \u001b[0m | \u001b[0m 13.27   \u001b[0m | \u001b[0m 86.39   \u001b[0m | \u001b[0m 0.001494\u001b[0m | \u001b[0m 77.27   \u001b[0m |\n",
      "| \u001b[0m 340     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.7752  \u001b[0m | \u001b[0m 1.958   \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.5945  \u001b[0m | \u001b[0m 1.595   \u001b[0m | \u001b[0m 1.779   \u001b[0m | \u001b[0m 0.1675  \u001b[0m | \u001b[0m 65.96   \u001b[0m | \u001b[0m 50.8    \u001b[0m | \u001b[0m 0.006977\u001b[0m | \u001b[0m 38.63   \u001b[0m |\n",
      "| \u001b[0m 341     \u001b[0m | \u001b[0m 0.9345  \u001b[0m | \u001b[0m 0.6376  \u001b[0m | \u001b[0m 6.924   \u001b[0m | \u001b[0m 0.5805  \u001b[0m | \u001b[0m 0.4582  \u001b[0m | \u001b[0m 1.282   \u001b[0m | \u001b[0m 0.8693  \u001b[0m | \u001b[0m 0.07984 \u001b[0m | \u001b[0m 33.81   \u001b[0m | \u001b[0m 16.32   \u001b[0m | \u001b[0m 0.005017\u001b[0m | \u001b[0m 33.74   \u001b[0m |\n",
      "| \u001b[0m 342     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.9222  \u001b[0m | \u001b[0m 9.267   \u001b[0m | \u001b[0m 0.642   \u001b[0m | \u001b[0m 0.85    \u001b[0m | \u001b[0m 1.67    \u001b[0m | \u001b[0m 0.3402  \u001b[0m | \u001b[0m 0.01646 \u001b[0m | \u001b[0m 86.46   \u001b[0m | \u001b[0m 72.63   \u001b[0m | \u001b[0m 0.00835 \u001b[0m | \u001b[0m 75.18   \u001b[0m |\n",
      "| \u001b[0m 343     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.8604  \u001b[0m | \u001b[0m 3.508   \u001b[0m | \u001b[0m 0.7038  \u001b[0m | \u001b[0m 0.6393  \u001b[0m | \u001b[0m 1.467   \u001b[0m | \u001b[0m 1.131   \u001b[0m | \u001b[0m 0.08678 \u001b[0m | \u001b[0m 74.22   \u001b[0m | \u001b[0m 60.29   \u001b[0m | \u001b[0m 0.007446\u001b[0m | \u001b[0m 64.29   \u001b[0m |\n",
      "| \u001b[0m 344     \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.6059  \u001b[0m | \u001b[0m 8.636   \u001b[0m | \u001b[0m 0.8287  \u001b[0m | \u001b[0m 0.877   \u001b[0m | \u001b[0m 0.000795\u001b[0m | \u001b[0m 1.576   \u001b[0m | \u001b[0m 0.04904 \u001b[0m | \u001b[0m 96.4    \u001b[0m | \u001b[0m 82.65   \u001b[0m | \u001b[0m 0.000743\u001b[0m | \u001b[0m 44.09   \u001b[0m |\n",
      "| \u001b[0m 345     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.7079  \u001b[0m | \u001b[0m 9.253   \u001b[0m | \u001b[0m 0.9592  \u001b[0m | \u001b[0m 0.4303  \u001b[0m | \u001b[0m 1.814   \u001b[0m | \u001b[0m 0.3774  \u001b[0m | \u001b[0m 0.1876  \u001b[0m | \u001b[0m 25.95   \u001b[0m | \u001b[0m 16.48   \u001b[0m | \u001b[0m 0.004855\u001b[0m | \u001b[0m 91.72   \u001b[0m |\n",
      "| \u001b[0m 346     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.4468  \u001b[0m | \u001b[0m 4.714   \u001b[0m | \u001b[0m 0.9847  \u001b[0m | \u001b[0m 0.5881  \u001b[0m | \u001b[0m 0.1753  \u001b[0m | \u001b[0m 0.9458  \u001b[0m | \u001b[0m 0.01577 \u001b[0m | \u001b[0m 17.93   \u001b[0m | \u001b[0m 31.24   \u001b[0m | \u001b[0m 0.00638 \u001b[0m | \u001b[0m 62.02   \u001b[0m |\n",
      "| \u001b[0m 347     \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.418   \u001b[0m | \u001b[0m 2.307   \u001b[0m | \u001b[0m 0.9326  \u001b[0m | \u001b[0m 0.7404  \u001b[0m | \u001b[0m 1.503   \u001b[0m | \u001b[0m 1.333   \u001b[0m | \u001b[0m 0.03142 \u001b[0m | \u001b[0m 46.84   \u001b[0m | \u001b[0m 35.16   \u001b[0m | \u001b[0m 0.000608\u001b[0m | \u001b[0m 44.03   \u001b[0m |\n",
      "| \u001b[0m 348     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.4148  \u001b[0m | \u001b[0m 2.407   \u001b[0m | \u001b[0m 0.8518  \u001b[0m | \u001b[0m 0.4031  \u001b[0m | \u001b[0m 0.7851  \u001b[0m | \u001b[0m 0.4433  \u001b[0m | \u001b[0m 0.02883 \u001b[0m | \u001b[0m 73.34   \u001b[0m | \u001b[0m 13.04   \u001b[0m | \u001b[0m 0.006709\u001b[0m | \u001b[0m 50.23   \u001b[0m |\n",
      "| \u001b[0m 349     \u001b[0m | \u001b[0m 0.9443  \u001b[0m | \u001b[0m 0.9612  \u001b[0m | \u001b[0m 1.209   \u001b[0m | \u001b[0m 0.505   \u001b[0m | \u001b[0m 0.685   \u001b[0m | \u001b[0m 1.461   \u001b[0m | \u001b[0m 0.8378  \u001b[0m | \u001b[0m 0.1299  \u001b[0m | \u001b[0m 31.43   \u001b[0m | \u001b[0m 10.97   \u001b[0m | \u001b[0m 0.008682\u001b[0m | \u001b[0m 57.99   \u001b[0m |\n",
      "| \u001b[0m 350     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.7229  \u001b[0m | \u001b[0m 3.283   \u001b[0m | \u001b[0m 0.5828  \u001b[0m | \u001b[0m 0.6217  \u001b[0m | \u001b[0m 2.051   \u001b[0m | \u001b[0m 2.236   \u001b[0m | \u001b[0m 0.08802 \u001b[0m | \u001b[0m 44.9    \u001b[0m | \u001b[0m 94.21   \u001b[0m | \u001b[0m 0.000778\u001b[0m | \u001b[0m 65.12   \u001b[0m |\n",
      "| \u001b[0m 351     \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.5586  \u001b[0m | \u001b[0m 5.59    \u001b[0m | \u001b[0m 0.803   \u001b[0m | \u001b[0m 0.6081  \u001b[0m | \u001b[0m 1.01    \u001b[0m | \u001b[0m 2.964   \u001b[0m | \u001b[0m 0.0871  \u001b[0m | \u001b[0m 35.5    \u001b[0m | \u001b[0m 86.87   \u001b[0m | \u001b[0m 0.009064\u001b[0m | \u001b[0m 57.99   \u001b[0m |\n",
      "| \u001b[0m 352     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.6542  \u001b[0m | \u001b[0m 1.248   \u001b[0m | \u001b[0m 0.626   \u001b[0m | \u001b[0m 0.513   \u001b[0m | \u001b[0m 1.879   \u001b[0m | \u001b[0m 1.229   \u001b[0m | \u001b[0m 0.1047  \u001b[0m | \u001b[0m 69.59   \u001b[0m | \u001b[0m 48.33   \u001b[0m | \u001b[0m 0.009468\u001b[0m | \u001b[0m 41.8    \u001b[0m |\n",
      "| \u001b[0m 353     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.5583  \u001b[0m | \u001b[0m 2.048   \u001b[0m | \u001b[0m 0.7449  \u001b[0m | \u001b[0m 0.5562  \u001b[0m | \u001b[0m 1.865   \u001b[0m | \u001b[0m 1.145   \u001b[0m | \u001b[0m 0.01869 \u001b[0m | \u001b[0m 58.49   \u001b[0m | \u001b[0m 45.73   \u001b[0m | \u001b[0m 0.004608\u001b[0m | \u001b[0m 94.72   \u001b[0m |\n",
      "| \u001b[0m 354     \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 0.7237  \u001b[0m | \u001b[0m 4.882   \u001b[0m | \u001b[0m 0.881   \u001b[0m | \u001b[0m 0.9351  \u001b[0m | \u001b[0m 0.2113  \u001b[0m | \u001b[0m 2.723   \u001b[0m | \u001b[0m 0.1438  \u001b[0m | \u001b[0m 19.27   \u001b[0m | \u001b[0m 13.02   \u001b[0m | \u001b[0m 0.009267\u001b[0m | \u001b[0m 92.14   \u001b[0m |\n",
      "| \u001b[0m 355     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.4537  \u001b[0m | \u001b[0m 4.22    \u001b[0m | \u001b[0m 0.8767  \u001b[0m | \u001b[0m 0.902   \u001b[0m | \u001b[0m 0.637   \u001b[0m | \u001b[0m 1.848   \u001b[0m | \u001b[0m 0.02584 \u001b[0m | \u001b[0m 67.12   \u001b[0m | \u001b[0m 48.85   \u001b[0m | \u001b[0m 0.009871\u001b[0m | \u001b[0m 29.68   \u001b[0m |\n",
      "| \u001b[0m 356     \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.5472  \u001b[0m | \u001b[0m 8.026   \u001b[0m | \u001b[0m 0.9453  \u001b[0m | \u001b[0m 0.4421  \u001b[0m | \u001b[0m 1.459   \u001b[0m | \u001b[0m 1.719   \u001b[0m | \u001b[0m 0.1198  \u001b[0m | \u001b[0m 31.5    \u001b[0m | \u001b[0m 90.61   \u001b[0m | \u001b[0m 0.007333\u001b[0m | \u001b[0m 69.21   \u001b[0m |\n",
      "| \u001b[0m 357     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.4098  \u001b[0m | \u001b[0m 8.496   \u001b[0m | \u001b[0m 0.6738  \u001b[0m | \u001b[0m 0.8107  \u001b[0m | \u001b[0m 1.154   \u001b[0m | \u001b[0m 2.673   \u001b[0m | \u001b[0m 0.1337  \u001b[0m | \u001b[0m 80.21   \u001b[0m | \u001b[0m 52.77   \u001b[0m | \u001b[0m 0.001622\u001b[0m | \u001b[0m 72.02   \u001b[0m |\n",
      "| \u001b[0m 358     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.7216  \u001b[0m | \u001b[0m 8.42    \u001b[0m | \u001b[0m 0.9976  \u001b[0m | \u001b[0m 0.6767  \u001b[0m | \u001b[0m 2.478   \u001b[0m | \u001b[0m 0.6884  \u001b[0m | \u001b[0m 0.07318 \u001b[0m | \u001b[0m 62.14   \u001b[0m | \u001b[0m 80.36   \u001b[0m | \u001b[0m 0.005106\u001b[0m | \u001b[0m 45.84   \u001b[0m |\n",
      "| \u001b[0m 359     \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 0.76    \u001b[0m | \u001b[0m 8.789   \u001b[0m | \u001b[0m 0.9249  \u001b[0m | \u001b[0m 0.813   \u001b[0m | \u001b[0m 2.927   \u001b[0m | \u001b[0m 2.293   \u001b[0m | \u001b[0m 0.02569 \u001b[0m | \u001b[0m 72.38   \u001b[0m | \u001b[0m 48.01   \u001b[0m | \u001b[0m 0.001914\u001b[0m | \u001b[0m 46.76   \u001b[0m |\n",
      "| \u001b[0m 360     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.7154  \u001b[0m | \u001b[0m 4.603   \u001b[0m | \u001b[0m 0.6772  \u001b[0m | \u001b[0m 0.7238  \u001b[0m | \u001b[0m 0.5274  \u001b[0m | \u001b[0m 0.8319  \u001b[0m | \u001b[0m 0.03213 \u001b[0m | \u001b[0m 99.47   \u001b[0m | \u001b[0m 86.3    \u001b[0m | \u001b[0m 0.005027\u001b[0m | \u001b[0m 78.99   \u001b[0m |\n",
      "| \u001b[0m 361     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.6571  \u001b[0m | \u001b[0m 7.541   \u001b[0m | \u001b[0m 0.6826  \u001b[0m | \u001b[0m 0.6105  \u001b[0m | \u001b[0m 0.4461  \u001b[0m | \u001b[0m 1.646   \u001b[0m | \u001b[0m 0.1759  \u001b[0m | \u001b[0m 59.2    \u001b[0m | \u001b[0m 67.27   \u001b[0m | \u001b[0m 0.002578\u001b[0m | \u001b[0m 34.92   \u001b[0m |\n",
      "| \u001b[0m 362     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.7534  \u001b[0m | \u001b[0m 7.778   \u001b[0m | \u001b[0m 0.864   \u001b[0m | \u001b[0m 0.8676  \u001b[0m | \u001b[0m 0.2135  \u001b[0m | \u001b[0m 2.023   \u001b[0m | \u001b[0m 0.03824 \u001b[0m | \u001b[0m 18.98   \u001b[0m | \u001b[0m 13.31   \u001b[0m | \u001b[0m 0.008345\u001b[0m | \u001b[0m 76.84   \u001b[0m |\n",
      "| \u001b[0m 363     \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.6322  \u001b[0m | \u001b[0m 8.714   \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.848   \u001b[0m | \u001b[0m 0.1847  \u001b[0m | \u001b[0m 1.286   \u001b[0m | \u001b[0m 0.1544  \u001b[0m | \u001b[0m 91.58   \u001b[0m | \u001b[0m 90.38   \u001b[0m | \u001b[0m 0.004742\u001b[0m | \u001b[0m 23.75   \u001b[0m |\n",
      "| \u001b[0m 364     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.5102  \u001b[0m | \u001b[0m 8.877   \u001b[0m | \u001b[0m 0.741   \u001b[0m | \u001b[0m 0.902   \u001b[0m | \u001b[0m 0.2105  \u001b[0m | \u001b[0m 0.3437  \u001b[0m | \u001b[0m 0.1182  \u001b[0m | \u001b[0m 18.01   \u001b[0m | \u001b[0m 20.37   \u001b[0m | \u001b[0m 0.006997\u001b[0m | \u001b[0m 41.03   \u001b[0m |\n",
      "| \u001b[0m 365     \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 0.8797  \u001b[0m | \u001b[0m 7.541   \u001b[0m | \u001b[0m 0.8952  \u001b[0m | \u001b[0m 0.8613  \u001b[0m | \u001b[0m 1.141   \u001b[0m | \u001b[0m 1.885   \u001b[0m | \u001b[0m 0.155   \u001b[0m | \u001b[0m 89.87   \u001b[0m | \u001b[0m 78.45   \u001b[0m | \u001b[0m 0.00734 \u001b[0m | \u001b[0m 96.08   \u001b[0m |\n",
      "| \u001b[0m 366     \u001b[0m | \u001b[0m 0.9408  \u001b[0m | \u001b[0m 0.6533  \u001b[0m | \u001b[0m 1.053   \u001b[0m | \u001b[0m 0.9255  \u001b[0m | \u001b[0m 0.7449  \u001b[0m | \u001b[0m 2.569   \u001b[0m | \u001b[0m 0.721   \u001b[0m | \u001b[0m 0.1904  \u001b[0m | \u001b[0m 75.79   \u001b[0m | \u001b[0m 35.09   \u001b[0m | \u001b[0m 0.008588\u001b[0m | \u001b[0m 86.7    \u001b[0m |\n",
      "| \u001b[0m 367     \u001b[0m | \u001b[0m 0.9359  \u001b[0m | \u001b[0m 0.4639  \u001b[0m | \u001b[0m 8.459   \u001b[0m | \u001b[0m 0.5022  \u001b[0m | \u001b[0m 0.4782  \u001b[0m | \u001b[0m 0.1914  \u001b[0m | \u001b[0m 0.4666  \u001b[0m | \u001b[0m 0.06011 \u001b[0m | \u001b[0m 41.07   \u001b[0m | \u001b[0m 16.78   \u001b[0m | \u001b[0m 0.001101\u001b[0m | \u001b[0m 99.05   \u001b[0m |\n",
      "| \u001b[0m 368     \u001b[0m | \u001b[0m 0.9446  \u001b[0m | \u001b[0m 0.454   \u001b[0m | \u001b[0m 1.491   \u001b[0m | \u001b[0m 0.9311  \u001b[0m | \u001b[0m 0.5806  \u001b[0m | \u001b[0m 0.2591  \u001b[0m | \u001b[0m 2.534   \u001b[0m | \u001b[0m 0.1198  \u001b[0m | \u001b[0m 29.75   \u001b[0m | \u001b[0m 19.41   \u001b[0m | \u001b[0m 0.000936\u001b[0m | \u001b[0m 64.05   \u001b[0m |\n",
      "| \u001b[0m 369     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.6635  \u001b[0m | \u001b[0m 4.484   \u001b[0m | \u001b[0m 0.7958  \u001b[0m | \u001b[0m 0.6341  \u001b[0m | \u001b[0m 0.4741  \u001b[0m | \u001b[0m 1.01    \u001b[0m | \u001b[0m 0.08925 \u001b[0m | \u001b[0m 75.34   \u001b[0m | \u001b[0m 16.06   \u001b[0m | \u001b[0m 0.006008\u001b[0m | \u001b[0m 72.26   \u001b[0m |\n",
      "| \u001b[0m 370     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.9899  \u001b[0m | \u001b[0m 1.494   \u001b[0m | \u001b[0m 0.6599  \u001b[0m | \u001b[0m 0.8977  \u001b[0m | \u001b[0m 1.36    \u001b[0m | \u001b[0m 0.8006  \u001b[0m | \u001b[0m 0.177   \u001b[0m | \u001b[0m 46.96   \u001b[0m | \u001b[0m 95.85   \u001b[0m | \u001b[0m 0.008411\u001b[0m | \u001b[0m 70.91   \u001b[0m |\n",
      "| \u001b[0m 371     \u001b[0m | \u001b[0m 0.9445  \u001b[0m | \u001b[0m 0.7386  \u001b[0m | \u001b[0m 8.151   \u001b[0m | \u001b[0m 0.7082  \u001b[0m | \u001b[0m 0.4604  \u001b[0m | \u001b[0m 1.606   \u001b[0m | \u001b[0m 0.5135  \u001b[0m | \u001b[0m 0.111   \u001b[0m | \u001b[0m 88.67   \u001b[0m | \u001b[0m 20.39   \u001b[0m | \u001b[0m 0.006007\u001b[0m | \u001b[0m 97.15   \u001b[0m |\n",
      "| \u001b[0m 372     \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.7908  \u001b[0m | \u001b[0m 8.158   \u001b[0m | \u001b[0m 0.8815  \u001b[0m | \u001b[0m 0.8621  \u001b[0m | \u001b[0m 0.825   \u001b[0m | \u001b[0m 1.387   \u001b[0m | \u001b[0m 0.02633 \u001b[0m | \u001b[0m 30.59   \u001b[0m | \u001b[0m 78.36   \u001b[0m | \u001b[0m 0.008742\u001b[0m | \u001b[0m 95.05   \u001b[0m |\n",
      "| \u001b[0m 373     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.5123  \u001b[0m | \u001b[0m 2.212   \u001b[0m | \u001b[0m 0.9527  \u001b[0m | \u001b[0m 0.5071  \u001b[0m | \u001b[0m 2.325   \u001b[0m | \u001b[0m 1.033   \u001b[0m | \u001b[0m 0.0493  \u001b[0m | \u001b[0m 32.33   \u001b[0m | \u001b[0m 12.74   \u001b[0m | \u001b[0m 0.009849\u001b[0m | \u001b[0m 58.36   \u001b[0m |\n",
      "| \u001b[0m 374     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.7206  \u001b[0m | \u001b[0m 8.567   \u001b[0m | \u001b[0m 0.7089  \u001b[0m | \u001b[0m 0.7819  \u001b[0m | \u001b[0m 0.6533  \u001b[0m | \u001b[0m 2.328   \u001b[0m | \u001b[0m 0.1736  \u001b[0m | \u001b[0m 21.38   \u001b[0m | \u001b[0m 37.65   \u001b[0m | \u001b[0m 0.003153\u001b[0m | \u001b[0m 94.08   \u001b[0m |\n",
      "| \u001b[0m 375     \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.6621  \u001b[0m | \u001b[0m 2.524   \u001b[0m | \u001b[0m 0.7345  \u001b[0m | \u001b[0m 0.6829  \u001b[0m | \u001b[0m 2.983   \u001b[0m | \u001b[0m 1.641   \u001b[0m | \u001b[0m 0.1859  \u001b[0m | \u001b[0m 35.63   \u001b[0m | \u001b[0m 25.86   \u001b[0m | \u001b[0m 0.001631\u001b[0m | \u001b[0m 23.33   \u001b[0m |\n",
      "| \u001b[0m 376     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.5775  \u001b[0m | \u001b[0m 7.287   \u001b[0m | \u001b[0m 0.5284  \u001b[0m | \u001b[0m 0.8094  \u001b[0m | \u001b[0m 1.451   \u001b[0m | \u001b[0m 0.9756  \u001b[0m | \u001b[0m 0.05119 \u001b[0m | \u001b[0m 18.0    \u001b[0m | \u001b[0m 85.34   \u001b[0m | \u001b[0m 0.004915\u001b[0m | \u001b[0m 87.81   \u001b[0m |\n",
      "| \u001b[0m 377     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.4681  \u001b[0m | \u001b[0m 4.42    \u001b[0m | \u001b[0m 0.787   \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.2536  \u001b[0m | \u001b[0m 2.288   \u001b[0m | \u001b[0m 0.168   \u001b[0m | \u001b[0m 18.33   \u001b[0m | \u001b[0m 97.19   \u001b[0m | \u001b[0m 0.006874\u001b[0m | \u001b[0m 63.35   \u001b[0m |\n",
      "| \u001b[0m 378     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.509   \u001b[0m | \u001b[0m 5.171   \u001b[0m | \u001b[0m 0.6943  \u001b[0m | \u001b[0m 0.8978  \u001b[0m | \u001b[0m 1.13    \u001b[0m | \u001b[0m 2.601   \u001b[0m | \u001b[0m 0.158   \u001b[0m | \u001b[0m 42.79   \u001b[0m | \u001b[0m 40.65   \u001b[0m | \u001b[0m 0.006455\u001b[0m | \u001b[0m 48.16   \u001b[0m |\n",
      "| \u001b[0m 379     \u001b[0m | \u001b[0m 0.934   \u001b[0m | \u001b[0m 0.5945  \u001b[0m | \u001b[0m 1.708   \u001b[0m | \u001b[0m 0.4439  \u001b[0m | \u001b[0m 0.4978  \u001b[0m | \u001b[0m 1.167   \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.1239  \u001b[0m | \u001b[0m 70.17   \u001b[0m | \u001b[0m 45.26   \u001b[0m | \u001b[0m 0.005459\u001b[0m | \u001b[0m 71.35   \u001b[0m |\n",
      "| \u001b[0m 380     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.6288  \u001b[0m | \u001b[0m 6.04    \u001b[0m | \u001b[0m 0.952   \u001b[0m | \u001b[0m 0.6006  \u001b[0m | \u001b[0m 0.519   \u001b[0m | \u001b[0m 1.296   \u001b[0m | \u001b[0m 0.1436  \u001b[0m | \u001b[0m 52.82   \u001b[0m | \u001b[0m 52.37   \u001b[0m | \u001b[0m 0.005634\u001b[0m | \u001b[0m 75.94   \u001b[0m |\n",
      "| \u001b[0m 381     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.9589  \u001b[0m | \u001b[0m 6.857   \u001b[0m | \u001b[0m 0.5346  \u001b[0m | \u001b[0m 0.9054  \u001b[0m | \u001b[0m 2.01    \u001b[0m | \u001b[0m 0.09924 \u001b[0m | \u001b[0m 0.1639  \u001b[0m | \u001b[0m 38.03   \u001b[0m | \u001b[0m 54.93   \u001b[0m | \u001b[0m 0.00639 \u001b[0m | \u001b[0m 50.58   \u001b[0m |\n",
      "| \u001b[0m 382     \u001b[0m | \u001b[0m 0.9386  \u001b[0m | \u001b[0m 0.9027  \u001b[0m | \u001b[0m 6.43    \u001b[0m | \u001b[0m 0.5603  \u001b[0m | \u001b[0m 0.5496  \u001b[0m | \u001b[0m 0.8793  \u001b[0m | \u001b[0m 1.196   \u001b[0m | \u001b[0m 0.1062  \u001b[0m | \u001b[0m 15.71   \u001b[0m | \u001b[0m 88.36   \u001b[0m | \u001b[0m 0.00256 \u001b[0m | \u001b[0m 68.53   \u001b[0m |\n",
      "| \u001b[0m 383     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.9162  \u001b[0m | \u001b[0m 9.389   \u001b[0m | \u001b[0m 0.9633  \u001b[0m | \u001b[0m 0.5949  \u001b[0m | \u001b[0m 2.075   \u001b[0m | \u001b[0m 0.7165  \u001b[0m | \u001b[0m 0.02812 \u001b[0m | \u001b[0m 26.01   \u001b[0m | \u001b[0m 10.12   \u001b[0m | \u001b[0m 0.007135\u001b[0m | \u001b[0m 72.15   \u001b[0m |\n",
      "| \u001b[0m 384     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.5077  \u001b[0m | \u001b[0m 8.238   \u001b[0m | \u001b[0m 0.9974  \u001b[0m | \u001b[0m 0.555   \u001b[0m | \u001b[0m 0.9113  \u001b[0m | \u001b[0m 2.062   \u001b[0m | \u001b[0m 0.01261 \u001b[0m | \u001b[0m 21.45   \u001b[0m | \u001b[0m 36.91   \u001b[0m | \u001b[0m 0.009216\u001b[0m | \u001b[0m 94.79   \u001b[0m |\n",
      "| \u001b[0m 385     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.6581  \u001b[0m | \u001b[0m 2.958   \u001b[0m | \u001b[0m 0.9103  \u001b[0m | \u001b[0m 0.6192  \u001b[0m | \u001b[0m 1.594   \u001b[0m | \u001b[0m 2.549   \u001b[0m | \u001b[0m 0.1978  \u001b[0m | \u001b[0m 98.34   \u001b[0m | \u001b[0m 51.73   \u001b[0m | \u001b[0m 0.008951\u001b[0m | \u001b[0m 37.25   \u001b[0m |\n",
      "| \u001b[0m 386     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.5582  \u001b[0m | \u001b[0m 6.951   \u001b[0m | \u001b[0m 0.6873  \u001b[0m | \u001b[0m 0.4566  \u001b[0m | \u001b[0m 2.485   \u001b[0m | \u001b[0m 1.311   \u001b[0m | \u001b[0m 0.06758 \u001b[0m | \u001b[0m 58.76   \u001b[0m | \u001b[0m 86.18   \u001b[0m | \u001b[0m 0.003702\u001b[0m | \u001b[0m 49.52   \u001b[0m |\n",
      "| \u001b[0m 387     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.4804  \u001b[0m | \u001b[0m 1.132   \u001b[0m | \u001b[0m 0.8796  \u001b[0m | \u001b[0m 0.6404  \u001b[0m | \u001b[0m 1.709   \u001b[0m | \u001b[0m 1.954   \u001b[0m | \u001b[0m 0.1317  \u001b[0m | \u001b[0m 72.4    \u001b[0m | \u001b[0m 43.08   \u001b[0m | \u001b[0m 0.009882\u001b[0m | \u001b[0m 35.96   \u001b[0m |\n",
      "| \u001b[0m 388     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.6484  \u001b[0m | \u001b[0m 2.885   \u001b[0m | \u001b[0m 0.8855  \u001b[0m | \u001b[0m 0.8149  \u001b[0m | \u001b[0m 2.01    \u001b[0m | \u001b[0m 1.617   \u001b[0m | \u001b[0m 0.02287 \u001b[0m | \u001b[0m 22.02   \u001b[0m | \u001b[0m 96.19   \u001b[0m | \u001b[0m 0.006715\u001b[0m | \u001b[0m 82.89   \u001b[0m |\n",
      "| \u001b[0m 389     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.7017  \u001b[0m | \u001b[0m 3.978   \u001b[0m | \u001b[0m 0.8562  \u001b[0m | \u001b[0m 0.8978  \u001b[0m | \u001b[0m 0.9924  \u001b[0m | \u001b[0m 0.5075  \u001b[0m | \u001b[0m 0.1865  \u001b[0m | \u001b[0m 48.27   \u001b[0m | \u001b[0m 78.49   \u001b[0m | \u001b[0m 0.000732\u001b[0m | \u001b[0m 76.59   \u001b[0m |\n",
      "| \u001b[0m 390     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.4852  \u001b[0m | \u001b[0m 5.556   \u001b[0m | \u001b[0m 0.4101  \u001b[0m | \u001b[0m 0.403   \u001b[0m | \u001b[0m 1.437   \u001b[0m | \u001b[0m 0.4049  \u001b[0m | \u001b[0m 0.1202  \u001b[0m | \u001b[0m 39.74   \u001b[0m | \u001b[0m 62.67   \u001b[0m | \u001b[0m 0.00691 \u001b[0m | \u001b[0m 60.04   \u001b[0m |\n",
      "| \u001b[0m 391     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.9719  \u001b[0m | \u001b[0m 3.718   \u001b[0m | \u001b[0m 0.4289  \u001b[0m | \u001b[0m 0.6052  \u001b[0m | \u001b[0m 2.672   \u001b[0m | \u001b[0m 2.473   \u001b[0m | \u001b[0m 0.1315  \u001b[0m | \u001b[0m 38.84   \u001b[0m | \u001b[0m 67.86   \u001b[0m | \u001b[0m 0.008068\u001b[0m | \u001b[0m 54.03   \u001b[0m |\n",
      "| \u001b[0m 392     \u001b[0m | \u001b[0m 0.9391  \u001b[0m | \u001b[0m 0.9676  \u001b[0m | \u001b[0m 6.669   \u001b[0m | \u001b[0m 0.5088  \u001b[0m | \u001b[0m 0.9707  \u001b[0m | \u001b[0m 0.8249  \u001b[0m | \u001b[0m 1.223   \u001b[0m | \u001b[0m 0.1131  \u001b[0m | \u001b[0m 27.52   \u001b[0m | \u001b[0m 94.64   \u001b[0m | \u001b[0m 0.004638\u001b[0m | \u001b[0m 21.73   \u001b[0m |\n",
      "| \u001b[0m 393     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.6908  \u001b[0m | \u001b[0m 7.001   \u001b[0m | \u001b[0m 0.5364  \u001b[0m | \u001b[0m 0.893   \u001b[0m | \u001b[0m 1.552   \u001b[0m | \u001b[0m 0.4832  \u001b[0m | \u001b[0m 0.06404 \u001b[0m | \u001b[0m 34.12   \u001b[0m | \u001b[0m 75.93   \u001b[0m | \u001b[0m 0.006277\u001b[0m | \u001b[0m 71.84   \u001b[0m |\n",
      "| \u001b[0m 394     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.4543  \u001b[0m | \u001b[0m 8.417   \u001b[0m | \u001b[0m 0.8106  \u001b[0m | \u001b[0m 0.7359  \u001b[0m | \u001b[0m 0.6736  \u001b[0m | \u001b[0m 2.07    \u001b[0m | \u001b[0m 0.1727  \u001b[0m | \u001b[0m 42.15   \u001b[0m | \u001b[0m 87.56   \u001b[0m | \u001b[0m 0.001702\u001b[0m | \u001b[0m 87.77   \u001b[0m |\n",
      "| \u001b[0m 395     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.4635  \u001b[0m | \u001b[0m 6.679   \u001b[0m | \u001b[0m 0.9633  \u001b[0m | \u001b[0m 0.6128  \u001b[0m | \u001b[0m 0.1836  \u001b[0m | \u001b[0m 1.419   \u001b[0m | \u001b[0m 0.0338  \u001b[0m | \u001b[0m 63.67   \u001b[0m | \u001b[0m 38.12   \u001b[0m | \u001b[0m 0.000102\u001b[0m | \u001b[0m 20.86   \u001b[0m |\n",
      "| \u001b[0m 396     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.7761  \u001b[0m | \u001b[0m 9.884   \u001b[0m | \u001b[0m 0.6423  \u001b[0m | \u001b[0m 0.4047  \u001b[0m | \u001b[0m 0.4703  \u001b[0m | \u001b[0m 2.903   \u001b[0m | \u001b[0m 0.1958  \u001b[0m | \u001b[0m 38.12   \u001b[0m | \u001b[0m 32.3    \u001b[0m | \u001b[0m 0.008388\u001b[0m | \u001b[0m 58.7    \u001b[0m |\n",
      "| \u001b[0m 397     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.5364  \u001b[0m | \u001b[0m 6.026   \u001b[0m | \u001b[0m 0.9086  \u001b[0m | \u001b[0m 0.8282  \u001b[0m | \u001b[0m 0.6425  \u001b[0m | \u001b[0m 0.403   \u001b[0m | \u001b[0m 0.05801 \u001b[0m | \u001b[0m 91.64   \u001b[0m | \u001b[0m 10.63   \u001b[0m | \u001b[0m 0.003887\u001b[0m | \u001b[0m 79.99   \u001b[0m |\n",
      "| \u001b[0m 398     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.4585  \u001b[0m | \u001b[0m 8.912   \u001b[0m | \u001b[0m 0.5083  \u001b[0m | \u001b[0m 0.7157  \u001b[0m | \u001b[0m 0.1456  \u001b[0m | \u001b[0m 2.571   \u001b[0m | \u001b[0m 0.07499 \u001b[0m | \u001b[0m 41.83   \u001b[0m | \u001b[0m 88.78   \u001b[0m | \u001b[0m 0.009294\u001b[0m | \u001b[0m 87.73   \u001b[0m |\n",
      "| \u001b[0m 399     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.8943  \u001b[0m | \u001b[0m 5.797   \u001b[0m | \u001b[0m 0.6327  \u001b[0m | \u001b[0m 0.883   \u001b[0m | \u001b[0m 0.6464  \u001b[0m | \u001b[0m 0.6057  \u001b[0m | \u001b[0m 0.044   \u001b[0m | \u001b[0m 91.32   \u001b[0m | \u001b[0m 95.89   \u001b[0m | \u001b[0m 0.000490\u001b[0m | \u001b[0m 84.5    \u001b[0m |\n",
      "| \u001b[0m 400     \u001b[0m | \u001b[0m 0.9394  \u001b[0m | \u001b[0m 0.434   \u001b[0m | \u001b[0m 5.638   \u001b[0m | \u001b[0m 0.9618  \u001b[0m | \u001b[0m 0.5135  \u001b[0m | \u001b[0m 2.99    \u001b[0m | \u001b[0m 1.691   \u001b[0m | \u001b[0m 0.01119 \u001b[0m | \u001b[0m 36.61   \u001b[0m | \u001b[0m 78.26   \u001b[0m | \u001b[0m 0.009179\u001b[0m | \u001b[0m 89.59   \u001b[0m |\n",
      "| \u001b[0m 401     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.7937  \u001b[0m | \u001b[0m 7.223   \u001b[0m | \u001b[0m 0.9937  \u001b[0m | \u001b[0m 0.4523  \u001b[0m | \u001b[0m 1.125   \u001b[0m | \u001b[0m 1.927   \u001b[0m | \u001b[0m 0.183   \u001b[0m | \u001b[0m 96.0    \u001b[0m | \u001b[0m 55.57   \u001b[0m | \u001b[0m 0.002715\u001b[0m | \u001b[0m 79.61   \u001b[0m |\n",
      "| \u001b[0m 402     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.4266  \u001b[0m | \u001b[0m 7.078   \u001b[0m | \u001b[0m 0.7013  \u001b[0m | \u001b[0m 0.4618  \u001b[0m | \u001b[0m 2.158   \u001b[0m | \u001b[0m 2.093   \u001b[0m | \u001b[0m 0.09958 \u001b[0m | \u001b[0m 35.23   \u001b[0m | \u001b[0m 83.06   \u001b[0m | \u001b[0m 0.005997\u001b[0m | \u001b[0m 32.66   \u001b[0m |\n",
      "| \u001b[0m 403     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.5919  \u001b[0m | \u001b[0m 6.69    \u001b[0m | \u001b[0m 0.5287  \u001b[0m | \u001b[0m 0.6256  \u001b[0m | \u001b[0m 0.185   \u001b[0m | \u001b[0m 0.08922 \u001b[0m | \u001b[0m 0.1506  \u001b[0m | \u001b[0m 21.66   \u001b[0m | \u001b[0m 37.84   \u001b[0m | \u001b[0m 0.005173\u001b[0m | \u001b[0m 70.49   \u001b[0m |\n",
      "| \u001b[0m 404     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.6475  \u001b[0m | \u001b[0m 2.522   \u001b[0m | \u001b[0m 0.809   \u001b[0m | \u001b[0m 0.4434  \u001b[0m | \u001b[0m 0.4649  \u001b[0m | \u001b[0m 2.889   \u001b[0m | \u001b[0m 0.1491  \u001b[0m | \u001b[0m 73.6    \u001b[0m | \u001b[0m 89.66   \u001b[0m | \u001b[0m 0.006615\u001b[0m | \u001b[0m 44.12   \u001b[0m |\n",
      "| \u001b[0m 405     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.464   \u001b[0m | \u001b[0m 8.963   \u001b[0m | \u001b[0m 0.4005  \u001b[0m | \u001b[0m 0.6688  \u001b[0m | \u001b[0m 0.01699 \u001b[0m | \u001b[0m 0.4339  \u001b[0m | \u001b[0m 0.1051  \u001b[0m | \u001b[0m 17.81   \u001b[0m | \u001b[0m 88.45   \u001b[0m | \u001b[0m 0.007353\u001b[0m | \u001b[0m 81.3    \u001b[0m |\n",
      "| \u001b[0m 406     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.5917  \u001b[0m | \u001b[0m 4.061   \u001b[0m | \u001b[0m 0.7328  \u001b[0m | \u001b[0m 0.8265  \u001b[0m | \u001b[0m 0.8519  \u001b[0m | \u001b[0m 2.902   \u001b[0m | \u001b[0m 0.08809 \u001b[0m | \u001b[0m 43.08   \u001b[0m | \u001b[0m 42.15   \u001b[0m | \u001b[0m 0.0025  \u001b[0m | \u001b[0m 48.11   \u001b[0m |\n",
      "| \u001b[0m 407     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.9216  \u001b[0m | \u001b[0m 9.534   \u001b[0m | \u001b[0m 0.7074  \u001b[0m | \u001b[0m 0.5344  \u001b[0m | \u001b[0m 0.6375  \u001b[0m | \u001b[0m 1.619   \u001b[0m | \u001b[0m 0.1684  \u001b[0m | \u001b[0m 51.98   \u001b[0m | \u001b[0m 81.34   \u001b[0m | \u001b[0m 0.003022\u001b[0m | \u001b[0m 96.36   \u001b[0m |\n",
      "| \u001b[0m 408     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.7346  \u001b[0m | \u001b[0m 6.672   \u001b[0m | \u001b[0m 0.648   \u001b[0m | \u001b[0m 0.4202  \u001b[0m | \u001b[0m 0.0784  \u001b[0m | \u001b[0m 2.252   \u001b[0m | \u001b[0m 0.07844 \u001b[0m | \u001b[0m 60.05   \u001b[0m | \u001b[0m 84.09   \u001b[0m | \u001b[0m 0.000890\u001b[0m | \u001b[0m 37.54   \u001b[0m |\n",
      "| \u001b[0m 409     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.5015  \u001b[0m | \u001b[0m 5.278   \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.8753  \u001b[0m | \u001b[0m 0.2748  \u001b[0m | \u001b[0m 2.319   \u001b[0m | \u001b[0m 0.1028  \u001b[0m | \u001b[0m 36.18   \u001b[0m | \u001b[0m 34.21   \u001b[0m | \u001b[0m 0.004455\u001b[0m | \u001b[0m 28.03   \u001b[0m |\n",
      "| \u001b[0m 410     \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.6769  \u001b[0m | \u001b[0m 9.251   \u001b[0m | \u001b[0m 0.7217  \u001b[0m | \u001b[0m 0.5264  \u001b[0m | \u001b[0m 2.675   \u001b[0m | \u001b[0m 2.511   \u001b[0m | \u001b[0m 0.1995  \u001b[0m | \u001b[0m 95.92   \u001b[0m | \u001b[0m 74.15   \u001b[0m | \u001b[0m 0.004215\u001b[0m | \u001b[0m 41.89   \u001b[0m |\n",
      "| \u001b[0m 411     \u001b[0m | \u001b[0m 0.9351  \u001b[0m | \u001b[0m 0.6473  \u001b[0m | \u001b[0m 4.567   \u001b[0m | \u001b[0m 0.593   \u001b[0m | \u001b[0m 0.4461  \u001b[0m | \u001b[0m 1.597   \u001b[0m | \u001b[0m 0.913   \u001b[0m | \u001b[0m 0.1869  \u001b[0m | \u001b[0m 78.78   \u001b[0m | \u001b[0m 98.0    \u001b[0m | \u001b[0m 0.009464\u001b[0m | \u001b[0m 40.63   \u001b[0m |\n",
      "| \u001b[0m 412     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.8541  \u001b[0m | \u001b[0m 4.11    \u001b[0m | \u001b[0m 0.4877  \u001b[0m | \u001b[0m 0.7716  \u001b[0m | \u001b[0m 2.993   \u001b[0m | \u001b[0m 0.812   \u001b[0m | \u001b[0m 0.02278 \u001b[0m | \u001b[0m 67.39   \u001b[0m | \u001b[0m 85.38   \u001b[0m | \u001b[0m 0.002504\u001b[0m | \u001b[0m 51.43   \u001b[0m |\n",
      "| \u001b[0m 413     \u001b[0m | \u001b[0m 0.9391  \u001b[0m | \u001b[0m 0.7712  \u001b[0m | \u001b[0m 2.445   \u001b[0m | \u001b[0m 0.9962  \u001b[0m | \u001b[0m 0.9379  \u001b[0m | \u001b[0m 2.107   \u001b[0m | \u001b[0m 0.5683  \u001b[0m | \u001b[0m 0.1355  \u001b[0m | \u001b[0m 87.19   \u001b[0m | \u001b[0m 43.48   \u001b[0m | \u001b[0m 0.002418\u001b[0m | \u001b[0m 33.45   \u001b[0m |\n",
      "| \u001b[0m 414     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.8526  \u001b[0m | \u001b[0m 4.39    \u001b[0m | \u001b[0m 0.7162  \u001b[0m | \u001b[0m 0.8413  \u001b[0m | \u001b[0m 2.682   \u001b[0m | \u001b[0m 1.282   \u001b[0m | \u001b[0m 0.1975  \u001b[0m | \u001b[0m 82.82   \u001b[0m | \u001b[0m 80.95   \u001b[0m | \u001b[0m 0.00596 \u001b[0m | \u001b[0m 31.96   \u001b[0m |\n",
      "| \u001b[0m 415     \u001b[0m | \u001b[0m 0.9392  \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 9.615   \u001b[0m | \u001b[0m 0.9182  \u001b[0m | \u001b[0m 0.4909  \u001b[0m | \u001b[0m 2.589   \u001b[0m | \u001b[0m 1.749   \u001b[0m | \u001b[0m 0.1421  \u001b[0m | \u001b[0m 21.77   \u001b[0m | \u001b[0m 56.46   \u001b[0m | \u001b[0m 0.003554\u001b[0m | \u001b[0m 41.22   \u001b[0m |\n",
      "| \u001b[0m 416     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.4147  \u001b[0m | \u001b[0m 2.35    \u001b[0m | \u001b[0m 0.8916  \u001b[0m | \u001b[0m 0.8195  \u001b[0m | \u001b[0m 1.928   \u001b[0m | \u001b[0m 1.495   \u001b[0m | \u001b[0m 0.1578  \u001b[0m | \u001b[0m 12.3    \u001b[0m | \u001b[0m 35.45   \u001b[0m | \u001b[0m 0.004137\u001b[0m | \u001b[0m 49.88   \u001b[0m |\n",
      "| \u001b[0m 417     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.615   \u001b[0m | \u001b[0m 6.526   \u001b[0m | \u001b[0m 0.4795  \u001b[0m | \u001b[0m 0.641   \u001b[0m | \u001b[0m 1.768   \u001b[0m | \u001b[0m 0.3173  \u001b[0m | \u001b[0m 0.02737 \u001b[0m | \u001b[0m 68.76   \u001b[0m | \u001b[0m 83.78   \u001b[0m | \u001b[0m 0.009877\u001b[0m | \u001b[0m 57.73   \u001b[0m |\n",
      "| \u001b[0m 418     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m 2.24    \u001b[0m | \u001b[0m 0.7091  \u001b[0m | \u001b[0m 0.5773  \u001b[0m | \u001b[0m 0.2279  \u001b[0m | \u001b[0m 0.8465  \u001b[0m | \u001b[0m 0.05558 \u001b[0m | \u001b[0m 50.67   \u001b[0m | \u001b[0m 87.02   \u001b[0m | \u001b[0m 0.004028\u001b[0m | \u001b[0m 31.64   \u001b[0m |\n",
      "| \u001b[0m 419     \u001b[0m | \u001b[0m 0.9445  \u001b[0m | \u001b[0m 0.812   \u001b[0m | \u001b[0m 8.132   \u001b[0m | \u001b[0m 0.5737  \u001b[0m | \u001b[0m 0.6128  \u001b[0m | \u001b[0m 2.935   \u001b[0m | \u001b[0m 0.007524\u001b[0m | \u001b[0m 0.1526  \u001b[0m | \u001b[0m 67.85   \u001b[0m | \u001b[0m 64.53   \u001b[0m | \u001b[0m 0.001321\u001b[0m | \u001b[0m 29.32   \u001b[0m |\n",
      "| \u001b[0m 420     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.5085  \u001b[0m | \u001b[0m 7.002   \u001b[0m | \u001b[0m 0.7678  \u001b[0m | \u001b[0m 0.846   \u001b[0m | \u001b[0m 1.914   \u001b[0m | \u001b[0m 2.73    \u001b[0m | \u001b[0m 0.01449 \u001b[0m | \u001b[0m 77.64   \u001b[0m | \u001b[0m 51.25   \u001b[0m | \u001b[0m 0.008414\u001b[0m | \u001b[0m 24.63   \u001b[0m |\n",
      "| \u001b[0m 421     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 4.585   \u001b[0m | \u001b[0m 0.4543  \u001b[0m | \u001b[0m 0.7893  \u001b[0m | \u001b[0m 2.305   \u001b[0m | \u001b[0m 0.6901  \u001b[0m | \u001b[0m 0.1285  \u001b[0m | \u001b[0m 98.96   \u001b[0m | \u001b[0m 28.07   \u001b[0m | \u001b[0m 0.004657\u001b[0m | \u001b[0m 62.07   \u001b[0m |\n",
      "| \u001b[0m 422     \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.7168  \u001b[0m | \u001b[0m 4.026   \u001b[0m | \u001b[0m 0.4344  \u001b[0m | \u001b[0m 0.9597  \u001b[0m | \u001b[0m 2.496   \u001b[0m | \u001b[0m 2.865   \u001b[0m | \u001b[0m 0.1492  \u001b[0m | \u001b[0m 15.36   \u001b[0m | \u001b[0m 86.34   \u001b[0m | \u001b[0m 0.006408\u001b[0m | \u001b[0m 87.7    \u001b[0m |\n",
      "| \u001b[0m 423     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.5776  \u001b[0m | \u001b[0m 4.93    \u001b[0m | \u001b[0m 0.5268  \u001b[0m | \u001b[0m 0.6077  \u001b[0m | \u001b[0m 2.321   \u001b[0m | \u001b[0m 0.7865  \u001b[0m | \u001b[0m 0.1717  \u001b[0m | \u001b[0m 97.75   \u001b[0m | \u001b[0m 27.81   \u001b[0m | \u001b[0m 0.009119\u001b[0m | \u001b[0m 62.14   \u001b[0m |\n",
      "| \u001b[0m 424     \u001b[0m | \u001b[0m 0.9411  \u001b[0m | \u001b[0m 0.5476  \u001b[0m | \u001b[0m 6.784   \u001b[0m | \u001b[0m 0.8193  \u001b[0m | \u001b[0m 0.8606  \u001b[0m | \u001b[0m 1.64    \u001b[0m | \u001b[0m 0.4046  \u001b[0m | \u001b[0m 0.1787  \u001b[0m | \u001b[0m 41.07   \u001b[0m | \u001b[0m 44.28   \u001b[0m | \u001b[0m 0.004738\u001b[0m | \u001b[0m 27.75   \u001b[0m |\n",
      "| \u001b[0m 425     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.4608  \u001b[0m | \u001b[0m 5.952   \u001b[0m | \u001b[0m 0.5152  \u001b[0m | \u001b[0m 0.836   \u001b[0m | \u001b[0m 2.401   \u001b[0m | \u001b[0m 1.633   \u001b[0m | \u001b[0m 0.06785 \u001b[0m | \u001b[0m 80.92   \u001b[0m | \u001b[0m 96.71   \u001b[0m | \u001b[0m 0.007494\u001b[0m | \u001b[0m 70.06   \u001b[0m |\n",
      "| \u001b[0m 426     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.4906  \u001b[0m | \u001b[0m 1.533   \u001b[0m | \u001b[0m 0.8093  \u001b[0m | \u001b[0m 0.8452  \u001b[0m | \u001b[0m 1.045   \u001b[0m | \u001b[0m 1.512   \u001b[0m | \u001b[0m 0.0712  \u001b[0m | \u001b[0m 82.06   \u001b[0m | \u001b[0m 61.8    \u001b[0m | \u001b[0m 0.00618 \u001b[0m | \u001b[0m 24.28   \u001b[0m |\n",
      "| \u001b[0m 427     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.4363  \u001b[0m | \u001b[0m 3.322   \u001b[0m | \u001b[0m 0.8231  \u001b[0m | \u001b[0m 0.6014  \u001b[0m | \u001b[0m 1.526   \u001b[0m | \u001b[0m 1.407   \u001b[0m | \u001b[0m 0.1485  \u001b[0m | \u001b[0m 91.41   \u001b[0m | \u001b[0m 59.2    \u001b[0m | \u001b[0m 0.005053\u001b[0m | \u001b[0m 25.15   \u001b[0m |\n",
      "| \u001b[0m 428     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.4431  \u001b[0m | \u001b[0m 2.108   \u001b[0m | \u001b[0m 0.6572  \u001b[0m | \u001b[0m 0.6156  \u001b[0m | \u001b[0m 0.8536  \u001b[0m | \u001b[0m 0.7948  \u001b[0m | \u001b[0m 0.03906 \u001b[0m | \u001b[0m 77.99   \u001b[0m | \u001b[0m 54.56   \u001b[0m | \u001b[0m 0.005205\u001b[0m | \u001b[0m 69.08   \u001b[0m |\n",
      "| \u001b[0m 429     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.9258  \u001b[0m | \u001b[0m 7.838   \u001b[0m | \u001b[0m 0.8061  \u001b[0m | \u001b[0m 0.4928  \u001b[0m | \u001b[0m 0.6216  \u001b[0m | \u001b[0m 2.16    \u001b[0m | \u001b[0m 0.1643  \u001b[0m | \u001b[0m 96.11   \u001b[0m | \u001b[0m 41.18   \u001b[0m | \u001b[0m 0.005137\u001b[0m | \u001b[0m 86.45   \u001b[0m |\n",
      "| \u001b[95m 430     \u001b[0m | \u001b[95m 0.9448  \u001b[0m | \u001b[95m 0.9364  \u001b[0m | \u001b[95m 6.517   \u001b[0m | \u001b[95m 0.8166  \u001b[0m | \u001b[95m 0.6416  \u001b[0m | \u001b[95m 1.896   \u001b[0m | \u001b[95m 1.818   \u001b[0m | \u001b[95m 0.1891  \u001b[0m | \u001b[95m 20.88   \u001b[0m | \u001b[95m 87.96   \u001b[0m | \u001b[95m 0.000704\u001b[0m | \u001b[95m 48.44   \u001b[0m |\n",
      "| \u001b[0m 431     \u001b[0m | \u001b[0m 0.9387  \u001b[0m | \u001b[0m 0.894   \u001b[0m | \u001b[0m 6.404   \u001b[0m | \u001b[0m 0.4673  \u001b[0m | \u001b[0m 0.9943  \u001b[0m | \u001b[0m 1.644   \u001b[0m | \u001b[0m 0.7458  \u001b[0m | \u001b[0m 0.1012  \u001b[0m | \u001b[0m 48.07   \u001b[0m | \u001b[0m 22.75   \u001b[0m | \u001b[0m 0.002449\u001b[0m | \u001b[0m 75.52   \u001b[0m |\n",
      "| \u001b[0m 432     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.5993  \u001b[0m | \u001b[0m 8.524   \u001b[0m | \u001b[0m 0.8518  \u001b[0m | \u001b[0m 0.4095  \u001b[0m | \u001b[0m 1.003   \u001b[0m | \u001b[0m 2.12    \u001b[0m | \u001b[0m 0.06339 \u001b[0m | \u001b[0m 22.37   \u001b[0m | \u001b[0m 37.45   \u001b[0m | \u001b[0m 0.002161\u001b[0m | \u001b[0m 93.21   \u001b[0m |\n",
      "| \u001b[0m 433     \u001b[0m | \u001b[0m 0.9399  \u001b[0m | \u001b[0m 0.5691  \u001b[0m | \u001b[0m 9.155   \u001b[0m | \u001b[0m 0.9218  \u001b[0m | \u001b[0m 0.9617  \u001b[0m | \u001b[0m 0.2219  \u001b[0m | \u001b[0m 1.271   \u001b[0m | \u001b[0m 0.1977  \u001b[0m | \u001b[0m 41.32   \u001b[0m | \u001b[0m 88.12   \u001b[0m | \u001b[0m 0.002857\u001b[0m | \u001b[0m 87.39   \u001b[0m |\n",
      "| \u001b[0m 434     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.9044  \u001b[0m | \u001b[0m 8.197   \u001b[0m | \u001b[0m 0.5044  \u001b[0m | \u001b[0m 0.7373  \u001b[0m | \u001b[0m 1.699   \u001b[0m | \u001b[0m 2.504   \u001b[0m | \u001b[0m 0.1597  \u001b[0m | \u001b[0m 44.46   \u001b[0m | \u001b[0m 52.61   \u001b[0m | \u001b[0m 0.008426\u001b[0m | \u001b[0m 56.28   \u001b[0m |\n",
      "| \u001b[0m 435     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.4832  \u001b[0m | \u001b[0m 2.093   \u001b[0m | \u001b[0m 0.8623  \u001b[0m | \u001b[0m 0.6897  \u001b[0m | \u001b[0m 1.494   \u001b[0m | \u001b[0m 2.905   \u001b[0m | \u001b[0m 0.147   \u001b[0m | \u001b[0m 25.27   \u001b[0m | \u001b[0m 79.08   \u001b[0m | \u001b[0m 0.00905 \u001b[0m | \u001b[0m 54.29   \u001b[0m |\n",
      "| \u001b[0m 436     \u001b[0m | \u001b[0m 0.9402  \u001b[0m | \u001b[0m 0.7053  \u001b[0m | \u001b[0m 7.842   \u001b[0m | \u001b[0m 0.5952  \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 2.363   \u001b[0m | \u001b[0m 0.3908  \u001b[0m | \u001b[0m 0.09542 \u001b[0m | \u001b[0m 42.54   \u001b[0m | \u001b[0m 47.37   \u001b[0m | \u001b[0m 0.003258\u001b[0m | \u001b[0m 77.74   \u001b[0m |\n",
      "| \u001b[0m 437     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.5076  \u001b[0m | \u001b[0m 8.152   \u001b[0m | \u001b[0m 0.8017  \u001b[0m | \u001b[0m 0.4483  \u001b[0m | \u001b[0m 1.469   \u001b[0m | \u001b[0m 2.634   \u001b[0m | \u001b[0m 0.05099 \u001b[0m | \u001b[0m 19.46   \u001b[0m | \u001b[0m 84.71   \u001b[0m | \u001b[0m 0.002796\u001b[0m | \u001b[0m 47.93   \u001b[0m |\n",
      "| \u001b[0m 438     \u001b[0m | \u001b[0m 0.9358  \u001b[0m | \u001b[0m 0.6946  \u001b[0m | \u001b[0m 7.306   \u001b[0m | \u001b[0m 0.4238  \u001b[0m | \u001b[0m 0.4837  \u001b[0m | \u001b[0m 0.7772  \u001b[0m | \u001b[0m 0.9966  \u001b[0m | \u001b[0m 0.1052  \u001b[0m | \u001b[0m 16.77   \u001b[0m | \u001b[0m 92.8    \u001b[0m | \u001b[0m 0.001193\u001b[0m | \u001b[0m 50.26   \u001b[0m |\n",
      "| \u001b[0m 439     \u001b[0m | \u001b[0m 0.9404  \u001b[0m | \u001b[0m 0.7048  \u001b[0m | \u001b[0m 3.89    \u001b[0m | \u001b[0m 0.7918  \u001b[0m | \u001b[0m 0.9622  \u001b[0m | \u001b[0m 1.188   \u001b[0m | \u001b[0m 1.782   \u001b[0m | \u001b[0m 0.07285 \u001b[0m | \u001b[0m 66.68   \u001b[0m | \u001b[0m 47.56   \u001b[0m | \u001b[0m 0.002048\u001b[0m | \u001b[0m 29.27   \u001b[0m |\n",
      "| \u001b[0m 440     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.8281  \u001b[0m | \u001b[0m 1.076   \u001b[0m | \u001b[0m 0.4381  \u001b[0m | \u001b[0m 0.8144  \u001b[0m | \u001b[0m 1.205   \u001b[0m | \u001b[0m 1.616   \u001b[0m | \u001b[0m 0.06878 \u001b[0m | \u001b[0m 76.69   \u001b[0m | \u001b[0m 22.75   \u001b[0m | \u001b[0m 0.00951 \u001b[0m | \u001b[0m 31.5    \u001b[0m |\n",
      "| \u001b[0m 441     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.8962  \u001b[0m | \u001b[0m 7.056   \u001b[0m | \u001b[0m 0.5639  \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.8327  \u001b[0m | \u001b[0m 0.8941  \u001b[0m | \u001b[0m 0.1018  \u001b[0m | \u001b[0m 31.3    \u001b[0m | \u001b[0m 46.46   \u001b[0m | \u001b[0m 0.001475\u001b[0m | \u001b[0m 49.19   \u001b[0m |\n",
      "| \u001b[0m 442     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.7974  \u001b[0m | \u001b[0m 8.188   \u001b[0m | \u001b[0m 0.8185  \u001b[0m | \u001b[0m 0.6413  \u001b[0m | \u001b[0m 1.077   \u001b[0m | \u001b[0m 0.775   \u001b[0m | \u001b[0m 0.09163 \u001b[0m | \u001b[0m 46.44   \u001b[0m | \u001b[0m 75.0    \u001b[0m | \u001b[0m 0.007558\u001b[0m | \u001b[0m 28.42   \u001b[0m |\n",
      "| \u001b[0m 443     \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.452   \u001b[0m | \u001b[0m 5.431   \u001b[0m | \u001b[0m 0.9181  \u001b[0m | \u001b[0m 0.7694  \u001b[0m | \u001b[0m 2.712   \u001b[0m | \u001b[0m 1.03    \u001b[0m | \u001b[0m 0.1998  \u001b[0m | \u001b[0m 98.65   \u001b[0m | \u001b[0m 27.7    \u001b[0m | \u001b[0m 0.002601\u001b[0m | \u001b[0m 61.47   \u001b[0m |\n",
      "| \u001b[0m 444     \u001b[0m | \u001b[0m 0.9446  \u001b[0m | \u001b[0m 0.8844  \u001b[0m | \u001b[0m 7.723   \u001b[0m | \u001b[0m 0.8345  \u001b[0m | \u001b[0m 0.563   \u001b[0m | \u001b[0m 0.987   \u001b[0m | \u001b[0m 1.454   \u001b[0m | \u001b[0m 0.1056  \u001b[0m | \u001b[0m 16.47   \u001b[0m | \u001b[0m 90.07   \u001b[0m | \u001b[0m 0.007421\u001b[0m | \u001b[0m 68.05   \u001b[0m |\n",
      "| \u001b[0m 445     \u001b[0m | \u001b[0m 0.9382  \u001b[0m | \u001b[0m 0.9031  \u001b[0m | \u001b[0m 6.488   \u001b[0m | \u001b[0m 0.9671  \u001b[0m | \u001b[0m 0.836   \u001b[0m | \u001b[0m 1.515   \u001b[0m | \u001b[0m 1.872   \u001b[0m | \u001b[0m 0.04599 \u001b[0m | \u001b[0m 68.76   \u001b[0m | \u001b[0m 73.36   \u001b[0m | \u001b[0m 0.004884\u001b[0m | \u001b[0m 65.07   \u001b[0m |\n",
      "| \u001b[0m 446     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.9063  \u001b[0m | \u001b[0m 3.896   \u001b[0m | \u001b[0m 0.631   \u001b[0m | \u001b[0m 0.5941  \u001b[0m | \u001b[0m 2.214   \u001b[0m | \u001b[0m 0.8874  \u001b[0m | \u001b[0m 0.04042 \u001b[0m | \u001b[0m 13.21   \u001b[0m | \u001b[0m 43.76   \u001b[0m | \u001b[0m 0.002361\u001b[0m | \u001b[0m 62.94   \u001b[0m |\n",
      "| \u001b[0m 447     \u001b[0m | \u001b[0m 0.9377  \u001b[0m | \u001b[0m 0.804   \u001b[0m | \u001b[0m 1.673   \u001b[0m | \u001b[0m 0.5196  \u001b[0m | \u001b[0m 0.5159  \u001b[0m | \u001b[0m 0.9256  \u001b[0m | \u001b[0m 1.083   \u001b[0m | \u001b[0m 0.1397  \u001b[0m | \u001b[0m 46.81   \u001b[0m | \u001b[0m 94.92   \u001b[0m | \u001b[0m 0.002149\u001b[0m | \u001b[0m 70.46   \u001b[0m |\n",
      "| \u001b[0m 448     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.7552  \u001b[0m | \u001b[0m 2.834   \u001b[0m | \u001b[0m 0.5789  \u001b[0m | \u001b[0m 0.6666  \u001b[0m | \u001b[0m 1.117   \u001b[0m | \u001b[0m 0.6629  \u001b[0m | \u001b[0m 0.1409  \u001b[0m | \u001b[0m 31.0    \u001b[0m | \u001b[0m 34.21   \u001b[0m | \u001b[0m 0.005656\u001b[0m | \u001b[0m 95.31   \u001b[0m |\n",
      "| \u001b[0m 449     \u001b[0m | \u001b[0m 0.9319  \u001b[0m | \u001b[0m 0.9747  \u001b[0m | \u001b[0m 3.398   \u001b[0m | \u001b[0m 0.5875  \u001b[0m | \u001b[0m 0.5416  \u001b[0m | \u001b[0m 1.286   \u001b[0m | \u001b[0m 2.339   \u001b[0m | \u001b[0m 0.1926  \u001b[0m | \u001b[0m 48.34   \u001b[0m | \u001b[0m 25.34   \u001b[0m | \u001b[0m 0.001565\u001b[0m | \u001b[0m 86.9    \u001b[0m |\n",
      "| \u001b[0m 450     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.8083  \u001b[0m | \u001b[0m 6.413   \u001b[0m | \u001b[0m 0.4432  \u001b[0m | \u001b[0m 0.4244  \u001b[0m | \u001b[0m 1.477   \u001b[0m | \u001b[0m 2.439   \u001b[0m | \u001b[0m 0.03374 \u001b[0m | \u001b[0m 13.99   \u001b[0m | \u001b[0m 65.3    \u001b[0m | \u001b[0m 0.003456\u001b[0m | \u001b[0m 91.48   \u001b[0m |\n",
      "| \u001b[0m 451     \u001b[0m | \u001b[0m 0.9406  \u001b[0m | \u001b[0m 0.8613  \u001b[0m | \u001b[0m 2.933   \u001b[0m | \u001b[0m 0.6813  \u001b[0m | \u001b[0m 0.9669  \u001b[0m | \u001b[0m 1.718   \u001b[0m | \u001b[0m 1.589   \u001b[0m | \u001b[0m 0.08546 \u001b[0m | \u001b[0m 99.11   \u001b[0m | \u001b[0m 83.29   \u001b[0m | \u001b[0m 0.005501\u001b[0m | \u001b[0m 70.66   \u001b[0m |\n",
      "| \u001b[0m 452     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.8275  \u001b[0m | \u001b[0m 9.55    \u001b[0m | \u001b[0m 0.6749  \u001b[0m | \u001b[0m 0.4542  \u001b[0m | \u001b[0m 1.406   \u001b[0m | \u001b[0m 0.8845  \u001b[0m | \u001b[0m 0.04462 \u001b[0m | \u001b[0m 40.04   \u001b[0m | \u001b[0m 33.21   \u001b[0m | \u001b[0m 0.002598\u001b[0m | \u001b[0m 73.1    \u001b[0m |\n",
      "| \u001b[0m 453     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.8481  \u001b[0m | \u001b[0m 7.436   \u001b[0m | \u001b[0m 0.7972  \u001b[0m | \u001b[0m 0.4718  \u001b[0m | \u001b[0m 1.029   \u001b[0m | \u001b[0m 0.9831  \u001b[0m | \u001b[0m 0.1502  \u001b[0m | \u001b[0m 85.49   \u001b[0m | \u001b[0m 23.74   \u001b[0m | \u001b[0m 0.001777\u001b[0m | \u001b[0m 94.84   \u001b[0m |\n",
      "| \u001b[0m 454     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.6329  \u001b[0m | \u001b[0m 3.332   \u001b[0m | \u001b[0m 0.782   \u001b[0m | \u001b[0m 0.8196  \u001b[0m | \u001b[0m 1.42    \u001b[0m | \u001b[0m 2.215   \u001b[0m | \u001b[0m 0.01788 \u001b[0m | \u001b[0m 42.76   \u001b[0m | \u001b[0m 44.2    \u001b[0m | \u001b[0m 0.000608\u001b[0m | \u001b[0m 49.09   \u001b[0m |\n",
      "| \u001b[0m 455     \u001b[0m | \u001b[0m 0.9333  \u001b[0m | \u001b[0m 0.6609  \u001b[0m | \u001b[0m 8.134   \u001b[0m | \u001b[0m 0.4791  \u001b[0m | \u001b[0m 0.4865  \u001b[0m | \u001b[0m 2.786   \u001b[0m | \u001b[0m 0.328   \u001b[0m | \u001b[0m 0.07094 \u001b[0m | \u001b[0m 83.57   \u001b[0m | \u001b[0m 22.3    \u001b[0m | \u001b[0m 0.006009\u001b[0m | \u001b[0m 27.54   \u001b[0m |\n",
      "| \u001b[0m 456     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.7293  \u001b[0m | \u001b[0m 3.12    \u001b[0m | \u001b[0m 0.916   \u001b[0m | \u001b[0m 0.7309  \u001b[0m | \u001b[0m 0.4059  \u001b[0m | \u001b[0m 0.4292  \u001b[0m | \u001b[0m 0.08834 \u001b[0m | \u001b[0m 25.07   \u001b[0m | \u001b[0m 76.26   \u001b[0m | \u001b[0m 0.007261\u001b[0m | \u001b[0m 40.89   \u001b[0m |\n",
      "| \u001b[0m 457     \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.8298  \u001b[0m | \u001b[0m 4.089   \u001b[0m | \u001b[0m 0.529   \u001b[0m | \u001b[0m 0.7673  \u001b[0m | \u001b[0m 1.029   \u001b[0m | \u001b[0m 1.672   \u001b[0m | \u001b[0m 0.09091 \u001b[0m | \u001b[0m 46.63   \u001b[0m | \u001b[0m 97.26   \u001b[0m | \u001b[0m 0.004346\u001b[0m | \u001b[0m 88.62   \u001b[0m |\n",
      "| \u001b[0m 458     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.4426  \u001b[0m | \u001b[0m 4.196   \u001b[0m | \u001b[0m 0.4618  \u001b[0m | \u001b[0m 0.9138  \u001b[0m | \u001b[0m 2.245   \u001b[0m | \u001b[0m 0.8525  \u001b[0m | \u001b[0m 0.04924 \u001b[0m | \u001b[0m 98.59   \u001b[0m | \u001b[0m 27.22   \u001b[0m | \u001b[0m 0.000939\u001b[0m | \u001b[0m 63.24   \u001b[0m |\n",
      "| \u001b[0m 459     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.6533  \u001b[0m | \u001b[0m 9.661   \u001b[0m | \u001b[0m 0.6872  \u001b[0m | \u001b[0m 0.777   \u001b[0m | \u001b[0m 0.2035  \u001b[0m | \u001b[0m 0.4282  \u001b[0m | \u001b[0m 0.03334 \u001b[0m | \u001b[0m 76.89   \u001b[0m | \u001b[0m 77.6    \u001b[0m | \u001b[0m 0.009464\u001b[0m | \u001b[0m 49.85   \u001b[0m |\n",
      "| \u001b[0m 460     \u001b[0m | \u001b[0m 0.9403  \u001b[0m | \u001b[0m 0.6785  \u001b[0m | \u001b[0m 6.203   \u001b[0m | \u001b[0m 0.5022  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.35    \u001b[0m | \u001b[0m 0.7563  \u001b[0m | \u001b[0m 0.1535  \u001b[0m | \u001b[0m 45.36   \u001b[0m | \u001b[0m 59.79   \u001b[0m | \u001b[0m 0.005363\u001b[0m | \u001b[0m 41.05   \u001b[0m |\n",
      "| \u001b[0m 461     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.9933  \u001b[0m | \u001b[0m 5.584   \u001b[0m | \u001b[0m 0.9031  \u001b[0m | \u001b[0m 0.4204  \u001b[0m | \u001b[0m 1.314   \u001b[0m | \u001b[0m 2.049   \u001b[0m | \u001b[0m 0.1272  \u001b[0m | \u001b[0m 24.84   \u001b[0m | \u001b[0m 40.66   \u001b[0m | \u001b[0m 0.008973\u001b[0m | \u001b[0m 30.32   \u001b[0m |\n",
      "| \u001b[0m 462     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.9003  \u001b[0m | \u001b[0m 8.241   \u001b[0m | \u001b[0m 0.4946  \u001b[0m | \u001b[0m 0.7856  \u001b[0m | \u001b[0m 1.968   \u001b[0m | \u001b[0m 1.078   \u001b[0m | \u001b[0m 0.1848  \u001b[0m | \u001b[0m 79.02   \u001b[0m | \u001b[0m 57.48   \u001b[0m | \u001b[0m 0.002951\u001b[0m | \u001b[0m 66.15   \u001b[0m |\n",
      "| \u001b[0m 463     \u001b[0m | \u001b[0m 0.9387  \u001b[0m | \u001b[0m 0.5662  \u001b[0m | \u001b[0m 3.016   \u001b[0m | \u001b[0m 0.4592  \u001b[0m | \u001b[0m 0.5199  \u001b[0m | \u001b[0m 2.259   \u001b[0m | \u001b[0m 1.278   \u001b[0m | \u001b[0m 0.1992  \u001b[0m | \u001b[0m 72.93   \u001b[0m | \u001b[0m 61.06   \u001b[0m | \u001b[0m 0.003109\u001b[0m | \u001b[0m 63.87   \u001b[0m |\n",
      "| \u001b[0m 464     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.4873  \u001b[0m | \u001b[0m 2.403   \u001b[0m | \u001b[0m 0.7057  \u001b[0m | \u001b[0m 0.6366  \u001b[0m | \u001b[0m 1.974   \u001b[0m | \u001b[0m 1.493   \u001b[0m | \u001b[0m 0.01482 \u001b[0m | \u001b[0m 58.74   \u001b[0m | \u001b[0m 45.46   \u001b[0m | \u001b[0m 0.006693\u001b[0m | \u001b[0m 93.61   \u001b[0m |\n",
      "| \u001b[0m 465     \u001b[0m | \u001b[0m 0.9408  \u001b[0m | \u001b[0m 0.4935  \u001b[0m | \u001b[0m 8.815   \u001b[0m | \u001b[0m 0.4583  \u001b[0m | \u001b[0m 0.7522  \u001b[0m | \u001b[0m 1.868   \u001b[0m | \u001b[0m 2.746   \u001b[0m | \u001b[0m 0.1772  \u001b[0m | \u001b[0m 22.17   \u001b[0m | \u001b[0m 36.76   \u001b[0m | \u001b[0m 0.002843\u001b[0m | \u001b[0m 93.83   \u001b[0m |\n",
      "| \u001b[0m 466     \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.8325  \u001b[0m | \u001b[0m 2.275   \u001b[0m | \u001b[0m 0.401   \u001b[0m | \u001b[0m 0.8295  \u001b[0m | \u001b[0m 1.982   \u001b[0m | \u001b[0m 1.498   \u001b[0m | \u001b[0m 0.1661  \u001b[0m | \u001b[0m 77.82   \u001b[0m | \u001b[0m 54.57   \u001b[0m | \u001b[0m 0.006556\u001b[0m | \u001b[0m 69.48   \u001b[0m |\n",
      "| \u001b[0m 467     \u001b[0m | \u001b[0m 0.9443  \u001b[0m | \u001b[0m 0.531   \u001b[0m | \u001b[0m 2.309   \u001b[0m | \u001b[0m 0.6492  \u001b[0m | \u001b[0m 0.5666  \u001b[0m | \u001b[0m 1.107   \u001b[0m | \u001b[0m 1.208   \u001b[0m | \u001b[0m 0.1226  \u001b[0m | \u001b[0m 18.45   \u001b[0m | \u001b[0m 33.2    \u001b[0m | \u001b[0m 0.005702\u001b[0m | \u001b[0m 63.51   \u001b[0m |\n",
      "| \u001b[0m 468     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.7242  \u001b[0m | \u001b[0m 1.592   \u001b[0m | \u001b[0m 0.4314  \u001b[0m | \u001b[0m 0.5802  \u001b[0m | \u001b[0m 2.295   \u001b[0m | \u001b[0m 1.773   \u001b[0m | \u001b[0m 0.1516  \u001b[0m | \u001b[0m 44.99   \u001b[0m | \u001b[0m 94.7    \u001b[0m | \u001b[0m 0.00347 \u001b[0m | \u001b[0m 65.01   \u001b[0m |\n",
      "| \u001b[0m 469     \u001b[0m | \u001b[0m 0.9374  \u001b[0m | \u001b[0m 0.8384  \u001b[0m | \u001b[0m 4.142   \u001b[0m | \u001b[0m 0.9784  \u001b[0m | \u001b[0m 0.9565  \u001b[0m | \u001b[0m 0.1515  \u001b[0m | \u001b[0m 0.8035  \u001b[0m | \u001b[0m 0.1037  \u001b[0m | \u001b[0m 46.58   \u001b[0m | \u001b[0m 96.8    \u001b[0m | \u001b[0m 0.003912\u001b[0m | \u001b[0m 88.51   \u001b[0m |\n",
      "| \u001b[0m 470     \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.8535  \u001b[0m | \u001b[0m 7.818   \u001b[0m | \u001b[0m 0.6721  \u001b[0m | \u001b[0m 0.473   \u001b[0m | \u001b[0m 1.087   \u001b[0m | \u001b[0m 0.4742  \u001b[0m | \u001b[0m 0.0293  \u001b[0m | \u001b[0m 61.76   \u001b[0m | \u001b[0m 28.8    \u001b[0m | \u001b[0m 0.008796\u001b[0m | \u001b[0m 67.35   \u001b[0m |\n",
      "| \u001b[0m 471     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.7211  \u001b[0m | \u001b[0m 1.528   \u001b[0m | \u001b[0m 0.6639  \u001b[0m | \u001b[0m 0.5282  \u001b[0m | \u001b[0m 1.736   \u001b[0m | \u001b[0m 0.1454  \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 47.88   \u001b[0m | \u001b[0m 91.16   \u001b[0m | \u001b[0m 0.004942\u001b[0m | \u001b[0m 93.25   \u001b[0m |\n",
      "| \u001b[0m 472     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.4155  \u001b[0m | \u001b[0m 4.659   \u001b[0m | \u001b[0m 0.431   \u001b[0m | \u001b[0m 0.4074  \u001b[0m | \u001b[0m 0.4931  \u001b[0m | \u001b[0m 2.4     \u001b[0m | \u001b[0m 0.1818  \u001b[0m | \u001b[0m 92.76   \u001b[0m | \u001b[0m 84.73   \u001b[0m | \u001b[0m 0.007922\u001b[0m | \u001b[0m 22.4    \u001b[0m |\n",
      "| \u001b[0m 473     \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.5797  \u001b[0m | \u001b[0m 2.237   \u001b[0m | \u001b[0m 0.6099  \u001b[0m | \u001b[0m 0.8903  \u001b[0m | \u001b[0m 2.796   \u001b[0m | \u001b[0m 1.822   \u001b[0m | \u001b[0m 0.1992  \u001b[0m | \u001b[0m 28.52   \u001b[0m | \u001b[0m 46.82   \u001b[0m | \u001b[0m 0.006851\u001b[0m | \u001b[0m 99.54   \u001b[0m |\n",
      "| \u001b[0m 474     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 2.549   \u001b[0m | \u001b[0m 0.505   \u001b[0m | \u001b[0m 0.6179  \u001b[0m | \u001b[0m 0.7923  \u001b[0m | \u001b[0m 1.711   \u001b[0m | \u001b[0m 0.1526  \u001b[0m | \u001b[0m 22.68   \u001b[0m | \u001b[0m 69.73   \u001b[0m | \u001b[0m 0.009777\u001b[0m | \u001b[0m 63.38   \u001b[0m |\n",
      "| \u001b[0m 475     \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.5394  \u001b[0m | \u001b[0m 5.468   \u001b[0m | \u001b[0m 0.8274  \u001b[0m | \u001b[0m 0.8874  \u001b[0m | \u001b[0m 2.319   \u001b[0m | \u001b[0m 2.726   \u001b[0m | \u001b[0m 0.02431 \u001b[0m | \u001b[0m 46.22   \u001b[0m | \u001b[0m 57.5    \u001b[0m | \u001b[0m 0.008329\u001b[0m | \u001b[0m 71.18   \u001b[0m |\n",
      "| \u001b[0m 476     \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.5475  \u001b[0m | \u001b[0m 4.028   \u001b[0m | \u001b[0m 0.7604  \u001b[0m | \u001b[0m 0.9392  \u001b[0m | \u001b[0m 0.9958  \u001b[0m | \u001b[0m 1.176   \u001b[0m | \u001b[0m 0.1965  \u001b[0m | \u001b[0m 86.97   \u001b[0m | \u001b[0m 53.21   \u001b[0m | \u001b[0m 0.009265\u001b[0m | \u001b[0m 75.88   \u001b[0m |\n",
      "| \u001b[0m 477     \u001b[0m | \u001b[0m 0.9448  \u001b[0m | \u001b[0m 0.5635  \u001b[0m | \u001b[0m 3.252   \u001b[0m | \u001b[0m 0.7154  \u001b[0m | \u001b[0m 0.7365  \u001b[0m | \u001b[0m 0.4606  \u001b[0m | \u001b[0m 1.383   \u001b[0m | \u001b[0m 0.1984  \u001b[0m | \u001b[0m 20.39   \u001b[0m | \u001b[0m 83.18   \u001b[0m | \u001b[0m 0.001431\u001b[0m | \u001b[0m 23.72   \u001b[0m |\n",
      "| \u001b[0m 478     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.664   \u001b[0m | \u001b[0m 5.085   \u001b[0m | \u001b[0m 0.5738  \u001b[0m | \u001b[0m 0.6886  \u001b[0m | \u001b[0m 1.721   \u001b[0m | \u001b[0m 1.857   \u001b[0m | \u001b[0m 0.1084  \u001b[0m | \u001b[0m 42.84   \u001b[0m | \u001b[0m 42.14   \u001b[0m | \u001b[0m 0.007994\u001b[0m | \u001b[0m 48.55   \u001b[0m |\n",
      "| \u001b[0m 479     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.8444  \u001b[0m | \u001b[0m 8.724   \u001b[0m | \u001b[0m 0.4977  \u001b[0m | \u001b[0m 0.655   \u001b[0m | \u001b[0m 0.672   \u001b[0m | \u001b[0m 1.271   \u001b[0m | \u001b[0m 0.14    \u001b[0m | \u001b[0m 26.86   \u001b[0m | \u001b[0m 57.79   \u001b[0m | \u001b[0m 0.005784\u001b[0m | \u001b[0m 20.03   \u001b[0m |\n",
      "| \u001b[0m 480     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.5149  \u001b[0m | \u001b[0m 9.294   \u001b[0m | \u001b[0m 0.6753  \u001b[0m | \u001b[0m 0.4317  \u001b[0m | \u001b[0m 2.755   \u001b[0m | \u001b[0m 1.57    \u001b[0m | \u001b[0m 0.1742  \u001b[0m | \u001b[0m 86.57   \u001b[0m | \u001b[0m 41.01   \u001b[0m | \u001b[0m 0.006193\u001b[0m | \u001b[0m 43.61   \u001b[0m |\n",
      "| \u001b[0m 481     \u001b[0m | \u001b[0m 0.9412  \u001b[0m | \u001b[0m 0.5669  \u001b[0m | \u001b[0m 3.537   \u001b[0m | \u001b[0m 0.5508  \u001b[0m | \u001b[0m 0.8084  \u001b[0m | \u001b[0m 1.851   \u001b[0m | \u001b[0m 2.313   \u001b[0m | \u001b[0m 0.1714  \u001b[0m | \u001b[0m 60.97   \u001b[0m | \u001b[0m 98.52   \u001b[0m | \u001b[0m 0.008893\u001b[0m | \u001b[0m 31.61   \u001b[0m |\n",
      "| \u001b[0m 482     \u001b[0m | \u001b[0m 0.938   \u001b[0m | \u001b[0m 0.9839  \u001b[0m | \u001b[0m 8.705   \u001b[0m | \u001b[0m 0.9559  \u001b[0m | \u001b[0m 0.9218  \u001b[0m | \u001b[0m 2.358   \u001b[0m | \u001b[0m 0.8306  \u001b[0m | \u001b[0m 0.1855  \u001b[0m | \u001b[0m 85.82   \u001b[0m | \u001b[0m 71.97   \u001b[0m | \u001b[0m 0.008389\u001b[0m | \u001b[0m 74.19   \u001b[0m |\n",
      "| \u001b[0m 483     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.8364  \u001b[0m | \u001b[0m 8.377   \u001b[0m | \u001b[0m 0.6198  \u001b[0m | \u001b[0m 0.6894  \u001b[0m | \u001b[0m 0.6859  \u001b[0m | \u001b[0m 1.074   \u001b[0m | \u001b[0m 0.04327 \u001b[0m | \u001b[0m 27.3    \u001b[0m | \u001b[0m 56.92   \u001b[0m | \u001b[0m 0.008963\u001b[0m | \u001b[0m 21.11   \u001b[0m |\n",
      "| \u001b[0m 484     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.4188  \u001b[0m | \u001b[0m 1.653   \u001b[0m | \u001b[0m 0.5014  \u001b[0m | \u001b[0m 0.6995  \u001b[0m | \u001b[0m 2.72    \u001b[0m | \u001b[0m 0.9666  \u001b[0m | \u001b[0m 0.07327 \u001b[0m | \u001b[0m 68.57   \u001b[0m | \u001b[0m 48.54   \u001b[0m | \u001b[0m 0.000268\u001b[0m | \u001b[0m 42.07   \u001b[0m |\n",
      "| \u001b[0m 485     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.6156  \u001b[0m | \u001b[0m 5.244   \u001b[0m | \u001b[0m 0.5154  \u001b[0m | \u001b[0m 0.8768  \u001b[0m | \u001b[0m 0.6553  \u001b[0m | \u001b[0m 0.7203  \u001b[0m | \u001b[0m 0.1364  \u001b[0m | \u001b[0m 78.66   \u001b[0m | \u001b[0m 86.75   \u001b[0m | \u001b[0m 0.005994\u001b[0m | \u001b[0m 61.54   \u001b[0m |\n",
      "| \u001b[0m 486     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.7898  \u001b[0m | \u001b[0m 5.577   \u001b[0m | \u001b[0m 0.9824  \u001b[0m | \u001b[0m 0.7898  \u001b[0m | \u001b[0m 1.878   \u001b[0m | \u001b[0m 1.471   \u001b[0m | \u001b[0m 0.09469 \u001b[0m | \u001b[0m 44.88   \u001b[0m | \u001b[0m 60.35   \u001b[0m | \u001b[0m 0.006632\u001b[0m | \u001b[0m 80.8    \u001b[0m |\n",
      "| \u001b[0m 487     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.7908  \u001b[0m | \u001b[0m 7.402   \u001b[0m | \u001b[0m 0.9389  \u001b[0m | \u001b[0m 0.4201  \u001b[0m | \u001b[0m 1.838   \u001b[0m | \u001b[0m 0.7877  \u001b[0m | \u001b[0m 0.1411  \u001b[0m | \u001b[0m 28.04   \u001b[0m | \u001b[0m 89.04   \u001b[0m | \u001b[0m 0.004035\u001b[0m | \u001b[0m 45.73   \u001b[0m |\n",
      "| \u001b[0m 488     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.5873  \u001b[0m | \u001b[0m 3.391   \u001b[0m | \u001b[0m 0.8863  \u001b[0m | \u001b[0m 0.9082  \u001b[0m | \u001b[0m 0.2975  \u001b[0m | \u001b[0m 1.165   \u001b[0m | \u001b[0m 0.1473  \u001b[0m | \u001b[0m 67.23   \u001b[0m | \u001b[0m 30.29   \u001b[0m | \u001b[0m 0.009169\u001b[0m | \u001b[0m 39.73   \u001b[0m |\n",
      "| \u001b[0m 489     \u001b[0m | \u001b[0m 0.9385  \u001b[0m | \u001b[0m 0.4862  \u001b[0m | \u001b[0m 9.225   \u001b[0m | \u001b[0m 0.587   \u001b[0m | \u001b[0m 0.8022  \u001b[0m | \u001b[0m 2.412   \u001b[0m | \u001b[0m 2.648   \u001b[0m | \u001b[0m 0.1924  \u001b[0m | \u001b[0m 69.37   \u001b[0m | \u001b[0m 77.11   \u001b[0m | \u001b[0m 0.003645\u001b[0m | \u001b[0m 42.68   \u001b[0m |\n",
      "| \u001b[0m 490     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.7469  \u001b[0m | \u001b[0m 3.854   \u001b[0m | \u001b[0m 0.6116  \u001b[0m | \u001b[0m 0.4235  \u001b[0m | \u001b[0m 2.746   \u001b[0m | \u001b[0m 2.432   \u001b[0m | \u001b[0m 0.1685  \u001b[0m | \u001b[0m 94.86   \u001b[0m | \u001b[0m 88.68   \u001b[0m | \u001b[0m 0.005157\u001b[0m | \u001b[0m 62.57   \u001b[0m |\n",
      "| \u001b[0m 491     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.5915  \u001b[0m | \u001b[0m 5.942   \u001b[0m | \u001b[0m 0.5828  \u001b[0m | \u001b[0m 0.6267  \u001b[0m | \u001b[0m 1.609   \u001b[0m | \u001b[0m 0.3331  \u001b[0m | \u001b[0m 0.09622 \u001b[0m | \u001b[0m 66.44   \u001b[0m | \u001b[0m 19.42   \u001b[0m | \u001b[0m 0.001514\u001b[0m | \u001b[0m 51.73   \u001b[0m |\n",
      "| \u001b[0m 492     \u001b[0m | \u001b[0m 0.9404  \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 1.702   \u001b[0m | \u001b[0m 0.9218  \u001b[0m | \u001b[0m 0.9342  \u001b[0m | \u001b[0m 0.8763  \u001b[0m | \u001b[0m 0.8849  \u001b[0m | \u001b[0m 0.06871 \u001b[0m | \u001b[0m 24.52   \u001b[0m | \u001b[0m 23.25   \u001b[0m | \u001b[0m 0.004934\u001b[0m | \u001b[0m 56.93   \u001b[0m |\n",
      "| \u001b[0m 493     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.8601  \u001b[0m | \u001b[0m 5.448   \u001b[0m | \u001b[0m 0.4492  \u001b[0m | \u001b[0m 0.6817  \u001b[0m | \u001b[0m 2.559   \u001b[0m | \u001b[0m 1.545   \u001b[0m | \u001b[0m 0.07356 \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 13.8    \u001b[0m | \u001b[0m 0.000176\u001b[0m | \u001b[0m 62.28   \u001b[0m |\n",
      "| \u001b[0m 494     \u001b[0m | \u001b[0m 0.9441  \u001b[0m | \u001b[0m 0.7402  \u001b[0m | \u001b[0m 8.831   \u001b[0m | \u001b[0m 0.6273  \u001b[0m | \u001b[0m 0.7288  \u001b[0m | \u001b[0m 0.8375  \u001b[0m | \u001b[0m 0.6923  \u001b[0m | \u001b[0m 0.08676 \u001b[0m | \u001b[0m 78.18   \u001b[0m | \u001b[0m 57.15   \u001b[0m | \u001b[0m 0.001213\u001b[0m | \u001b[0m 67.74   \u001b[0m |\n",
      "| \u001b[0m 495     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.7537  \u001b[0m | \u001b[0m 7.884   \u001b[0m | \u001b[0m 0.8785  \u001b[0m | \u001b[0m 0.6164  \u001b[0m | \u001b[0m 1.569   \u001b[0m | \u001b[0m 0.2141  \u001b[0m | \u001b[0m 0.1541  \u001b[0m | \u001b[0m 10.23   \u001b[0m | \u001b[0m 13.23   \u001b[0m | \u001b[0m 0.006495\u001b[0m | \u001b[0m 56.22   \u001b[0m |\n",
      "| \u001b[0m 496     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.9088  \u001b[0m | \u001b[0m 8.514   \u001b[0m | \u001b[0m 0.6726  \u001b[0m | \u001b[0m 0.6064  \u001b[0m | \u001b[0m 0.4836  \u001b[0m | \u001b[0m 2.224   \u001b[0m | \u001b[0m 0.1882  \u001b[0m | \u001b[0m 23.27   \u001b[0m | \u001b[0m 37.19   \u001b[0m | \u001b[0m 0.006133\u001b[0m | \u001b[0m 92.98   \u001b[0m |\n",
      "| \u001b[0m 497     \u001b[0m | \u001b[0m 0.9372  \u001b[0m | \u001b[0m 0.5809  \u001b[0m | \u001b[0m 6.414   \u001b[0m | \u001b[0m 0.9855  \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 1.795   \u001b[0m | \u001b[0m 2.954   \u001b[0m | \u001b[0m 0.1725  \u001b[0m | \u001b[0m 24.97   \u001b[0m | \u001b[0m 84.35   \u001b[0m | \u001b[0m 0.003537\u001b[0m | \u001b[0m 22.0    \u001b[0m |\n",
      "| \u001b[0m 498     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 9.924   \u001b[0m | \u001b[0m 0.8167  \u001b[0m | \u001b[0m 0.4194  \u001b[0m | \u001b[0m 0.1648  \u001b[0m | \u001b[0m 2.928   \u001b[0m | \u001b[0m 0.06649 \u001b[0m | \u001b[0m 79.36   \u001b[0m | \u001b[0m 44.35   \u001b[0m | \u001b[0m 0.008651\u001b[0m | \u001b[0m 54.88   \u001b[0m |\n",
      "| \u001b[0m 499     \u001b[0m | \u001b[0m 0.9386  \u001b[0m | \u001b[0m 0.7244  \u001b[0m | \u001b[0m 7.282   \u001b[0m | \u001b[0m 0.9305  \u001b[0m | \u001b[0m 0.9544  \u001b[0m | \u001b[0m 1.172   \u001b[0m | \u001b[0m 2.971   \u001b[0m | \u001b[0m 0.1495  \u001b[0m | \u001b[0m 59.39   \u001b[0m | \u001b[0m 67.47   \u001b[0m | \u001b[0m 0.005504\u001b[0m | \u001b[0m 34.37   \u001b[0m |\n",
      "| \u001b[0m 500     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.6794  \u001b[0m | \u001b[0m 7.949   \u001b[0m | \u001b[0m 0.4387  \u001b[0m | \u001b[0m 0.5743  \u001b[0m | \u001b[0m 0.8067  \u001b[0m | \u001b[0m 0.07557 \u001b[0m | \u001b[0m 0.1239  \u001b[0m | \u001b[0m 93.08   \u001b[0m | \u001b[0m 60.09   \u001b[0m | \u001b[0m 0.005723\u001b[0m | \u001b[0m 25.75   \u001b[0m |\n",
      "| \u001b[0m 501     \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.8127  \u001b[0m | \u001b[0m 1.564   \u001b[0m | \u001b[0m 0.4555  \u001b[0m | \u001b[0m 0.6593  \u001b[0m | \u001b[0m 2.925   \u001b[0m | \u001b[0m 1.211   \u001b[0m | \u001b[0m 0.05362 \u001b[0m | \u001b[0m 18.98   \u001b[0m | \u001b[0m 38.35   \u001b[0m | \u001b[0m 0.007373\u001b[0m | \u001b[0m 82.75   \u001b[0m |\n",
      "| \u001b[0m 502     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.4704  \u001b[0m | \u001b[0m 1.72    \u001b[0m | \u001b[0m 0.4564  \u001b[0m | \u001b[0m 0.5666  \u001b[0m | \u001b[0m 2.01    \u001b[0m | \u001b[0m 1.991   \u001b[0m | \u001b[0m 0.03111 \u001b[0m | \u001b[0m 36.12   \u001b[0m | \u001b[0m 11.23   \u001b[0m | \u001b[0m 0.006562\u001b[0m | \u001b[0m 62.38   \u001b[0m |\n",
      "| \u001b[0m 503     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.7082  \u001b[0m | \u001b[0m 8.416   \u001b[0m | \u001b[0m 0.609   \u001b[0m | \u001b[0m 0.8869  \u001b[0m | \u001b[0m 1.721   \u001b[0m | \u001b[0m 0.5919  \u001b[0m | \u001b[0m 0.1574  \u001b[0m | \u001b[0m 39.09   \u001b[0m | \u001b[0m 18.68   \u001b[0m | \u001b[0m 0.009602\u001b[0m | \u001b[0m 98.76   \u001b[0m |\n",
      "| \u001b[0m 504     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.9066  \u001b[0m | \u001b[0m 8.545   \u001b[0m | \u001b[0m 0.6879  \u001b[0m | \u001b[0m 0.9209  \u001b[0m | \u001b[0m 0.7627  \u001b[0m | \u001b[0m 1.524   \u001b[0m | \u001b[0m 0.03629 \u001b[0m | \u001b[0m 25.25   \u001b[0m | \u001b[0m 92.94   \u001b[0m | \u001b[0m 0.009853\u001b[0m | \u001b[0m 84.04   \u001b[0m |\n",
      "| \u001b[0m 505     \u001b[0m | \u001b[0m 0.9355  \u001b[0m | \u001b[0m 0.697   \u001b[0m | \u001b[0m 2.57    \u001b[0m | \u001b[0m 0.5606  \u001b[0m | \u001b[0m 0.5041  \u001b[0m | \u001b[0m 1.819   \u001b[0m | \u001b[0m 2.307   \u001b[0m | \u001b[0m 0.1735  \u001b[0m | \u001b[0m 90.87   \u001b[0m | \u001b[0m 76.01   \u001b[0m | \u001b[0m 0.001992\u001b[0m | \u001b[0m 56.05   \u001b[0m |\n",
      "| \u001b[0m 506     \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.8634  \u001b[0m | \u001b[0m 6.208   \u001b[0m | \u001b[0m 0.7405  \u001b[0m | \u001b[0m 0.5391  \u001b[0m | \u001b[0m 2.721   \u001b[0m | \u001b[0m 2.926   \u001b[0m | \u001b[0m 0.1544  \u001b[0m | \u001b[0m 38.0    \u001b[0m | \u001b[0m 46.86   \u001b[0m | \u001b[0m 0.00887 \u001b[0m | \u001b[0m 53.28   \u001b[0m |\n",
      "| \u001b[0m 507     \u001b[0m | \u001b[0m 0.9385  \u001b[0m | \u001b[0m 0.4825  \u001b[0m | \u001b[0m 9.938   \u001b[0m | \u001b[0m 0.5023  \u001b[0m | \u001b[0m 0.9665  \u001b[0m | \u001b[0m 0.1203  \u001b[0m | \u001b[0m 2.718   \u001b[0m | \u001b[0m 0.01104 \u001b[0m | \u001b[0m 25.19   \u001b[0m | \u001b[0m 25.96   \u001b[0m | \u001b[0m 0.004782\u001b[0m | \u001b[0m 20.22   \u001b[0m |\n",
      "| \u001b[0m 508     \u001b[0m | \u001b[0m 0.939   \u001b[0m | \u001b[0m 0.5816  \u001b[0m | \u001b[0m 8.26    \u001b[0m | \u001b[0m 0.9634  \u001b[0m | \u001b[0m 0.9487  \u001b[0m | \u001b[0m 0.4021  \u001b[0m | \u001b[0m 1.491   \u001b[0m | \u001b[0m 0.02288 \u001b[0m | \u001b[0m 91.15   \u001b[0m | \u001b[0m 77.12   \u001b[0m | \u001b[0m 0.003114\u001b[0m | \u001b[0m 97.03   \u001b[0m |\n",
      "| \u001b[0m 509     \u001b[0m | \u001b[0m 0.9402  \u001b[0m | \u001b[0m 0.6083  \u001b[0m | \u001b[0m 1.084   \u001b[0m | \u001b[0m 0.7731  \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 2.625   \u001b[0m | \u001b[0m 1.315   \u001b[0m | \u001b[0m 0.1894  \u001b[0m | \u001b[0m 70.27   \u001b[0m | \u001b[0m 44.21   \u001b[0m | \u001b[0m 0.002261\u001b[0m | \u001b[0m 27.95   \u001b[0m |\n",
      "| \u001b[0m 510     \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.6051  \u001b[0m | \u001b[0m 3.558   \u001b[0m | \u001b[0m 0.6976  \u001b[0m | \u001b[0m 0.8353  \u001b[0m | \u001b[0m 1.372   \u001b[0m | \u001b[0m 0.191   \u001b[0m | \u001b[0m 0.07889 \u001b[0m | \u001b[0m 30.79   \u001b[0m | \u001b[0m 92.17   \u001b[0m | \u001b[0m 0.003911\u001b[0m | \u001b[0m 33.28   \u001b[0m |\n",
      "| \u001b[0m 511     \u001b[0m | \u001b[0m 0.9416  \u001b[0m | \u001b[0m 0.6343  \u001b[0m | \u001b[0m 6.309   \u001b[0m | \u001b[0m 0.7303  \u001b[0m | \u001b[0m 0.9294  \u001b[0m | \u001b[0m 2.444   \u001b[0m | \u001b[0m 0.1306  \u001b[0m | \u001b[0m 0.08276 \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 72.35   \u001b[0m | \u001b[0m 0.00194 \u001b[0m | \u001b[0m 59.74   \u001b[0m |\n",
      "| \u001b[0m 512     \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.536   \u001b[0m | \u001b[0m 6.473   \u001b[0m | \u001b[0m 0.5968  \u001b[0m | \u001b[0m 0.4111  \u001b[0m | \u001b[0m 1.592   \u001b[0m | \u001b[0m 1.889   \u001b[0m | \u001b[0m 0.1615  \u001b[0m | \u001b[0m 20.14   \u001b[0m | \u001b[0m 90.96   \u001b[0m | \u001b[0m 0.001004\u001b[0m | \u001b[0m 59.59   \u001b[0m |\n",
      "| \u001b[0m 513     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.4807  \u001b[0m | \u001b[0m 3.99    \u001b[0m | \u001b[0m 0.6939  \u001b[0m | \u001b[0m 0.6006  \u001b[0m | \u001b[0m 0.7409  \u001b[0m | \u001b[0m 0.2552  \u001b[0m | \u001b[0m 0.1265  \u001b[0m | \u001b[0m 98.95   \u001b[0m | \u001b[0m 86.28   \u001b[0m | \u001b[0m 0.00186 \u001b[0m | \u001b[0m 79.08   \u001b[0m |\n",
      "| \u001b[0m 514     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.7681  \u001b[0m | \u001b[0m 4.103   \u001b[0m | \u001b[0m 0.8901  \u001b[0m | \u001b[0m 0.5979  \u001b[0m | \u001b[0m 2.191   \u001b[0m | \u001b[0m 1.904   \u001b[0m | \u001b[0m 0.06638 \u001b[0m | \u001b[0m 68.45   \u001b[0m | \u001b[0m 80.8    \u001b[0m | \u001b[0m 0.009693\u001b[0m | \u001b[0m 89.37   \u001b[0m |\n",
      "| \u001b[0m 515     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.498   \u001b[0m | \u001b[0m 1.468   \u001b[0m | \u001b[0m 0.9334  \u001b[0m | \u001b[0m 0.6807  \u001b[0m | \u001b[0m 0.5579  \u001b[0m | \u001b[0m 2.34    \u001b[0m | \u001b[0m 0.07463 \u001b[0m | \u001b[0m 27.3    \u001b[0m | \u001b[0m 95.18   \u001b[0m | \u001b[0m 0.008782\u001b[0m | \u001b[0m 50.6    \u001b[0m |\n",
      "| \u001b[0m 516     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.7901  \u001b[0m | \u001b[0m 5.231   \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.8461  \u001b[0m | \u001b[0m 1.052   \u001b[0m | \u001b[0m 2.471   \u001b[0m | \u001b[0m 0.1419  \u001b[0m | \u001b[0m 82.84   \u001b[0m | \u001b[0m 81.11   \u001b[0m | \u001b[0m 0.00301 \u001b[0m | \u001b[0m 67.92   \u001b[0m |\n",
      "| \u001b[0m 517     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.9045  \u001b[0m | \u001b[0m 5.397   \u001b[0m | \u001b[0m 0.7723  \u001b[0m | \u001b[0m 0.6058  \u001b[0m | \u001b[0m 1.583   \u001b[0m | \u001b[0m 0.7099  \u001b[0m | \u001b[0m 0.03843 \u001b[0m | \u001b[0m 40.37   \u001b[0m | \u001b[0m 63.6    \u001b[0m | \u001b[0m 0.009759\u001b[0m | \u001b[0m 60.12   \u001b[0m |\n",
      "| \u001b[0m 518     \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.8845  \u001b[0m | \u001b[0m 9.194   \u001b[0m | \u001b[0m 0.8853  \u001b[0m | \u001b[0m 0.6796  \u001b[0m | \u001b[0m 1.708   \u001b[0m | \u001b[0m 2.071   \u001b[0m | \u001b[0m 0.06783 \u001b[0m | \u001b[0m 25.5    \u001b[0m | \u001b[0m 56.38   \u001b[0m | \u001b[0m 0.009181\u001b[0m | \u001b[0m 20.47   \u001b[0m |\n",
      "| \u001b[0m 519     \u001b[0m | \u001b[0m 0.9347  \u001b[0m | \u001b[0m 0.7857  \u001b[0m | \u001b[0m 7.908   \u001b[0m | \u001b[0m 0.4804  \u001b[0m | \u001b[0m 0.527   \u001b[0m | \u001b[0m 0.7198  \u001b[0m | \u001b[0m 0.578   \u001b[0m | \u001b[0m 0.1009  \u001b[0m | \u001b[0m 25.39   \u001b[0m | \u001b[0m 25.9    \u001b[0m | \u001b[0m 0.007249\u001b[0m | \u001b[0m 72.03   \u001b[0m |\n",
      "| \u001b[0m 520     \u001b[0m | \u001b[0m 0.9409  \u001b[0m | \u001b[0m 0.8867  \u001b[0m | \u001b[0m 7.947   \u001b[0m | \u001b[0m 0.6924  \u001b[0m | \u001b[0m 0.7631  \u001b[0m | \u001b[0m 2.937   \u001b[0m | \u001b[0m 2.909   \u001b[0m | \u001b[0m 0.1218  \u001b[0m | \u001b[0m 63.83   \u001b[0m | \u001b[0m 38.2    \u001b[0m | \u001b[0m 0.008714\u001b[0m | \u001b[0m 21.38   \u001b[0m |\n",
      "| \u001b[0m 521     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.8777  \u001b[0m | \u001b[0m 8.131   \u001b[0m | \u001b[0m 0.9088  \u001b[0m | \u001b[0m 0.7177  \u001b[0m | \u001b[0m 1.125   \u001b[0m | \u001b[0m 2.482   \u001b[0m | \u001b[0m 0.1536  \u001b[0m | \u001b[0m 26.37   \u001b[0m | \u001b[0m 57.63   \u001b[0m | \u001b[0m 0.00414 \u001b[0m | \u001b[0m 20.97   \u001b[0m |\n",
      "| \u001b[0m 522     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 7.95    \u001b[0m | \u001b[0m 0.4375  \u001b[0m | \u001b[0m 0.6502  \u001b[0m | \u001b[0m 0.2698  \u001b[0m | \u001b[0m 1.238   \u001b[0m | \u001b[0m 0.06998 \u001b[0m | \u001b[0m 13.07   \u001b[0m | \u001b[0m 26.25   \u001b[0m | \u001b[0m 0.009664\u001b[0m | \u001b[0m 85.08   \u001b[0m |\n",
      "| \u001b[0m 523     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.6266  \u001b[0m | \u001b[0m 6.903   \u001b[0m | \u001b[0m 0.8031  \u001b[0m | \u001b[0m 0.542   \u001b[0m | \u001b[0m 1.236   \u001b[0m | \u001b[0m 2.129   \u001b[0m | \u001b[0m 0.01601 \u001b[0m | \u001b[0m 13.38   \u001b[0m | \u001b[0m 63.55   \u001b[0m | \u001b[0m 0.004667\u001b[0m | \u001b[0m 91.15   \u001b[0m |\n",
      "| \u001b[0m 524     \u001b[0m | \u001b[0m 0.9406  \u001b[0m | \u001b[0m 0.5996  \u001b[0m | \u001b[0m 6.397   \u001b[0m | \u001b[0m 0.7464  \u001b[0m | \u001b[0m 0.8026  \u001b[0m | \u001b[0m 0.0683  \u001b[0m | \u001b[0m 0.01293 \u001b[0m | \u001b[0m 0.02683 \u001b[0m | \u001b[0m 81.54   \u001b[0m | \u001b[0m 29.66   \u001b[0m | \u001b[0m 0.006325\u001b[0m | \u001b[0m 80.32   \u001b[0m |\n",
      "| \u001b[0m 525     \u001b[0m | \u001b[0m 0.9389  \u001b[0m | \u001b[0m 0.659   \u001b[0m | \u001b[0m 5.541   \u001b[0m | \u001b[0m 0.9845  \u001b[0m | \u001b[0m 0.9652  \u001b[0m | \u001b[0m 1.468   \u001b[0m | \u001b[0m 0.7461  \u001b[0m | \u001b[0m 0.1675  \u001b[0m | \u001b[0m 61.57   \u001b[0m | \u001b[0m 78.53   \u001b[0m | \u001b[0m 0.000311\u001b[0m | \u001b[0m 76.62   \u001b[0m |\n",
      "| \u001b[0m 526     \u001b[0m | \u001b[0m 0.9445  \u001b[0m | \u001b[0m 0.7548  \u001b[0m | \u001b[0m 7.97    \u001b[0m | \u001b[0m 0.5162  \u001b[0m | \u001b[0m 0.662   \u001b[0m | \u001b[0m 2.287   \u001b[0m | \u001b[0m 2.789   \u001b[0m | \u001b[0m 0.03786 \u001b[0m | \u001b[0m 82.16   \u001b[0m | \u001b[0m 96.28   \u001b[0m | \u001b[0m 0.005897\u001b[0m | \u001b[0m 31.4    \u001b[0m |\n",
      "| \u001b[0m 527     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 5.799   \u001b[0m | \u001b[0m 0.7208  \u001b[0m | \u001b[0m 0.7063  \u001b[0m | \u001b[0m 0.5679  \u001b[0m | \u001b[0m 0.4378  \u001b[0m | \u001b[0m 0.137   \u001b[0m | \u001b[0m 60.37   \u001b[0m | \u001b[0m 23.64   \u001b[0m | \u001b[0m 0.000908\u001b[0m | \u001b[0m 83.88   \u001b[0m |\n",
      "| \u001b[0m 528     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.5681  \u001b[0m | \u001b[0m 6.736   \u001b[0m | \u001b[0m 0.7654  \u001b[0m | \u001b[0m 0.5302  \u001b[0m | \u001b[0m 0.7296  \u001b[0m | \u001b[0m 1.054   \u001b[0m | \u001b[0m 0.1934  \u001b[0m | \u001b[0m 29.08   \u001b[0m | \u001b[0m 79.81   \u001b[0m | \u001b[0m 0.004353\u001b[0m | \u001b[0m 27.05   \u001b[0m |\n",
      "| \u001b[0m 529     \u001b[0m | \u001b[0m 0.9446  \u001b[0m | \u001b[0m 0.8528  \u001b[0m | \u001b[0m 8.852   \u001b[0m | \u001b[0m 0.8105  \u001b[0m | \u001b[0m 0.5823  \u001b[0m | \u001b[0m 1.548   \u001b[0m | \u001b[0m 1.408   \u001b[0m | \u001b[0m 0.1198  \u001b[0m | \u001b[0m 25.4    \u001b[0m | \u001b[0m 58.05   \u001b[0m | \u001b[0m 0.006171\u001b[0m | \u001b[0m 20.43   \u001b[0m |\n",
      "| \u001b[0m 530     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.9364  \u001b[0m | \u001b[0m 5.344   \u001b[0m | \u001b[0m 0.4706  \u001b[0m | \u001b[0m 0.756   \u001b[0m | \u001b[0m 0.1146  \u001b[0m | \u001b[0m 2.04    \u001b[0m | \u001b[0m 0.1422  \u001b[0m | \u001b[0m 81.72   \u001b[0m | \u001b[0m 26.16   \u001b[0m | \u001b[0m 0.009912\u001b[0m | \u001b[0m 89.41   \u001b[0m |\n",
      "| \u001b[0m 531     \u001b[0m | \u001b[0m 0.9341  \u001b[0m | \u001b[0m 0.638   \u001b[0m | \u001b[0m 5.071   \u001b[0m | \u001b[0m 0.5695  \u001b[0m | \u001b[0m 0.5156  \u001b[0m | \u001b[0m 2.214   \u001b[0m | \u001b[0m 2.656   \u001b[0m | \u001b[0m 0.1889  \u001b[0m | \u001b[0m 94.4    \u001b[0m | \u001b[0m 88.83   \u001b[0m | \u001b[0m 0.006078\u001b[0m | \u001b[0m 63.69   \u001b[0m |\n",
      "| \u001b[0m 532     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.4979  \u001b[0m | \u001b[0m 9.356   \u001b[0m | \u001b[0m 0.5022  \u001b[0m | \u001b[0m 0.7711  \u001b[0m | \u001b[0m 0.3045  \u001b[0m | \u001b[0m 2.311   \u001b[0m | \u001b[0m 0.1923  \u001b[0m | \u001b[0m 81.15   \u001b[0m | \u001b[0m 65.95   \u001b[0m | \u001b[0m 0.004068\u001b[0m | \u001b[0m 43.78   \u001b[0m |\n",
      "| \u001b[0m 533     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.6157  \u001b[0m | \u001b[0m 7.656   \u001b[0m | \u001b[0m 0.8999  \u001b[0m | \u001b[0m 0.538   \u001b[0m | \u001b[0m 1.612   \u001b[0m | \u001b[0m 1.033   \u001b[0m | \u001b[0m 0.07498 \u001b[0m | \u001b[0m 28.0    \u001b[0m | \u001b[0m 69.73   \u001b[0m | \u001b[0m 0.008512\u001b[0m | \u001b[0m 22.72   \u001b[0m |\n",
      "| \u001b[0m 534     \u001b[0m | \u001b[0m 0.9366  \u001b[0m | \u001b[0m 0.9134  \u001b[0m | \u001b[0m 8.638   \u001b[0m | \u001b[0m 0.5056  \u001b[0m | \u001b[0m 0.4726  \u001b[0m | \u001b[0m 0.718   \u001b[0m | \u001b[0m 1.841   \u001b[0m | \u001b[0m 0.06259 \u001b[0m | \u001b[0m 97.06   \u001b[0m | \u001b[0m 68.12   \u001b[0m | \u001b[0m 0.004304\u001b[0m | \u001b[0m 72.53   \u001b[0m |\n",
      "| \u001b[0m 535     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.6056  \u001b[0m | \u001b[0m 1.749   \u001b[0m | \u001b[0m 0.4392  \u001b[0m | \u001b[0m 0.6058  \u001b[0m | \u001b[0m 1.166   \u001b[0m | \u001b[0m 1.442   \u001b[0m | \u001b[0m 0.09952 \u001b[0m | \u001b[0m 47.37   \u001b[0m | \u001b[0m 27.91   \u001b[0m | \u001b[0m 0.001844\u001b[0m | \u001b[0m 70.0    \u001b[0m |\n",
      "| \u001b[0m 536     \u001b[0m | \u001b[0m 0.9348  \u001b[0m | \u001b[0m 0.7647  \u001b[0m | \u001b[0m 4.511   \u001b[0m | \u001b[0m 0.4974  \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 2.276   \u001b[0m | \u001b[0m 1.957   \u001b[0m | \u001b[0m 0.06669 \u001b[0m | \u001b[0m 17.67   \u001b[0m | \u001b[0m 78.72   \u001b[0m | \u001b[0m 0.003069\u001b[0m | \u001b[0m 63.93   \u001b[0m |\n",
      "| \u001b[0m 537     \u001b[0m | \u001b[0m 0.9357  \u001b[0m | \u001b[0m 0.7255  \u001b[0m | \u001b[0m 4.603   \u001b[0m | \u001b[0m 0.5927  \u001b[0m | \u001b[0m 0.5048  \u001b[0m | \u001b[0m 1.509   \u001b[0m | \u001b[0m 0.44    \u001b[0m | \u001b[0m 0.04314 \u001b[0m | \u001b[0m 13.28   \u001b[0m | \u001b[0m 43.46   \u001b[0m | \u001b[0m 0.00758 \u001b[0m | \u001b[0m 63.14   \u001b[0m |\n",
      "| \u001b[0m 538     \u001b[0m | \u001b[0m 0.9399  \u001b[0m | \u001b[0m 0.5568  \u001b[0m | \u001b[0m 3.481   \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.9585  \u001b[0m | \u001b[0m 0.06125 \u001b[0m | \u001b[0m 1.55    \u001b[0m | \u001b[0m 0.1484  \u001b[0m | \u001b[0m 57.44   \u001b[0m | \u001b[0m 80.92   \u001b[0m | \u001b[0m 0.008547\u001b[0m | \u001b[0m 42.51   \u001b[0m |\n",
      "| \u001b[0m 539     \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.6533  \u001b[0m | \u001b[0m 9.941   \u001b[0m | \u001b[0m 0.6764  \u001b[0m | \u001b[0m 0.6293  \u001b[0m | \u001b[0m 0.05259 \u001b[0m | \u001b[0m 0.2639  \u001b[0m | \u001b[0m 0.1617  \u001b[0m | \u001b[0m 39.96   \u001b[0m | \u001b[0m 99.48   \u001b[0m | \u001b[0m 0.000374\u001b[0m | \u001b[0m 53.14   \u001b[0m |\n",
      "| \u001b[0m 540     \u001b[0m | \u001b[0m 0.935   \u001b[0m | \u001b[0m 0.6194  \u001b[0m | \u001b[0m 7.385   \u001b[0m | \u001b[0m 0.4246  \u001b[0m | \u001b[0m 0.4633  \u001b[0m | \u001b[0m 0.4931  \u001b[0m | \u001b[0m 0.6149  \u001b[0m | \u001b[0m 0.0254  \u001b[0m | \u001b[0m 55.74   \u001b[0m | \u001b[0m 64.13   \u001b[0m | \u001b[0m 0.009686\u001b[0m | \u001b[0m 86.35   \u001b[0m |\n",
      "| \u001b[0m 541     \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.5692  \u001b[0m | \u001b[0m 6.724   \u001b[0m | \u001b[0m 0.8779  \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.6425  \u001b[0m | \u001b[0m 0.4617  \u001b[0m | \u001b[0m 0.1493  \u001b[0m | \u001b[0m 99.88   \u001b[0m | \u001b[0m 48.65   \u001b[0m | \u001b[0m 0.007179\u001b[0m | \u001b[0m 28.22   \u001b[0m |\n",
      "| \u001b[0m 542     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.5179  \u001b[0m | \u001b[0m 7.333   \u001b[0m | \u001b[0m 0.6993  \u001b[0m | \u001b[0m 0.7351  \u001b[0m | \u001b[0m 2.645   \u001b[0m | \u001b[0m 0.534   \u001b[0m | \u001b[0m 0.06062 \u001b[0m | \u001b[0m 38.88   \u001b[0m | \u001b[0m 55.08   \u001b[0m | \u001b[0m 0.009946\u001b[0m | \u001b[0m 50.58   \u001b[0m |\n",
      "| \u001b[0m 543     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.7585  \u001b[0m | \u001b[0m 7.215   \u001b[0m | \u001b[0m 0.8031  \u001b[0m | \u001b[0m 0.5622  \u001b[0m | \u001b[0m 0.05227 \u001b[0m | \u001b[0m 0.6558  \u001b[0m | \u001b[0m 0.04595 \u001b[0m | \u001b[0m 70.01   \u001b[0m | \u001b[0m 92.99   \u001b[0m | \u001b[0m 0.001539\u001b[0m | \u001b[0m 37.75   \u001b[0m |\n",
      "| \u001b[0m 544     \u001b[0m | \u001b[0m 0.9415  \u001b[0m | \u001b[0m 0.8216  \u001b[0m | \u001b[0m 2.699   \u001b[0m | \u001b[0m 0.6512  \u001b[0m | \u001b[0m 0.8368  \u001b[0m | \u001b[0m 2.512   \u001b[0m | \u001b[0m 1.425   \u001b[0m | \u001b[0m 0.1503  \u001b[0m | \u001b[0m 45.62   \u001b[0m | \u001b[0m 94.27   \u001b[0m | \u001b[0m 0.002174\u001b[0m | \u001b[0m 65.8    \u001b[0m |\n",
      "| \u001b[0m 545     \u001b[0m | \u001b[0m 0.9383  \u001b[0m | \u001b[0m 0.6207  \u001b[0m | \u001b[0m 9.95    \u001b[0m | \u001b[0m 0.6902  \u001b[0m | \u001b[0m 0.9383  \u001b[0m | \u001b[0m 1.82    \u001b[0m | \u001b[0m 2.302   \u001b[0m | \u001b[0m 0.1603  \u001b[0m | \u001b[0m 18.66   \u001b[0m | \u001b[0m 14.07   \u001b[0m | \u001b[0m 0.008653\u001b[0m | \u001b[0m 48.95   \u001b[0m |\n",
      "| \u001b[0m 546     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.7619  \u001b[0m | \u001b[0m 8.864   \u001b[0m | \u001b[0m 0.5377  \u001b[0m | \u001b[0m 0.9134  \u001b[0m | \u001b[0m 0.5924  \u001b[0m | \u001b[0m 1.338   \u001b[0m | \u001b[0m 0.1302  \u001b[0m | \u001b[0m 60.88   \u001b[0m | \u001b[0m 55.81   \u001b[0m | \u001b[0m 0.00964 \u001b[0m | \u001b[0m 71.3    \u001b[0m |\n",
      "| \u001b[0m 547     \u001b[0m | \u001b[0m 0.9418  \u001b[0m | \u001b[0m 0.4386  \u001b[0m | \u001b[0m 3.691   \u001b[0m | \u001b[0m 0.899   \u001b[0m | \u001b[0m 0.514   \u001b[0m | \u001b[0m 0.3582  \u001b[0m | \u001b[0m 2.764   \u001b[0m | \u001b[0m 0.1886  \u001b[0m | \u001b[0m 73.69   \u001b[0m | \u001b[0m 44.66   \u001b[0m | \u001b[0m 0.000507\u001b[0m | \u001b[0m 54.81   \u001b[0m |\n",
      "| \u001b[0m 548     \u001b[0m | \u001b[0m 0.9402  \u001b[0m | \u001b[0m 0.5945  \u001b[0m | \u001b[0m 5.74    \u001b[0m | \u001b[0m 0.6594  \u001b[0m | \u001b[0m 0.9536  \u001b[0m | \u001b[0m 2.759   \u001b[0m | \u001b[0m 0.04958 \u001b[0m | \u001b[0m 0.03058 \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 31.24   \u001b[0m | \u001b[0m 0.007136\u001b[0m | \u001b[0m 60.09   \u001b[0m |\n",
      "| \u001b[0m 549     \u001b[0m | \u001b[0m 0.9391  \u001b[0m | \u001b[0m 0.9609  \u001b[0m | \u001b[0m 3.147   \u001b[0m | \u001b[0m 0.5523  \u001b[0m | \u001b[0m 0.948   \u001b[0m | \u001b[0m 0.8428  \u001b[0m | \u001b[0m 0.6686  \u001b[0m | \u001b[0m 0.08374 \u001b[0m | \u001b[0m 20.4    \u001b[0m | \u001b[0m 82.82   \u001b[0m | \u001b[0m 0.006005\u001b[0m | \u001b[0m 22.59   \u001b[0m |\n",
      "| \u001b[0m 550     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.4356  \u001b[0m | \u001b[0m 7.205   \u001b[0m | \u001b[0m 0.8767  \u001b[0m | \u001b[0m 0.4848  \u001b[0m | \u001b[0m 1.038   \u001b[0m | \u001b[0m 2.046   \u001b[0m | \u001b[0m 0.171   \u001b[0m | \u001b[0m 80.49   \u001b[0m | \u001b[0m 41.71   \u001b[0m | \u001b[0m 0.009402\u001b[0m | \u001b[0m 97.87   \u001b[0m |\n",
      "| \u001b[0m 551     \u001b[0m | \u001b[0m 0.9431  \u001b[0m | \u001b[0m 0.7926  \u001b[0m | \u001b[0m 8.095   \u001b[0m | \u001b[0m 0.4206  \u001b[0m | \u001b[0m 0.8541  \u001b[0m | \u001b[0m 0.3749  \u001b[0m | \u001b[0m 2.88    \u001b[0m | \u001b[0m 0.02415 \u001b[0m | \u001b[0m 19.2    \u001b[0m | \u001b[0m 66.4    \u001b[0m | \u001b[0m 0.006123\u001b[0m | \u001b[0m 46.83   \u001b[0m |\n",
      "| \u001b[0m 552     \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.6979  \u001b[0m | \u001b[0m 6.189   \u001b[0m | \u001b[0m 0.6279  \u001b[0m | \u001b[0m 0.7351  \u001b[0m | \u001b[0m 1.88    \u001b[0m | \u001b[0m 0.397   \u001b[0m | \u001b[0m 0.1843  \u001b[0m | \u001b[0m 66.78   \u001b[0m | \u001b[0m 25.54   \u001b[0m | \u001b[0m 0.002594\u001b[0m | \u001b[0m 72.35   \u001b[0m |\n",
      "| \u001b[0m 553     \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.8135  \u001b[0m | \u001b[0m 1.687   \u001b[0m | \u001b[0m 0.6312  \u001b[0m | \u001b[0m 0.4648  \u001b[0m | \u001b[0m 0.3566  \u001b[0m | \u001b[0m 2.54    \u001b[0m | \u001b[0m 0.1885  \u001b[0m | \u001b[0m 67.32   \u001b[0m | \u001b[0m 37.62   \u001b[0m | \u001b[0m 0.003249\u001b[0m | \u001b[0m 31.56   \u001b[0m |\n",
      "| \u001b[0m 554     \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.8063  \u001b[0m | \u001b[0m 8.431   \u001b[0m | \u001b[0m 0.4549  \u001b[0m | \u001b[0m 0.6132  \u001b[0m | \u001b[0m 0.6039  \u001b[0m | \u001b[0m 1.733   \u001b[0m | \u001b[0m 0.1438  \u001b[0m | \u001b[0m 42.5    \u001b[0m | \u001b[0m 86.94   \u001b[0m | \u001b[0m 0.001489\u001b[0m | \u001b[0m 87.81   \u001b[0m |\n",
      "| \u001b[0m 555     \u001b[0m | \u001b[0m 0.9419  \u001b[0m | \u001b[0m 0.6937  \u001b[0m | \u001b[0m 1.643   \u001b[0m | \u001b[0m 0.462   \u001b[0m | \u001b[0m 0.8081  \u001b[0m | \u001b[0m 2.829   \u001b[0m | \u001b[0m 1.438   \u001b[0m | \u001b[0m 0.08724 \u001b[0m | \u001b[0m 28.12   \u001b[0m | \u001b[0m 53.94   \u001b[0m | \u001b[0m 0.001786\u001b[0m | \u001b[0m 55.87   \u001b[0m |\n",
      "| \u001b[0m 556     \u001b[0m | \u001b[0m 0.9345  \u001b[0m | \u001b[0m 0.6142  \u001b[0m | \u001b[0m 1.676   \u001b[0m | \u001b[0m 0.4845  \u001b[0m | \u001b[0m 0.4378  \u001b[0m | \u001b[0m 0.7198  \u001b[0m | \u001b[0m 0.7504  \u001b[0m | \u001b[0m 0.07787 \u001b[0m | \u001b[0m 99.97   \u001b[0m | \u001b[0m 32.88   \u001b[0m | \u001b[0m 0.00155 \u001b[0m | \u001b[0m 44.24   \u001b[0m |\n",
      "| \u001b[0m 557     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.5203  \u001b[0m | \u001b[0m 6.005   \u001b[0m | \u001b[0m 0.6529  \u001b[0m | \u001b[0m 0.6801  \u001b[0m | \u001b[0m 2.019   \u001b[0m | \u001b[0m 0.2105  \u001b[0m | \u001b[0m 0.1253  \u001b[0m | \u001b[0m 14.23   \u001b[0m | \u001b[0m 72.67   \u001b[0m | \u001b[0m 0.008417\u001b[0m | \u001b[0m 79.84   \u001b[0m |\n",
      "| \u001b[0m 558     \u001b[0m | \u001b[0m 0.9327  \u001b[0m | \u001b[0m 0.5172  \u001b[0m | \u001b[0m 7.104   \u001b[0m | \u001b[0m 0.5938  \u001b[0m | \u001b[0m 0.5511  \u001b[0m | \u001b[0m 0.04771 \u001b[0m | \u001b[0m 1.511   \u001b[0m | \u001b[0m 0.07532 \u001b[0m | \u001b[0m 58.89   \u001b[0m | \u001b[0m 67.63   \u001b[0m | \u001b[0m 0.005628\u001b[0m | \u001b[0m 35.3    \u001b[0m |\n",
      "| \u001b[0m 559     \u001b[0m | \u001b[0m 0.941   \u001b[0m | \u001b[0m 0.6479  \u001b[0m | \u001b[0m 6.178   \u001b[0m | \u001b[0m 0.86    \u001b[0m | \u001b[0m 0.8396  \u001b[0m | \u001b[0m 2.914   \u001b[0m | \u001b[0m 1.585   \u001b[0m | \u001b[0m 0.08765 \u001b[0m | \u001b[0m 77.52   \u001b[0m | \u001b[0m 48.67   \u001b[0m | \u001b[0m 0.009624\u001b[0m | \u001b[0m 28.28   \u001b[0m |\n",
      "| \u001b[0m 560     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.7722  \u001b[0m | \u001b[0m 6.137   \u001b[0m | \u001b[0m 0.8697  \u001b[0m | \u001b[0m 0.73    \u001b[0m | \u001b[0m 1.558   \u001b[0m | \u001b[0m 2.057   \u001b[0m | \u001b[0m 0.1335  \u001b[0m | \u001b[0m 62.38   \u001b[0m | \u001b[0m 94.92   \u001b[0m | \u001b[0m 0.008036\u001b[0m | \u001b[0m 88.11   \u001b[0m |\n",
      "| \u001b[0m 561     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.4869  \u001b[0m | \u001b[0m 2.331   \u001b[0m | \u001b[0m 0.8023  \u001b[0m | \u001b[0m 0.7426  \u001b[0m | \u001b[0m 0.9493  \u001b[0m | \u001b[0m 2.203   \u001b[0m | \u001b[0m 0.01726 \u001b[0m | \u001b[0m 21.88   \u001b[0m | \u001b[0m 38.17   \u001b[0m | \u001b[0m 0.008902\u001b[0m | \u001b[0m 31.11   \u001b[0m |\n",
      "| \u001b[0m 562     \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.4465  \u001b[0m | \u001b[0m 2.159   \u001b[0m | \u001b[0m 0.9743  \u001b[0m | \u001b[0m 0.4333  \u001b[0m | \u001b[0m 1.489   \u001b[0m | \u001b[0m 1.264   \u001b[0m | \u001b[0m 0.1238  \u001b[0m | \u001b[0m 17.82   \u001b[0m | \u001b[0m 33.2    \u001b[0m | \u001b[0m 0.003243\u001b[0m | \u001b[0m 63.15   \u001b[0m |\n",
      "| \u001b[0m 563     \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.6425  \u001b[0m | \u001b[0m 7.132   \u001b[0m | \u001b[0m 0.5964  \u001b[0m | \u001b[0m 0.9491  \u001b[0m | \u001b[0m 2.012   \u001b[0m | \u001b[0m 0.446   \u001b[0m | \u001b[0m 0.07733 \u001b[0m | \u001b[0m 37.48   \u001b[0m | \u001b[0m 11.71   \u001b[0m | \u001b[0m 0.000182\u001b[0m | \u001b[0m 55.01   \u001b[0m |\n",
      "| \u001b[0m 564     \u001b[0m | \u001b[0m 0.934   \u001b[0m | \u001b[0m 0.7256  \u001b[0m | \u001b[0m 2.807   \u001b[0m | \u001b[0m 0.4816  \u001b[0m | \u001b[0m 0.5012  \u001b[0m | \u001b[0m 1.451   \u001b[0m | \u001b[0m 1.3     \u001b[0m | \u001b[0m 0.08964 \u001b[0m | \u001b[0m 53.93   \u001b[0m | \u001b[0m 87.57   \u001b[0m | \u001b[0m 0.004026\u001b[0m | \u001b[0m 50.65   \u001b[0m |\n",
      "| \u001b[0m 565     \u001b[0m | \u001b[0m 0.9382  \u001b[0m | \u001b[0m 0.5949  \u001b[0m | \u001b[0m 4.356   \u001b[0m | \u001b[0m 0.917   \u001b[0m | \u001b[0m 0.9248  \u001b[0m | \u001b[0m 2.963   \u001b[0m | \u001b[0m 1.361   \u001b[0m | \u001b[0m 0.03299 \u001b[0m | \u001b[0m 77.37   \u001b[0m | \u001b[0m 12.97   \u001b[0m | \u001b[0m 0.000505\u001b[0m | \u001b[0m 31.67   \u001b[0m |\n",
      "| \u001b[0m 566     \u001b[0m | \u001b[0m 0.9401  \u001b[0m | \u001b[0m 0.4445  \u001b[0m | \u001b[0m 5.389   \u001b[0m | \u001b[0m 0.575   \u001b[0m | \u001b[0m 0.8539  \u001b[0m | \u001b[0m 2.072   \u001b[0m | \u001b[0m 2.727   \u001b[0m | \u001b[0m 0.1772  \u001b[0m | \u001b[0m 96.99   \u001b[0m | \u001b[0m 60.91   \u001b[0m | \u001b[0m 0.004976\u001b[0m | \u001b[0m 44.07   \u001b[0m |\n",
      "| \u001b[0m 567     \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.6205  \u001b[0m | \u001b[0m 6.046   \u001b[0m | \u001b[0m 0.7003  \u001b[0m | \u001b[0m 0.6945  \u001b[0m | \u001b[0m 0.3744  \u001b[0m | \u001b[0m 1.035   \u001b[0m | \u001b[0m 0.1882  \u001b[0m | \u001b[0m 62.84   \u001b[0m | \u001b[0m 12.41   \u001b[0m | \u001b[0m 0.003627\u001b[0m | \u001b[0m 53.26   \u001b[0m |\n",
      "| \u001b[0m 568     \u001b[0m | \u001b[0m 0.9444  \u001b[0m | \u001b[0m 0.9393  \u001b[0m | \u001b[0m 2.193   \u001b[0m | \u001b[0m 0.7185  \u001b[0m | \u001b[0m 0.4612  \u001b[0m | \u001b[0m 1.586   \u001b[0m | \u001b[0m 1.577   \u001b[0m | \u001b[0m 0.01743 \u001b[0m | \u001b[0m 24.02   \u001b[0m | \u001b[0m 72.43   \u001b[0m | \u001b[0m 0.002569\u001b[0m | \u001b[0m 76.98   \u001b[0m |\n",
      "| \u001b[0m 569     \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.4075  \u001b[0m | \u001b[0m 1.775   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8209  \u001b[0m | \u001b[0m 1.638   \u001b[0m | \u001b[0m 0.2065  \u001b[0m | \u001b[0m 0.1718  \u001b[0m | \u001b[0m 99.71   \u001b[0m | \u001b[0m 81.28   \u001b[0m | \u001b[0m 0.009289\u001b[0m | \u001b[0m 32.54   \u001b[0m |\n",
      "| \u001b[0m 570     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.6378  \u001b[0m | \u001b[0m 1.98    \u001b[0m | \u001b[0m 0.7579  \u001b[0m | \u001b[0m 0.7351  \u001b[0m | \u001b[0m 1.647   \u001b[0m | \u001b[0m 0.8952  \u001b[0m | \u001b[0m 0.1718  \u001b[0m | \u001b[0m 29.63   \u001b[0m | \u001b[0m 63.02   \u001b[0m | \u001b[0m 0.006798\u001b[0m | \u001b[0m 57.3    \u001b[0m |\n",
      "| \u001b[0m 571     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.5061  \u001b[0m | \u001b[0m 4.877   \u001b[0m | \u001b[0m 0.5389  \u001b[0m | \u001b[0m 0.4325  \u001b[0m | \u001b[0m 0.8188  \u001b[0m | \u001b[0m 2.592   \u001b[0m | \u001b[0m 0.09234 \u001b[0m | \u001b[0m 24.83   \u001b[0m | \u001b[0m 40.92   \u001b[0m | \u001b[0m 0.003533\u001b[0m | \u001b[0m 30.2    \u001b[0m |\n",
      "| \u001b[0m 572     \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 0.531   \u001b[0m | \u001b[0m 2.925   \u001b[0m | \u001b[0m 0.5684  \u001b[0m | \u001b[0m 0.9773  \u001b[0m | \u001b[0m 0.6994  \u001b[0m | \u001b[0m 1.736   \u001b[0m | \u001b[0m 0.03895 \u001b[0m | \u001b[0m 75.1    \u001b[0m | \u001b[0m 68.9    \u001b[0m | \u001b[0m 0.007035\u001b[0m | \u001b[0m 81.26   \u001b[0m |\n",
      "| \u001b[0m 573     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.6265  \u001b[0m | \u001b[0m 9.299   \u001b[0m | \u001b[0m 0.9693  \u001b[0m | \u001b[0m 0.4832  \u001b[0m | \u001b[0m 0.8227  \u001b[0m | \u001b[0m 1.212   \u001b[0m | \u001b[0m 0.0283  \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 99.93   \u001b[0m | \u001b[0m 0.001097\u001b[0m | \u001b[0m 39.62   \u001b[0m |\n",
      "| \u001b[0m 574     \u001b[0m | \u001b[0m 0.9312  \u001b[0m | \u001b[0m 0.4312  \u001b[0m | \u001b[0m 4.288   \u001b[0m | \u001b[0m 0.5613  \u001b[0m | \u001b[0m 0.9818  \u001b[0m | \u001b[0m 2.533   \u001b[0m | \u001b[0m 1.607   \u001b[0m | \u001b[0m 0.07726 \u001b[0m | \u001b[0m 24.25   \u001b[0m | \u001b[0m 11.98   \u001b[0m | \u001b[0m 0.005714\u001b[0m | \u001b[0m 44.97   \u001b[0m |\n",
      "| \u001b[0m 575     \u001b[0m | \u001b[0m 0.9399  \u001b[0m | \u001b[0m 0.4182  \u001b[0m | \u001b[0m 7.151   \u001b[0m | \u001b[0m 0.7175  \u001b[0m | \u001b[0m 0.4554  \u001b[0m | \u001b[0m 1.983   \u001b[0m | \u001b[0m 2.703   \u001b[0m | \u001b[0m 0.03739 \u001b[0m | \u001b[0m 84.05   \u001b[0m | \u001b[0m 64.22   \u001b[0m | \u001b[0m 0.00154 \u001b[0m | \u001b[0m 60.75   \u001b[0m |\n",
      "| \u001b[0m 576     \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.6665  \u001b[0m | \u001b[0m 4.058   \u001b[0m | \u001b[0m 0.9493  \u001b[0m | \u001b[0m 0.4032  \u001b[0m | \u001b[0m 2.273   \u001b[0m | \u001b[0m 0.782   \u001b[0m | \u001b[0m 0.06937 \u001b[0m | \u001b[0m 69.39   \u001b[0m | \u001b[0m 34.44   \u001b[0m | \u001b[0m 0.001856\u001b[0m | \u001b[0m 22.78   \u001b[0m |\n",
      "| \u001b[0m 577     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.4381  \u001b[0m | \u001b[0m 7.158   \u001b[0m | \u001b[0m 0.9703  \u001b[0m | \u001b[0m 0.5862  \u001b[0m | \u001b[0m 0.7264  \u001b[0m | \u001b[0m 0.1683  \u001b[0m | \u001b[0m 0.04735 \u001b[0m | \u001b[0m 84.54   \u001b[0m | \u001b[0m 23.64   \u001b[0m | \u001b[0m 0.002685\u001b[0m | \u001b[0m 95.63   \u001b[0m |\n",
      "| \u001b[0m 578     \u001b[0m | \u001b[0m 0.9421  \u001b[0m | \u001b[0m 0.7573  \u001b[0m | \u001b[0m 9.372   \u001b[0m | \u001b[0m 0.446   \u001b[0m | \u001b[0m 0.4194  \u001b[0m | \u001b[0m 0.3791  \u001b[0m | \u001b[0m 0.537   \u001b[0m | \u001b[0m 0.02025 \u001b[0m | \u001b[0m 17.17   \u001b[0m | \u001b[0m 88.86   \u001b[0m | \u001b[0m 0.008033\u001b[0m | \u001b[0m 81.26   \u001b[0m |\n",
      "| \u001b[0m 579     \u001b[0m | \u001b[0m 0.9422  \u001b[0m | \u001b[0m 0.6957  \u001b[0m | \u001b[0m 2.124   \u001b[0m | \u001b[0m 0.651   \u001b[0m | \u001b[0m 0.501   \u001b[0m | \u001b[0m 2.498   \u001b[0m | \u001b[0m 0.8108  \u001b[0m | \u001b[0m 0.1231  \u001b[0m | \u001b[0m 48.1    \u001b[0m | \u001b[0m 90.73   \u001b[0m | \u001b[0m 0.004431\u001b[0m | \u001b[0m 93.47   \u001b[0m |\n",
      "| \u001b[0m 580     \u001b[0m | \u001b[0m 0.9329  \u001b[0m | \u001b[0m 0.5016  \u001b[0m | \u001b[0m 8.782   \u001b[0m | \u001b[0m 0.4766  \u001b[0m | \u001b[0m 0.4963  \u001b[0m | \u001b[0m 2.807   \u001b[0m | \u001b[0m 2.724   \u001b[0m | \u001b[0m 0.1814  \u001b[0m | \u001b[0m 34.62   \u001b[0m | \u001b[0m 11.06   \u001b[0m | \u001b[0m 0.009428\u001b[0m | \u001b[0m 24.31   \u001b[0m |\n",
      "| \u001b[0m 581     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.7801  \u001b[0m | \u001b[0m 5.589   \u001b[0m | \u001b[0m 0.9019  \u001b[0m | \u001b[0m 0.7411  \u001b[0m | \u001b[0m 2.104   \u001b[0m | \u001b[0m 0.9644  \u001b[0m | \u001b[0m 0.01493 \u001b[0m | \u001b[0m 17.89   \u001b[0m | \u001b[0m 94.61   \u001b[0m | \u001b[0m 0.001435\u001b[0m | \u001b[0m 91.72   \u001b[0m |\n",
      "| \u001b[0m 582     \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 0.7876  \u001b[0m | \u001b[0m 6.077   \u001b[0m | \u001b[0m 0.9261  \u001b[0m | \u001b[0m 0.492   \u001b[0m | \u001b[0m 0.09843 \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 0.1299  \u001b[0m | \u001b[0m 39.85   \u001b[0m | \u001b[0m 51.8    \u001b[0m | \u001b[0m 0.005867\u001b[0m | \u001b[0m 94.76   \u001b[0m |\n",
      "| \u001b[0m 583     \u001b[0m | \u001b[0m 0.9439  \u001b[0m | \u001b[0m 0.8879  \u001b[0m | \u001b[0m 7.887   \u001b[0m | \u001b[0m 0.532   \u001b[0m | \u001b[0m 0.7065  \u001b[0m | \u001b[0m 0.8747  \u001b[0m | \u001b[0m 1.592   \u001b[0m | \u001b[0m 0.08607 \u001b[0m | \u001b[0m 67.7    \u001b[0m | \u001b[0m 16.24   \u001b[0m | \u001b[0m 0.004142\u001b[0m | \u001b[0m 67.84   \u001b[0m |\n",
      "| \u001b[0m 584     \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.7953  \u001b[0m | \u001b[0m 1.526   \u001b[0m | \u001b[0m 0.8123  \u001b[0m | \u001b[0m 0.8196  \u001b[0m | \u001b[0m 2.935   \u001b[0m | \u001b[0m 1.554   \u001b[0m | \u001b[0m 0.09801 \u001b[0m | \u001b[0m 66.48   \u001b[0m | \u001b[0m 50.64   \u001b[0m | \u001b[0m 0.006545\u001b[0m | \u001b[0m 38.87   \u001b[0m |\n",
      "| \u001b[0m 585     \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.6036  \u001b[0m | \u001b[0m 4.331   \u001b[0m | \u001b[0m 0.5579  \u001b[0m | \u001b[0m 0.8624  \u001b[0m | \u001b[0m 1.299   \u001b[0m | \u001b[0m 2.13    \u001b[0m | \u001b[0m 0.1012  \u001b[0m | \u001b[0m 62.02   \u001b[0m | \u001b[0m 88.48   \u001b[0m | \u001b[0m 0.000343\u001b[0m | \u001b[0m 20.99   \u001b[0m |\n",
      "| \u001b[0m 586     \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.4542  \u001b[0m | \u001b[0m 5.242   \u001b[0m | \u001b[0m 0.6882  \u001b[0m | \u001b[0m 0.9234  \u001b[0m | \u001b[0m 1.704   \u001b[0m | \u001b[0m 0.2349  \u001b[0m | \u001b[0m 0.02294 \u001b[0m | \u001b[0m 40.73   \u001b[0m | \u001b[0m 63.29   \u001b[0m | \u001b[0m 0.003753\u001b[0m | \u001b[0m 60.22   \u001b[0m |\n",
      "| \u001b[0m 587     \u001b[0m | \u001b[0m 0.9284  \u001b[0m | \u001b[0m 0.7919  \u001b[0m | \u001b[0m 3.967   \u001b[0m | \u001b[0m 0.4008  \u001b[0m | \u001b[0m 0.5052  \u001b[0m | \u001b[0m 0.7931  \u001b[0m | \u001b[0m 0.1135  \u001b[0m | \u001b[0m 0.06083 \u001b[0m | \u001b[0m 69.76   \u001b[0m | \u001b[0m 26.26   \u001b[0m | \u001b[0m 0.000311\u001b[0m | \u001b[0m 89.88   \u001b[0m |\n",
      "| \u001b[0m 588     \u001b[0m | \u001b[0m 0.934   \u001b[0m | \u001b[0m 0.5841  \u001b[0m | \u001b[0m 5.965   \u001b[0m | \u001b[0m 0.4408  \u001b[0m | \u001b[0m 0.4878  \u001b[0m | \u001b[0m 1.63    \u001b[0m | \u001b[0m 1.343   \u001b[0m | \u001b[0m 0.1589  \u001b[0m | \u001b[0m 91.3    \u001b[0m | \u001b[0m 15.09   \u001b[0m | \u001b[0m 0.00798 \u001b[0m | \u001b[0m 68.27   \u001b[0m |\n",
      "| \u001b[0m 589     \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.7056  \u001b[0m | \u001b[0m 4.839   \u001b[0m | \u001b[0m 0.6472  \u001b[0m | \u001b[0m 0.7362  \u001b[0m | \u001b[0m 2.856   \u001b[0m | \u001b[0m 2.893   \u001b[0m | \u001b[0m 0.1948  \u001b[0m | \u001b[0m 14.76   \u001b[0m | \u001b[0m 32.82   \u001b[0m | \u001b[0m 0.006683\u001b[0m | \u001b[0m 31.05   \u001b[0m |\n",
      "| \u001b[0m 590     \u001b[0m | \u001b[0m 0.9368  \u001b[0m | \u001b[0m 0.8444  \u001b[0m | \u001b[0m 8.094   \u001b[0m | \u001b[0m 0.9548  \u001b[0m | \u001b[0m 0.9396  \u001b[0m | \u001b[0m 1.775   \u001b[0m | \u001b[0m 0.3796  \u001b[0m | \u001b[0m 0.1172  \u001b[0m | \u001b[0m 71.86   \u001b[0m | \u001b[0m 95.91   \u001b[0m | \u001b[0m 0.002984\u001b[0m | \u001b[0m 86.17   \u001b[0m |\n",
      "| \u001b[0m 591     \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 0.411   \u001b[0m | \u001b[0m 8.52    \u001b[0m | \u001b[0m 0.9291  \u001b[0m | \u001b[0m 0.5664  \u001b[0m | \u001b[0m 0.6262  \u001b[0m | \u001b[0m 2.262   \u001b[0m | \u001b[0m 0.1963  \u001b[0m | \u001b[0m 26.62   \u001b[0m | \u001b[0m 57.99   \u001b[0m | \u001b[0m 0.009248\u001b[0m | \u001b[0m 21.04   \u001b[0m |\n",
      "| \u001b[0m 592     \u001b[0m | \u001b[0m 0.944   \u001b[0m | \u001b[0m 0.8207  \u001b[0m | \u001b[0m 8.241   \u001b[0m | \u001b[0m 0.6356  \u001b[0m | \u001b[0m 0.6872  \u001b[0m | \u001b[0m 1.139   \u001b[0m | \u001b[0m 2.7     \u001b[0m | \u001b[0m 0.1445  \u001b[0m | \u001b[0m 54.87   \u001b[0m | \u001b[0m 81.16   \u001b[0m | \u001b[0m 0.009594\u001b[0m | \u001b[0m 81.15   \u001b[0m |\n",
      "| \u001b[0m 593     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.7242  \u001b[0m | \u001b[0m 8.575   \u001b[0m | \u001b[0m 0.6467  \u001b[0m | \u001b[0m 0.9718  \u001b[0m | \u001b[0m 0.1855  \u001b[0m | \u001b[0m 1.049   \u001b[0m | \u001b[0m 0.1833  \u001b[0m | \u001b[0m 99.06   \u001b[0m | \u001b[0m 97.83   \u001b[0m | \u001b[0m 0.007255\u001b[0m | \u001b[0m 89.91   \u001b[0m |\n",
      "| \u001b[0m 594     \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 0.5204  \u001b[0m | \u001b[0m 3.537   \u001b[0m | \u001b[0m 0.714   \u001b[0m | \u001b[0m 0.9141  \u001b[0m | \u001b[0m 0.2714  \u001b[0m | \u001b[0m 0.5645  \u001b[0m | \u001b[0m 0.1759  \u001b[0m | \u001b[0m 47.43   \u001b[0m | \u001b[0m 75.61   \u001b[0m | \u001b[0m 0.003367\u001b[0m | \u001b[0m 98.08   \u001b[0m |\n",
      "| \u001b[0m 595     \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.5263  \u001b[0m | \u001b[0m 1.945   \u001b[0m | \u001b[0m 0.5414  \u001b[0m | \u001b[0m 0.4284  \u001b[0m | \u001b[0m 1.406   \u001b[0m | \u001b[0m 2.306   \u001b[0m | \u001b[0m 0.0537  \u001b[0m | \u001b[0m 23.31   \u001b[0m | \u001b[0m 17.66   \u001b[0m | \u001b[0m 0.005253\u001b[0m | \u001b[0m 82.57   \u001b[0m |\n",
      "| \u001b[0m 596     \u001b[0m | \u001b[0m 0.9392  \u001b[0m | \u001b[0m 0.8798  \u001b[0m | \u001b[0m 8.457   \u001b[0m | \u001b[0m 0.936   \u001b[0m | \u001b[0m 0.8287  \u001b[0m | \u001b[0m 0.8447  \u001b[0m | \u001b[0m 2.046   \u001b[0m | \u001b[0m 0.04101 \u001b[0m | \u001b[0m 54.88   \u001b[0m | \u001b[0m 80.81   \u001b[0m | \u001b[0m 0.003487\u001b[0m | \u001b[0m 81.68   \u001b[0m |\n",
      "| \u001b[0m 597     \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 0.4427  \u001b[0m | \u001b[0m 1.786   \u001b[0m | \u001b[0m 0.9661  \u001b[0m | \u001b[0m 0.8915  \u001b[0m | \u001b[0m 1.866   \u001b[0m | \u001b[0m 2.376   \u001b[0m | \u001b[0m 0.1717  \u001b[0m | \u001b[0m 88.03   \u001b[0m | \u001b[0m 82.58   \u001b[0m | \u001b[0m 0.000133\u001b[0m | \u001b[0m 34.47   \u001b[0m |\n",
      "| \u001b[0m 598     \u001b[0m | \u001b[0m 0.9432  \u001b[0m | \u001b[0m 0.8641  \u001b[0m | \u001b[0m 4.707   \u001b[0m | \u001b[0m 0.4152  \u001b[0m | \u001b[0m 0.7412  \u001b[0m | \u001b[0m 1.249   \u001b[0m | \u001b[0m 0.6562  \u001b[0m | \u001b[0m 0.04784 \u001b[0m | \u001b[0m 87.1    \u001b[0m | \u001b[0m 47.64   \u001b[0m | \u001b[0m 0.008188\u001b[0m | \u001b[0m 48.68   \u001b[0m |\n",
      "| \u001b[0m 599     \u001b[0m | \u001b[0m 0.9427  \u001b[0m | \u001b[0m 0.6161  \u001b[0m | \u001b[0m 4.241   \u001b[0m | \u001b[0m 0.612   \u001b[0m | \u001b[0m 0.8952  \u001b[0m | \u001b[0m 0.03368 \u001b[0m | \u001b[0m 2.125   \u001b[0m | \u001b[0m 0.1884  \u001b[0m | \u001b[0m 75.71   \u001b[0m | \u001b[0m 24.06   \u001b[0m | \u001b[0m 0.004975\u001b[0m | \u001b[0m 46.53   \u001b[0m |\n",
      "| \u001b[0m 600     \u001b[0m | \u001b[0m 0.942   \u001b[0m | \u001b[0m 0.4943  \u001b[0m | \u001b[0m 7.747   \u001b[0m | \u001b[0m 0.8633  \u001b[0m | \u001b[0m 0.8555  \u001b[0m | \u001b[0m 1.457   \u001b[0m | \u001b[0m 0.5801  \u001b[0m | \u001b[0m 0.07101 \u001b[0m | \u001b[0m 10.72   \u001b[0m | \u001b[0m 13.02   \u001b[0m | \u001b[0m 0.008232\u001b[0m | \u001b[0m 56.78   \u001b[0m |\n",
      "=============================================================================================================================================================\n",
      "Training with 8 features\n",
      "Training with features:  ['age_approx', 'anatom_site_general_challenge', 'sex', 'predictions_6', 'predictions_9', 'predictions_19', 'predictions_5', 'predictions_18']\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 1\"\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.952006\tvalid_1's auc: 0.921118\n",
      "[100]\ttraining's auc: 0.954893\tvalid_1's auc: 0.933389\n",
      "[150]\ttraining's auc: 0.955091\tvalid_1's auc: 0.934061\n",
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's auc: 0.954936\tvalid_1's auc: 0.934268\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 2\"\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.952714\tvalid_1's auc: 0.94719\n",
      "[100]\ttraining's auc: 0.952593\tvalid_1's auc: 0.947061\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's auc: 0.952587\tvalid_1's auc: 0.947591\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 3\"\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.95314\tvalid_1's auc: 0.946018\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's auc: 0.952451\tvalid_1's auc: 0.946202\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 4\"\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.952291\tvalid_1's auc: 0.948497\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.950674\tvalid_1's auc: 0.950176\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 5\"\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.951663\tvalid_1's auc: 0.94703\n",
      "[100]\ttraining's auc: 0.952263\tvalid_1's auc: 0.947672\n",
      "[150]\ttraining's auc: 0.952203\tvalid_1's auc: 0.947583\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's auc: 0.952295\tvalid_1's auc: 0.947846\n",
      "Our oof roc auc score for our lgbm model is 0.9448031518896025\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_lgbm(train, test, params, features, verbose_eval, folds = 5):\n",
    "    \n",
    "    if verbose_eval != False:\n",
    "        print(f'Training with {len(features)} features')\n",
    "        print('Training with features: ', features)\n",
    "    \n",
    "    \n",
    "    # groupkfolds to predict evaluate unknown clients (just like the test set)\n",
    "    kf = GroupKFold(n_splits = folds)\n",
    "    target = 'target'\n",
    "    \n",
    "    oof_pred = np.zeros(len(train))\n",
    "    y_pred = np.zeros(len(test))\n",
    "     \n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(train, groups = train['tfrecord'])):\n",
    "        if verbose_eval != False:\n",
    "            print('\\n')\n",
    "            print('-'*50)\n",
    "            print(f'Training fold {fold + 1}\"')\n",
    "        x_train, x_val = train[features].iloc[tr_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[target][tr_ind], train[target][val_ind]\n",
    "        train_set = lgb.Dataset(x_train, y_train)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "        \n",
    "        model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n",
    "                         valid_sets = [train_set, val_set], verbose_eval = verbose_eval)\n",
    "        \n",
    "        \n",
    "        oof_pred[val_ind] = model.predict(x_val)\n",
    "        \n",
    "        y_pred += model.predict(test[features]) / kf.n_splits\n",
    "        \n",
    "    rauc = metrics.roc_auc_score(train['target'], oof_pred)\n",
    "    if verbose_eval != False:\n",
    "        print(f'Our oof roc auc score for our lgbm model is {rauc}')\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    return rauc, y_pred, oof_pred\n",
    "\n",
    "def run_lgb_bayesian(num_leaves, learning_rate, max_depth, lambda_l1, lambda_l2, bagging_fraction, bagging_freq, colsample_bytree, colsample_bynode, min_data_per_leaf, min_sum_hessian_per_leaf):\n",
    "    \n",
    "    params = {\n",
    "        'boosting_type': 'rf',\n",
    "        'metric': 'auc',\n",
    "        'objective': 'binary',\n",
    "        'n_jobs': -1,\n",
    "        'seed': SEED,\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': int(max_depth),\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': int(bagging_freq),\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'colsample_bynode': colsample_bynode,\n",
    "        'min_data_per_leaf': int(min_data_per_leaf),\n",
    "        'min_sum_hessian_per_leaf': min_sum_hessian_per_leaf,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    rauc, y_pred, oof_pred = train_and_evaluate_lgbm(train, test, params, good_features, False)\n",
    "    return rauc\n",
    "\n",
    "\n",
    "# run bayesian optimization with optimal features\n",
    "bounds_lgb = {\n",
    "    'num_leaves': (20, 100),\n",
    "    'learning_rate': (0.01, 0.2),\n",
    "    'max_depth': (8, 100),\n",
    "    'lambda_l1': (0, 3),\n",
    "    'lambda_l2': (0, 3),\n",
    "    'bagging_fraction': (0.4, 0.9999),\n",
    "    'bagging_freq': (1, 10),\n",
    "    'colsample_bytree': (0.4, 1),\n",
    "    'colsample_bynode': (0.4, 1),\n",
    "    'min_data_per_leaf': (10, 100),\n",
    "    'min_sum_hessian_per_leaf': (0.0001, 0.01)\n",
    "}\n",
    "\n",
    "lgb_bo = BayesianOptimization(run_lgb_bayesian, bounds_lgb, random_state = SEED)\n",
    "lgb_bo.maximize(init_points = 300, n_iter = 300, acq = 'ucb', xi = 0.0, alpha = 1e-6)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'rf',\n",
    "    'metric': 'auc',\n",
    "    'objective': 'binary',\n",
    "    'n_jobs': -1,\n",
    "    'seed': SEED,\n",
    "    'num_leaves': int(lgb_bo.max['params']['num_leaves']),\n",
    "    'learning_rate': lgb_bo.max['params']['learning_rate'],\n",
    "    'max_depth': int(lgb_bo.max['params']['max_depth']),\n",
    "    'lambda_l1': lgb_bo.max['params']['lambda_l1'],\n",
    "    'lambda_l2': lgb_bo.max['params']['lambda_l2'],\n",
    "    'bagging_fraction': lgb_bo.max['params']['bagging_fraction'],\n",
    "    'bagging_freq': int(lgb_bo.max['params']['bagging_freq']),\n",
    "    'colsample_bytree': lgb_bo.max['params']['colsample_bytree'],\n",
    "    'colsample_bynode': lgb_bo.max['params']['colsample_bynode'],\n",
    "    'min_data_per_leaf': int(lgb_bo.max['params']['min_data_per_leaf']),\n",
    "    'min_sum_hessian_per_leaf': lgb_bo.max['params']['min_sum_hessian_per_leaf']\n",
    "}\n",
    "\n",
    "\n",
    "# train with new hyperparameters\n",
    "roc_auc, y_pred, oof_pred = train_and_evaluate_lgbm(train, test, params, good_features, 50)\n",
    "\n",
    "# saving out of folds predictions\n",
    "train['prediction'] = oof_pred\n",
    "train[['image_name', 'target', 'prediction']].to_csv(f'lgbm_1_{SEED}.csv', index = False)\n",
    "\n",
    "# predict\n",
    "test['target'] = y_pred \n",
    "sub = test[['image_name', 'target']]\n",
    "sub.to_csv(f'sub_lgbm_1_{SEED}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.325388,
     "end_time": "2020-08-16T19:35:09.902979",
     "exception": false,
     "start_time": "2020-08-16T19:35:09.577591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4702.061601,
   "end_time": "2020-08-16T19:35:10.350161",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-16T18:16:48.288560",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
